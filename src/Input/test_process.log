{
  "corpus": [
    {
      "content": [
        {
          "section": "abstract",
          "title": "ABSTRACT",
          "sentences": {
            "0": "Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule-based methods.",
            "1": "In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers.",
            "2": "The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, corpus genre or language to another.",
            "3": "Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging.",
            "4": "The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rulebased tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below."
          }
        },
        {
          "section": "introduction",
          "title": "1. INTRODUCTION",
          "sentences": {
            "5": "There has been a dramatic increase in the application of probabilistic models to natural language processing over the last few years.",
            "6": "The appeal of stochastic techniques over traditional rule-based techniques comes from the ease with which the necessary statistics can be automatically acquired and the fact that very little handcrafted knowledge need be built into the system.",
            "7": "In contrast, the rules in rule-based systems are usually difficult to construct and are typically not very robust.",
            "8": "One area in which the statistical approach has done particularly well is automatic part of speech tagging, assigning each word in an input sentence its proper part of speech [1, 2, 3, 4, 6, 9, 11, 12].",
            "9": "Stochastic taggers have obtained a high degree of accuracy without performing any syntactic analysis on the input.",
            "10": "These stochastic part of speech taggers make use of a Markov model which captures lexical and contextual information.",
            "11": "The parameters of the model can be estimated from tagged [1, 3, 4, 6, 12] or untagged [2, 9, 11] text.",
            "12": "Once the parameters of the model are estimated, a sentence can then be automatically tagged by assigning it the tag sequence which is assigned the highest probability by the model.",
            "13": "Performance is often enhanced with the aid of various higher level pre- and postprocessing procedures or by manually tuning the model.",
            "14": "A number of rule-based taggers have been built [10, 7, 8].",
            "15": "[10] and [7] both have error rates substantially higher than state of the art stochastic taggers.",
            "16": "[8] disambiguates words within a deterministic parser.",
            "17": "We wanted to determine whether a simple rule-based tagger without any knowledge of syntax can perform as well as a stochastic tagger, or if part of speech tagging really is a domain to which stochastic techniques are better suited.",
            "18": "In this paper we describe a rule-based tagger which performs as well as taggers based upon probabilistic models.",
            "19": "The rule-based tagger overcomes the limitations common in rule-based approaches to language processing: it is robust, and the rules are automatically acquired.",
            "20": "In addition, the tagger has many advantages over stochastic taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules as opposed to the large tables of statistics needed for stochastic taggers, ease of finding and implementing improvements to the tagger, and better portability from one tag set or corpus genre to another."
          }
        },
        {
          "section": "method",
          "title": "2. THE TAGGER",
          "sentences": {
            "21": "The tagger works by automatically recognizing and remedying its weaknesses, thereby incrementally improving its performance.",
            "22": "The tagger initially tags by assigning each word its most likely tag, estimated by examining a large tagged corpus, without regard to context.",
            "23": "In both sentences below, run would be tagged as a verb: The run lasted thirty minutes.",
            "24": "We run three miles every day.",
            "25": "The initial tagger has two procedures built in to improve performance; both make use of no contextual information.",
            "26": "One procedure is provided with information that words that were not in the training corpus and are capitalized tend to be proper nouns, and attempts to fix tagging mistakes accordingly.",
            "27": "This information could be acquired automatically (see below), but is prespecified in the current implementation.",
            "28": "In addition, there is a procedure which attempts to tag words not seen in the training corpus by assigning such words the tag most common for words ending in the same three letters.",
            "29": "For example, blahblahous would be tagged as an adjective, because this is the most common tag for words ending in ous.",
            "30": "This information is derived automatically from the training corpus.",
            "31": "This very simple algorithm has an error rate of about 7.9% when trained on 90% of the tagged Brown Corpusl [5], and tested on a separate 5% of the corpus'.",
            "32": "Training consists of compiling a list of the most common tag for each word in the training corpus.",
            "33": "The tagger then acquires patches to improve its performance.",
            "34": "Patch templates are of the form: have been tagged with tagb in the patch corpus.",
            "35": "Next, for each error triple, it is determined which instantiation of a template from the prespecified set of patch templates results in the greatest error reduction.",
            "36": "Currently, the patch templates are: Change tag a to tag b when: The initial tagger was trained on 90% of the corpus (the training corpus).",
            "37": "5% was held back to be used for the patch acquisition procedure (the patch corpus) and 5% for testing.",
            "38": "Once the initial tagger is trained, it is used to tag the patch corpus.",
            "39": "A list of tagging errors is compiled by comparing the output of the tagger to the correct tagging of the patch corpus.",
            "40": "This list consists of triples < taga, tag b, number >, indicating the number of times the tagger mistagged a word with tag° when it should The Brown Corpus contains about 1.1 million words from a variety of genres of written English.",
            "41": "There are 192 tags in the tag set, 96 of which occur more than one hundred times in the corpus.",
            "42": "2The test set contained text from all genres in the Brown Corpus.",
            "43": "For each error triple < tago,tagb,number > and patch, we compute the reduction in error which results from applying the patch to remedy the mistagging of a word as taga when it should have been tagged tam,.",
            "44": "We then compute the number of new errors caused by applying the patch; that is, the number of times the patch results in a word being tagged as tagb when it should be tagged Jaya.",
            "45": "The net improvement is calculated by subtracting the latter value from the former.",
            "46": "For example, when the initial tagger tags the patch corpus, it mistags 159 words as verbs when they should be nouns.",
            "47": "If the patch change the tag from verb to noun if one of the two preceding words is tagged as a determiner is applied, it corrects 98 of the 159 errors.",
            "48": "However, it results in an additional 18 errors from changing tags which really should have been verb to noun.",
            "49": "This patch results in a net decrease of 80 errors on the patch corpus.",
            "50": "The patch which results in the greatest improvement to the patch corpus is added to the list of patches.",
            "51": "The patch is then applied in order to improve the tagging of the patch corpus, and the patch acquisition procedure continues.",
            "52": "The first ten patches found by the system are listed below3.",
            "53": "The first patch states that if a word is tagged TO and the following word is tagged AT, then switch the tag from TO to IN.",
            "54": "This is because a noun phrase is much more likely to immediately follow a preposition than to immediately follow infinitive TO.",
            "55": "The second patch states that a tag should be switched from VBN to VBD if the preceding word is capitalized.",
            "56": "This patch arises from two facts: the past verb tag is more likely than the past participle verb tag after a proper noun, and is also the more likely tag for the second word of the sentence.4 The third patch states that VBD should be changed to VBN if any of the preceding three words are tagged HVD.",
            "57": "Once the list of patches has been acquired, new text can be tagged as follows.",
            "58": "First, tag the text using the basic lexical tagger.",
            "59": "Next, apply each patch in turn to the corpus to decrease the error rate.",
            "60": "A patch which changes the tagging of a word from a to b only applies if the word has been tagged b somewhere in the training corpus.",
            "61": "Note that one need not be too careful when constructing the list of patch templates.",
            "62": "Adding a bad template to the list will not worsen performance.",
            "63": "If a template is bad, then no rules which are instantiations of that template will appear in the final list of patches learned by the tagger.",
            "64": "This makes it easy to experiment with extensions to the tagger."
          }
        },
        {
          "section": "evaluation",
          "title": "3. RESULTS",
          "sentences": {
            "65": "The tagger was tested on 5% of the Brown Corpus including sections from every genre.",
            "66": "First, the test corpus was tagged by the simple lexical tagger.",
            "67": "Next, each of the patches was in turn applied to the corpus.",
            "68": "Below is a graph showing the improvement in accuracy from applying patches.",
            "69": "It is significant that with only 71 patches, an error rate of 5.1% was obtained5.",
            "70": "Of the 71 patches, 66 resulted in a reduction in the number of errors in the test corpus, 3 resulted in no net change, and 2 resulted in a higher number of errors.",
            "71": "Almost all patches which were effective on the training corpus were also effective on the test corpus.",
            "72": "Unfortunately, it is difficult to compare our results with other published results.",
            "73": "In [12], an error rate of 3-4% on one domain, Wall Street Journal articles and 5.6% on another domain, texts on terrorism in Latin American countries, is quoted.",
            "74": "However, both the domains and the tag set are different from what we use.",
            "75": "[1] reports an accuracy of \"95-99% correct, depending on the definition of correct\".",
            "76": "We implemented a version of the algorithm described in [1] which did not make use of a dictionary to extend its lexical knowledge.",
            "77": "When trained and tested on the same samples used in our experiment, we found the error rate to be about 4.5%.",
            "78": "[3] quotes a 4% error rate when testing and training on the same text.",
            "79": "[6] reports an accuracy of 96-97%.",
            "80": "Their probabilistic tagger has been augmented with a handcrafted procedure to pretag problematic \"idioms\".",
            "81": "This procedure, which requires that a list of idioms be laboriously created by hand, contributes 3% toward the accuracy of their tagger, according to [3].",
            "82": "The idiom list would have to be rewritten if one wished to use this tagger for a different tag set or a different corpus.",
            "83": "It is interesting to note that the information contained in the idiom list can be automatically acquired by the rule-based tagger.",
            "84": "For example, their tagger had difficulty tagging as old as.",
            "85": "An explicit rule was written to pretag as old as with the proper tags.",
            "86": "According to the tagging scheme of the Brown Corpus, the first as should be tagged as a qualifier, and the second as a subordinating conjunction.",
            "87": "In the rule-based tagger, the most common tag for as is subordinating conjunction.",
            "88": "So initially, the second as is tagged correctly and the first as is tagged incorrectly.",
            "89": "To remedy this, the system acquires the patch: if the current word is tagged as a subordinating conjunction, and so is the word two positions ahead, then change the tag of the current word to qualifier'.",
            "90": "The rule-based tagger has automatically learned how to properly tag this \"idiom\".",
            "91": "Regardless of the precise rankings of the various taggers, we have demonstrated that a simple rule-based tagger with very few rules performs on par with stochastic taggers.",
            "92": "It should be mentioned that our results were obtained without the use of a dictionary.",
            "93": "Incorporating a large dictionary into the system would improve performance in two ways.",
            "94": "First, it would increase the accuracy in tagging words not seen in the training corpus, since part of speech information for some words not appearing in the training corpus can be obtained from the dictionary.",
            "95": "Second, it would increase the error reduction resulting from applying patches.",
            "96": "When a patch indicates that a word should be tagged with tagb instead of taga, the tag is only switched if the word was tagged with tagb somewhere in the training corpus.",
            "97": "Using a dictionary would provide more accurate knowledge about the set of permissible part of speech tags for a particular word.",
            "98": "We plan to incorporate a dictionary into the tagger in the future.",
            "99": "As an estimate of the improvement possible by using a dictionary, we ran two experiments where all words were known by the system.",
            "100": "First, the Brown Corpus was divided into a training corpus of about one million words, a patch corpus of about 65,000 words and a test corpus of about 65,000 words.",
            "101": "Patches were acquired as described above.",
            "102": "When tested on the test corpus, with lexical information derived solely from the training corpus, the error rate was 5%.",
            "103": "Next, the same patches weie used, but lexical information was gathered from the entire Brown Corpus.",
            "104": "This reduced the error rate to 4.1%.",
            "105": "Finally, the same experiment was run with lexical information gathered solely from the test corpus.",
            "106": "This resulted in a 3.5% error rate.",
            "107": "Note that the patches used in the two experiments with no unknown words were not the optimal patches for these tests, since they were derived from a corpus that contained unknown words."
          }
        },
        {
          "section": "conclusions",
          "title": "4. CONCLUSIONS",
          "sentences": {
            "108": "We have presented a simple rule-based part of speech tagger which performs as well as existing stochastic taggers, but has significant advantages over these taggers.",
            "109": "The tagger is extremely portable.",
            "110": "Many of the higher level procedures used to improve the performance of stochastic taggers would not readily transfer over to a different tag set or genre, and certainly would not transfer over to a different language.",
            "111": "Everything except for the proper noun discovery procedure is automatically acquired by the rule-based tagger7, making it much more portable than a stochastic tagger.",
            "112": "If the tagger were trained on a different corpus, a different set of patches suitable for that corpus would be found automatically.",
            "113": "Large tables of statistics are not needed for the rulebased tagger.",
            "114": "In a stochastic tagger, tens of thousands of lines of statistical information are needed to capture contextual information.",
            "115": "This information is usually a table of trigram statistics, indicating for all tags taga,tagb and tags the probability that tag, follows taga and tagb.",
            "116": "In the rule-based tagger, contextual information is captured in fewer than eighty rules.",
            "117": "This makes for a much more perspicuous tagger, aiding in better understanding and simplifying further development of the tagger.",
            "118": "Contextual information is expressed in a much more compact and understandable form.",
            "119": "As can be seen from comparing error rates, this compact representation of contextual information is just as effective as the information hidden in the large tables of contextual probabilities.",
            "120": "Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable approach for part of speech tagging.",
            "121": "The fact that the simple rule-based tagger can perform so well should offer encouragement for researchers to further explore rule-based tagging, searching for a better and more expressive set of patch templates and other variations on this simple but effective theme."
          }
        }
      ],
      "doc_id": 0,
      "actual_doc_id": "H92-1022-parscit-section.xml"
    },
    {
      "content": [
        {
          "section": "abstract",
          "title": "Abstract",
          "sentences": {
            "0": "This paper describes a semantic role labeling system that uses features derived from different syntactic views, and combines them within a phrase-based chunking paradigm.",
            "1": "For an input sentence, syntactic constituent structure parses are generated by a Charniak parser and a Collins parser.",
            "2": "Semantic role labels are assigned to the constituents of each parse using Support Vector Machine classifiers.",
            "3": "The resulting semantic role labels are converted to an IOB representation.",
            "4": "These IOB representations are used as additional features, along with flat syntactic chunks, by a chunking SVM classifier that produces the final SRL output.",
            "5": "This strategy for combining features from three different syntactic views gives a significant improvement in performance over roles produced by using any one of the syntactic views individually."
          }
        },
        {
          "section": "introduction",
          "title": "1 Introduction",
          "sentences": {
            "6": "The task of Semantic Role Labeling (SRL) involves tagging groups of words in a sentence with the semantic roles that they play with respect to a particular predicate in that sentence.",
            "7": "Our approach is to use supervised machine learning classifiers to produce the role labels based on features extracted from the input.",
            "8": "This approach is neutral to the particular set of labels used, and will learn to tag input according to the annotated data that it is trained on.",
            "9": "The task reported on here is to produce PropBank (Kingsbury and Palmer, 2002) labels, given the features provided for the CoNLL-2005 closed task (Carreras and M`arquez, 2005).",
            "10": "We have previously reported on using SVM classifiers for semantic role labeling.",
            "11": "In this work, we formulate the semantic labeling problem as a multiclass classification problem using Support Vector Machine (SVM) classifiers.",
            "12": "Some of these systems use features based on syntactic constituents produced by a Charniak parser (Pradhan et al., 2003; Pradhan et al., 2004) and others use only a flat syntactic representation produced by a syntactic chunker (Hacioglu et al., 2003; Hacioglu and Ward, 2003; Hacioglu, 2004; Hacioglu et al., 2004).",
            "13": "The latter approach lacks the information provided by the hierarchical syntactic structure, and the former imposes a limitation that the possible candidate roles should be one of the nodes already present in the syntax tree.",
            "14": "We found that, while the chunk based systems are very efficient and robust, the systems that use features based on full syntactic parses are generally more accurate.",
            "15": "Analysis of the source of errors for the parse constituent based systems showed that incorrect parses were a major source of error.",
            "16": "The syntactic parser did not produce any constituent that corresponded to the correct segmentation for the semantic argument.",
            "17": "In Pradhan et al.",
            "18": "(2005), we reported on a first attempt to overcome this problem by combining semantic role labels produced from different syntactic parses.",
            "19": "The hope is that the syntactic parsers will make different errors, and that combining their outputs will improve on either system alone.",
            "20": "This initial attempt used features from a Charniak parser, a Minipar parser and a chunk based parser.",
            "21": "It did show some improvement from the combination, but the method for combining the information was heuristic and sub-optimal.",
            "22": "In this paper, we report on what we believe is an improved framework for combining information from different syntactic views.",
            "23": "Our goal is to preserve the robustness and flexibility of the segmentation of the phrase-based chunker, but to take advantage of features from full syntactic parses.",
            "24": "We also want to combine features from different syntactic parses to gain additional robustness.",
            "25": "To this end, we use features generated from a Charniak parser and a Collins parser, as supplied for the CoNLL-2005 closed task."
          }
        },
        {
          "section": "method",
          "title": "2 System Description",
          "sentences": {
            "26": "We again formulate the semantic labeling problem as a multi-class classification problem using Support Vector Machine (SVM) classifiers.",
            "27": "TinySVM1 along with YamCha2 (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system.",
            "28": "Using what is known as the ONE VS ALL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a NULL class.",
            "29": "The general framework is to train separate semantic role labeling systems for each of the parse tree views, and then to use the role arguments output by these systems as additional features in a semantic role classifier using a flat syntactic view.",
            "30": "The constituent based classifiers walk a syntactic parse tree and classify each node as NULL (no role) or as one of the set of semantic roles.",
            "31": "Chunk based systems classify each base phrase as being the B(eginning) of a semantic role, I(nside) a semantic role, or O(utside) any semantic role (ie.",
            "32": "NULL). This is referred to as an IOB representation (Ramshaw and Marcus, 1995).",
            "33": "The constituent level roles are mapped to the IOB representation used by the chunker.",
            "34": "The IOB tags are then used as features for a separate base-phase semantic role labeler (chunker), in addition to the standard set of features used by the chunker.",
            "35": "An n-fold cross-validation paradigm is used to train the constituent based role classifiers and the chunk based classifier.",
            "36": "For the system reported here, two full syntactic parsers were used, a Charniak parser and a Collins parser.",
            "37": "Features were extracted by first generating the Collins and Charniak syntax trees from the wordby-word decomposed trees in the CoNLL data.",
            "38": "The chunking system for combining all features was trained using a 4-fold paradigm.",
            "39": "In each fold, separate SVM classifiers were trained for the Collins and Charniak parses using 75% of the training data.",
            "40": "That is, one system assigned role labels to the nodes in Charniak based trees and a separate system assigned roles to nodes in Collins based trees.",
            "41": "The other 25% of the training data was then labeled by each of the systems.",
            "42": "Iterating this process 4 times created the training set for the chunker.",
            "43": "After the chunker was trained, the Charniak and Collins based semantic labelers were then retrained using all of the training data.",
            "44": "Two pieces of the system have problems scaling to large training sets – the final chunk based classifier and the NULL VS NON-NULL classifier for the parse tree syntactic views.",
            "45": "Two techniques were used to reduce the amount of training data – active sampling and NULL filtering.",
            "46": "The active sampling process was performed as follows.",
            "47": "We first train a system using 10k seed examples from the training set.",
            "48": "We then labeled an additional block of data using this system.",
            "49": "Any sentences containing an error were added to the seed training set.",
            "50": "The system was retrained and the procedure repeated until there were no misclassified sentences remaining in the training data.",
            "51": "The set of examples produced by this procedure was used to train the final NULL VS NON-NULL classifier.",
            "52": "The same procedure was carried out for the chunking system.",
            "53": "After both these were trained, we tagged the training data using them and removed all most likely NULLs from the data.",
            "54": "Table 1 lists the features used in the constituent based systems.",
            "55": "They are a combination of features introduced by Gildea and Jurafsky (2002), ones proposed in Pradhan et al.",
            "56": "(2004), Surdeanu et al.",
            "57": "(2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004).",
            "58": "These features are extracted from the parse tree being labeled.",
            "59": "In addition to the features extracted from the parse tree being labeled, five features were extracted from the other parse tree (phrase, head word, head word POS, path and predicate sub-categorization).",
            "60": "So for example, when assigning labels to constituents in a Charniak parse, all of the features in Table 1 were extracted from the Charniak tree, and in addition phrase, head word, head word POS, path and sub-categorization were extracted from the Collins tree.",
            "61": "We have previously determined that using different sets of features for each argument (role) achieves better results than using the same set of features for all argument classes.",
            "62": "A simple feature selection was implemented by adding features one by one to an initial set of features and selecting those that contribute significantly to the performance.",
            "63": "As described in Pradhan et al.",
            "64": "(2004), we post-process lattices of n-best decision using a trigram language model of argument sequences.",
            "65": "Table 2 lists the features used by the chunker.",
            "66": "These are the same set of features that were used in the CoNLL-2004 semantic role labeling task by Hacioglu, et al.",
            "67": "(2004) with the addition of the two semantic argument (IOB) features.",
            "68": "For each token (base phrase) to be tagged, a set of features is created from a fixed size context that surrounds each token.",
            "69": "In addition to the features in Table 2, it also uses previous semantic tags that have already been assigned to the tokens contained in the linguistic context.",
            "70": "A 5-token sliding window is used for the context.",
            "71": "SVMs were trained for begin (B) and inside (I) classes of all arguments and an outside (O) class."
          }
        },
        {
          "section": "method",
          "title": "3 Experimental Results",
          "sentences": {
            "72": "Table 3 shows the results obtained on the WSJ development set (Section 24), the WSJ test set (Section 23) and the Brown test set (Section ck/01-03)"
          }
        },
        {
          "section": "method",
          "title": "4 Acknowledgments",
          "sentences": {
            "73": "This research was partially supported by the ARDA AQUAINT program via contract OCG4423B and by the NSF via grants IS-9978025 and ITR/HCI 0086132.",
            "74": "Computer time was provided by NSF ARI Grant #CDA-9601817, NSF MRI Grant #CNS0420873, NASA AIST grant #NAG2-1646, DOE SciDAC grant #DE-FG02-04ER63870, NSF sponsorship of the National Center for Atmospheric Research, and a grant from the IBM Shared University Research (SUR) program.",
            "75": "Special thanks to Matthew Woitaszek, Theron Voran and the other administrative team of the Hemisphere and Occam Beowulf clusters.",
            "76": "Without these the training would never be possible."
          }
        }
      ],
      "doc_id": 1,
      "actual_doc_id": "W05-0634-parscit-section.xml"
    },
    {
      "content": [
        {
          "section": "abstract",
          "title": "Abstract",
          "sentences": {
            "0": "This paper describes a machine translation architecture that integrates the use of examples for flexible, idiomatic translations with the use of linguistic rules for broad coverage and grammatical accuracy.",
            "1": "We have implemented a prototype for English-to-Japanese translation, and our evaluation shows that the system has good translation quality, and only requires reasonable computational resources."
          }
        },
        {
          "section": "introduction",
          "title": "1 Introduction",
          "sentences": {
            "2": "Machine translation by analogy to pairs of corresponding expressions in the source and target languages, or “example-based translation”, was first proposed by (Nagao 1984).",
            "3": "Recent work in the example-based framework includes memory-based translation (Sato & Nagao 1990), similarity-driven translation (Watanabe 1992), transfer-driven machine translation (Furuse & Iida 1996), and pattern-based machine translation (Watanabe & Takeda 1998).",
            "4": "The example-based approach promises easy translation knowledge acquisition, more flexible transfer than brittle rule-based approaches, and idiomatic translations.",
            "5": "At the same time, the use of linguistic rules offers a number of important benefits.",
            "6": "Detailed linguistic analysis can allow an example-based machine translation system to handle a wide variety of input, since rules can be used to factor out all linguistic variations that do not influence the example-based transfer.",
            "7": "Rule-based language generation from detailed linguistic representations can lead to higher grammatical output quality.",
            "8": "Finally, a modular system architecture that uses domain-independent linguistic regularities in separate linguistic modules allows extending the system to much broader domains.",
            "9": "The HARMONY architecture for hybrid analogical and rule-based machine translation of naturally occurring colloquial language combines the advantages of both these approaches."
          }
        },
        {
          "section": "method",
          "title": "2 The Travel Domain",
          "sentences": {
            "10": "Our prototype implementation of the HARMONY architecture was designed to cover the “travel domain”.",
            "11": "This is composed of words, phrases, expressions, and sentences related to international travel, similar to what is covered by typical travel phrase books.",
            "12": "Two principles guided our detailed definition of the translation domain.",
            "13": "First, the translation domain should not be limited to a narrow sub-domain, such as appointment scheduling or hotel reservations.",
            "14": "Second, the expressions considered in the domain should reflect the fact that people quickly adapt to limitations in human-machine or machine-mediated communication by simplifying the input.",
            "15": "For example, (Sugaya et al. 1999) found that the average length of actual human utterances in a hotel reservation task using speech translation was only 6.1 words, much shorter than some of the data that has been used in previous work on speech translation.",
            "16": "The current vocabulary of 7,500 words is divided into a group of general words, a number of extensible word groups (such as names of food items or diseases), and a number of area-specific word groups (such as names of cities or tourist destinations).The travel domain is divided into eight “situations”: A general situation (including everyday conversation); transportation; accommodation; sightseeing; shopping; wining, dining, and nightlife; banking and postal; and doctor and pharmacy.",
            "17": "We created a corpus for this domain, and divided it into a development set of 7,000 expressions, and a separate, unseen test set of 5,000 expressions.",
            "18": "The development set is used for creation and refinement of the translation knowledge sources, and the test set is only used for evaluations.",
            "19": "(Each evaluation uses a new, random 500-word sample from the 5,000 word test set).",
            "20": "The corpus was balanced to illustrate the widest possible variety of types of words, phrases, syntactic structures, semantic patterns, and pragmatic functions.",
            "21": "The average length of the expressions in the corpus is 6.5 words.",
            "22": "Some examples from the development corpus are shown below.",
            "23": "Even though this domain might seem rather limited, it still contains many challenges for machine translation."
          }
        },
        {
          "section": "method",
          "title": "3 NLP Infrastructure",
          "sentences": {
            "24": "The prototype implementation is constructed out of components that are based on a powerful infrastructure for natural language processing and language engineering.",
            "25": "The three main aspects of this infrastructure are the Grammar Programming Language (GPL), the GPL compiler, and the GPL runtime environment.",
            "26": "The Grammar Programming Language (GPL) is an imperative programming language for feature-structure-based rewrite grammars.",
            "27": "GPL is a formalism that allows the direct expression of linguistic algorithms for parsing, transfer, and generation.",
            "28": "Some ideas in GPL can be traced back to Tomita’s pseudo-unification formalism (Tomita 1988), and to Lexical-Functional Grammar (Dalrymple et al. 1995).",
            "29": "GPL includes variables, simple and complex tests, and various manipulation operators.",
            "30": "GPL also includes control flow statements including if-then-else, switch, iteration over sub-feature-structures, and other features.",
            "31": "An example of a simplified GPL rule for English generation is shown in Figure 1.",
            "32": "GPL grammars are compiled into C code by the GPL compiler.",
            "33": "The GPL compiler was created using the Unix tools lex and yacc (Levine et al. 1990).",
            "34": "For each rewrite rule, the GPL compiler creates a main action function, which carries out most of the tests and manipulations specified by the GPL statements.",
            "35": "The GPL compiler handles disjunctive feature structures in an efficient manner by keeping track of sub-feature-structure references within each GPL rule, and by generating an expansion function that is called once before the action function.",
            "36": "The compiler also tracks variable references, and generates and tracks separate test functions for nested test expressions.",
            "37": "The result of compiling a GPL grammar is an encapsulated object that can be accessed via a public interface function.",
            "38": "This interface function serves as the link between the compiled GPL grammars, and the various language-independent and domain-independent software engines for parsing, transfer, generation, and others.",
            "39": "This is illustrated in Figure 2.",
            "40": "The compiled GPL grammars use the feature structure library, which provides services for efficiently representing, testing, manipulating, and managing memory for feature structures.",
            "41": "A special-purpose memory manager maintains separate stacks of memory pages for each object size.",
            "42": "This scheme allows garbage collection that is so fast that it can be performed after every attempted GPL rule execution.",
            "43": "In our experiments with Japanese and English parsing, we found that per-rule garbage collection reduced the overall read/write memory requirements by as much as a factor of four to six."
          }
        },
        {
          "section": "method",
          "title": "4 Source Language Analysis",
          "sentences": {
            "44": "Translation is divided into the steps of analysis, transfer, and generation.",
            "45": "Source-language analysis is illustrated in Figure 3.",
            "46": "English analysis begins with tokenization and morphological analysis, which creates a lattice that contains lexical feature structures.",
            "47": "During multi-word matching, expressions from the multi-word lexicon (such as White House or take on) are detected in the word lattice, and new arcs with the appropriate lexical feature structures are added.",
            "48": "Lexical ambiguity reduction reduces the number of arcs in the word lattice.",
            "49": "This module carries out part-of-speech tagging over the lattice, and reduces the lattice to those lexical feature structures that are part of the number of best paths that represents the best speed/accuracy trade-off (currently two).",
            "50": "This calculation is based on the usual lexical and contextual bigram probabilities that were estimated from a training corpus, but it also takes into account manual costs that can be added to lexicon entries, or to individual part-of-speech bigrams.",
            "51": "The resulting reduced lattice with lexical single-word and multi-word feature structures is parsed using the GLR parsing algorithm extended to lattice input (Tomita 1986).",
            "52": "The English parsing grammar consists of 540 GPL rules.",
            "53": "The output is a sentential feature structure that represents the input to the transfer component."
          }
        },
        {
          "section": "method",
          "title": "5 Transfer",
          "sentences": {
            "54": "Transfer from the source-language sentential feature structure to the target-language sentential feature structure is accomplished with a hybrid rule-based and example-based method.",
            "55": "This is illustrated in Figure 4.",
            "56": "The input feature structure is passed to the linguistic transfer procedure.",
            "57": "This consists of a rule-rewriting software engine that executes the compiled English-to-Japanese transfer grammar.",
            "58": "The transfer grammar consists of 140 GPL rules, and its job is to specify linguistic constraints on examples, combine multiple examples, transfer information that is beyond the scope of the example database, and perform various other transformations.",
            "59": "The overall effect is to broaden the linguistic coverage, and to raise the grammatical accuracy far beyond the level of a traditional example-based transfer procedure.",
            "60": "The linguistic transfer procedure operates on the input feature structure in a recursive manner, and it invokes the example matching procedure to find the best translation example for various parts of the input.",
            "61": "The example matching procedure retrieves the best translation examples from the example database, which contains 14,000 example pairs ranging from individual words to entire sentences.",
            "62": "In an off-line step, the example pairs are parsed, disambiguated, and indexed for corresponding constituents using a Treebanking tool.",
            "63": "At each invocation of the example matching procedure, linguistic constraints from the transfer grammar are used to limit the search space to appropriate examples.",
            "64": "In an off-line step, these constraints are pre-compiled into a complex index that allows a preliminary fast match.",
            "65": "Examples that survive the fast match are matched and aligned with the input feature structure (or sub-feature-structure, during recursive invocations) using the thesaurus to calculate word similarity, and using various other constraints and costs for inserting, deleting, or altering slots and features.",
            "66": "Rather than rely on the exact distance in the thesaurus to calculate lexical similarity, we use a scheme that is based on the information content of thesaurus nodes, similar to (Resnik 1995)."
          }
        },
        {
          "section": "method",
          "title": "6 Target-language Generation",
          "sentences": {
            "67": "The Japanese target-language feature structure forms the input to the generation module, which is summarized in Figure 5 below.",
            "68": "This module also consists of a rule-rewriting software engine, executing the compiled GPL Japanese generation grammar, which consists of 200 GPL rules.",
            "69": "The generator uses the Japanese lexicon to create the Japanese target-language expression."
          }
        },
        {
          "section": "conclusions",
          "title": "7 Evaluation and Conclusions",
          "sentences": {
            "70": "We evaluated the translation system using a random 500-expression sample from the unseen test set (see Section 2 above).",
            "71": "The translations were manually assigned to one of the following categories of translation quality: Failure.",
            "72": "Complete translation failure, due to lack of coverage of a rule-based component.",
            "73": "Wrong. A translation that is completely wrong, or that has major errors in an important part, such as in the main clause.",
            "74": "Major Problem.",
            "75": "A translation that has a missing, extra, or incorrect constituent, such as a subject, object, or adjectival/prepositional predicate.",
            "76": "Minor Problem.",
            "77": "A translation that has a missing, extra, or incorrect minor part, such as an intensifier, tense, aspect, temporal or locative adjunct, adverb, adjective or other prenominal modifier, prepositional phrase, verb conjugation form, adjective form, or required word or constituent order.",
            "78": "Stylistic Problem.",
            "79": "Stylistic problems include awkward but tolerable word order, incorrect Japanese particles, incorrect idioms, and similar.",
            "80": "Flawless. A translation that does not exhibit any of the above problems is considered flawless.",
            "81": "The results of the evaluation are shown in Table 1 below.",
            "82": "Overall, 84% of the translations convey the meaning in an acceptable manner.",
            "83": "We also evaluated the computational resource requirements of the system.",
            "84": "On a Pentium III running at 500 MHz, the average translation speed was 0.44 seconds.",
            "85": "The memory requirements are summarized in Table 2 below.",
            "86": "Our plans for further work include extending the size of the input vocabulary, and developing mechanisms for closer integration with speech recognition and speech synthesis components for speech-to-speech translation.",
            "87": "We are also working on the Japanese-to-English translation direction, and we plan to report results on this in the future."
          }
        },
        {
          "section": "acknowledgments",
          "title": "Acknowledgements",
          "sentences": {
            "88": "Our thanks go to Robert Bowen, Benjamin Hartwell, Chigusa Inaba, Kaori Shibatani, Hirono Stonelake, and Kazue Watanabe for their language engineering efforts, and to Edward Ho for user interface and application development."
          }
        }
      ],
      "doc_id": 2,
      "actual_doc_id": "C00-2152-parscit-section.xml"
    },
    {
      "content": [
        {
          "section": "abstract",
          "title": "Abstract",
          "sentences": {
            "0": "We propose a gold standard for evaluating two types of information extraction output -- noun phrase (NP) chunks (Abney 1991; Ramshaw and Marcus 1995) and technical terms (Justeson and Katz 1995; Daille 2000; Jacquemin 2002).",
            "1": "The gold standard is built around the notion that since different semantic and syntactic variants of terms are arguably correct, a fully satisfactory assessment of the quality of the output must include task-based evaluation.",
            "2": "We conducted an experiment that assessed subjects’ choice of index terms in an information access task.",
            "3": "Subjects showed significant preference for index terms that are longer, as measured by number of words, and more complex, as measured by number of prepositions.",
            "4": "These terms, which were identified by a human indexer, serve as the gold standard.",
            "5": "The experimental protocol is a reliable and rigorous method for evaluating the quality of a set of terms.",
            "6": "An important advantage of this task-based evaluation is that a set of index terms which is different than the gold standard can ‘win’ by providing better information access than the gold standard itself does.",
            "7": "And although the individual human subject experiments are time consuming, the experimental interface, test materials and data analysis programs are completely re-usable."
          }
        },
        {
          "section": "introduction",
          "title": "1 Introduction",
          "sentences": {
            "8": "The standard metrics for evaluation of the output of NLP systems are precision and recall.",
            "9": "Given an arguably correct list of the units that a system would identify if it performed perfectly, there should in principle be no discrepancy between the units identified by a system and the units that are either useful in a particular application or are preferred by human beings for use in a particular task.",
            "10": "But when the satisfactory output can take many different forms, as in summarization and generation, evaluation by precision and recall is not sufficient.",
            "11": "In these cases, the challenge for system designers and users is to effectively distinguish between systems that provide generally satisfactory output and systems that do not.",
            "12": "NP chunks (Abney 1991; Ramshaw and Marcus 1995; Evans and Zhai 1996; Frantzi and Ananiadou 1996) and technical terms (Dagan and Church 1994; Justeson and Katz 1995; Daille 1996; Jacquemin 2001; Bourigault et al. 2002) fall into this difficult-toassess category.",
            "13": "NPs are recursive structures.",
            "14": "For the maximal NP large number of recent newspaper articles on biomedical science and clinical practice, a fullfledged parser would legitimately identify (at least) seven NPs in addition to the maximal one: large number; recent newspaper articles; large number of recent newspaper articles; biomedical science; clinical practice; biomedical science and clinical practice; and recent newspaper articles on biomedical science and clinical practice.",
            "15": "To evaluate the performance of a parser, NP chunks can usefully be evaluated by a gold standard; many systems (e.g.",
            "16": ", Ramshaw and Marcus 1995 and Cardie and Pierce 1988) use the Penn Treebank for this type of evaluation.",
            "17": "But for most applications, output that lists a maximal NP and each of its component NPs is bulky and redundant.",
            "18": "Even a system that achieves 100% precision and recall in identifying all of the NPs in a document needs criteria for determining which units to use in different contexts or applications.",
            "19": "Technical terms are a subset of NP chunks.",
            "20": "Jacquemin (2001:3) defines terms as multi-word “vehicles of scientific and technical information”.1 The operational difficulty, of course, is to decide whether a specific term is a vehicle of scientific and technical information (e.g.",
            "21": ", birth date or light truck).",
            "22": "Evaluation of mechanisms that filter out some terms while retaining others is subject to this difficulty.",
            "23": "This is exactly the kind of case where context plays a significant role in deciding whether a term conforms to a definition and where experts disagree.",
            "24": "In this paper, we turn to an information access task in order to assess terms identified by different techniques.",
            "25": "There are two basic types of information access mechanisms, searching and browsing.",
            "26": "In searching, the user generates the search terms; in lected terms; by this measure the subjects’ preference for the human terms was more than 7 times greater than the preference for either of the automatic techniques.",
            "27": "(In Table 1 and in the rest of this paper, all index term counts are by type rather than by token, unless otherwise indicated).",
            "28": "subjects relative to number of terms in the entire index.",
            "29": "This initial experiment strongly indicates that 1) people have a demonstrable preference for different types of index terms; 2) these human terms are a very good gold standard.",
            "30": "If subjects use a greater proportion of the terms identified by a particular technique, the terms can be judged better than the terms identified by another technique, even if the terms are different.",
            "31": "Any automatic technique capable of identifying terms that are preferred over these human terms would be a very strong system indeed.",
            "32": "Furthermore, the properties of the terms preferred by the experimental subjects can be used to guide design of systems for identifying and selecting NP chunks and technical terms.",
            "33": "In the next section, we describe the design of the experiment and in Section 3, we report on what the experimental data shows about human preferences for different kinds of index terms."
          }
        },
        {
          "section": "method",
          "title": "2 Experimental design",
          "sentences": {
            "34": "Our experiment assesses the index terms vis a vis their usefulness in a strictly controlled information access task.",
            "35": "Subjects responded to a set of questions whose answers were contained in a 350 page collegelevel text (Rice, Ronald E., McCreadie, Maureen and Chang, Shan-ju L. (2001) Accessing and Browsing Information and Communication.",
            "36": "Cambridge, MA: MIT Press).",
            "37": "Subjects used the Experimental Searching and Browsing Interface (ESBI) which forces them to access text via the index terms; direct text searching was prohibited.",
            "38": "25 subjects participated in the experiment; they were undergraduate and graduate students at Rutgers University.",
            "39": "The experiments were conducted by graduate students at the Rutgers University School of Communication, Information and Library Studies (SCILS).",
            "40": "browsing, the user recognizes potentially useful terms from a list of terms presented by the system.",
            "41": "When an information seeker can readily think up a suitable term or linguistic expression to represent the information need, direct searching of text by user-generated terms is faster and more effective than browsing.",
            "42": "However, when users do not know (or can’t remember) the exact expression used in relevant documents, they necessarily struggle to find relevant information in full-text search systems.",
            "43": "Experimental studies have repeatedly shown that information seekers use many different terms to describe the same concept and few of these terms are used frequently (Furnas et al. 1987; Saracevic et al. 1988; Bates et al. 1998).",
            "44": "When information seekers are unable to figure out the term used to describe a concept in a relevant document, electronic indexes are required for successful information access.",
            "45": "NP chunks and technical terms have been proposed for use in this task (Boguraev and Kennedy 1997; Wacholder 1998).",
            "46": "NP chunks and technical terms have also been used in phrase browsing and phrase hierarchies (Jones and Staveley 1999; NevillManning et al. 1999; Witten et al. 1999; Lawrie and Croft 2000) and summarization (e.g.",
            "47": ", McKeown et al. 1999; Oakes and Paice 2001).",
            "48": "In fact, the distinction between task-based evaluation of a system and precision/recall evaluation of the quality of system output is similar to the extrinsic/intrinsic evaluation of summarization (Gallier and Jones 1993).",
            "49": "In order to focus on the subjects’ choice of index terms rather than on other aspects of the information access process, we asked subject to find answers to questions in a college level text book.",
            "50": "Subjects used the Experimental Searching and Browsing Interface (ESBI) to browse a list of terms that were identified by different techniques and then merged.",
            "51": "Subjects select an index term by clicking on it in order to hyperlink to the text itself.",
            "52": "By design, ESBI forces the subjects to access the text indirectly, by searching and browsing the list of index terms, rather than by direct searching of the text.",
            "53": "Three sets of terms were used in the experiment: one set (HS) was identified using the head-sorting method of Wacholder (1998); the second set (TT) was identified by an implementation of the technical term algorithm of Justeson and Katz (1995); a third set (HUM) was created by a human indexer.",
            "54": "The methods for identifying these terms will be discussed in greater detail below.",
            "55": "Somewhat to our surprise, subjects displayed a very strong preference for the index terms that were identified by the human indexer.",
            "56": "Table 1 shows that when measured by percentage terms selected, subjects chose over 13% of the available human terms, but only 1.73% and 1.43% of the automatically seSubjects used the Experimental Searching and Browsing Interface (ESBI) to find the answers to the questions.",
            "57": "After an initial training session, ESBI presents the user with a Search/Browse screen (not shown); the question appears at the top of the screen.",
            "58": "The subject may enter a string to search for in the index, or click on the \"Browse\" button for access to the whole index.",
            "59": "At this point, \"search\" and \"browse\" apply only to the list of index terms, not to the text.",
            "60": "The user may either browse the entire list of index terms or may enter a search term and specify criteria to select the subset of terms that will be returned.",
            "61": "Most people begin with the latter option because the complete list of index terms is too long to be easily browsed.",
            "62": "The user may select (click on) an index term to view a list of the contexts in which the term appears.",
            "63": "If the context appears useful, the user may choose to view the term in its full context; if not, the user may either do additional browsing or start the process over again.",
            "64": "Figure 1 shows a screen shot of ESBI after the searcher has entered the string democracy in the search box.",
            "65": "This view shows the demo question and the workspace for entering answers.",
            "66": "The string was (previously) entered in the search box and all index terms that include the word democracy are displayed.",
            "67": "Although it is not illustrated here, ESBI also permits substring searching and the option to specify case sensitivity.",
            "68": "Regardless of the technique by which the term was identified, terms are organized by grammatical head of the phrase.",
            "69": "Preliminary analysis of our results has shown that most subjects like this analysis, which resembles standard organization of back-of-the-book indexes.",
            "70": "Readers may notice that the word participation appears at the left-most margin, where it represents the set of terms whose head is participation.",
            "71": "The indented occurrence represents the individual term.",
            "72": "Selecting the left-most occurrence brings up contexts for all phrases for which participation is a head.",
            "73": "Selecting on the indented occurrence brings up contexts for the noun participation only when it is not part of a larger phrase.",
            "74": "This is explained to subjects during the pre-experimental training and an experimenter is present to remind subjects of this distinction if a question arises during the experiment.",
            "75": "Readers may also notice that in Figure 1, one of the terms, participation require, is ungrammatical.",
            "76": "This particular error was caused by a faulty part-ofspeech tag.",
            "77": "But since automatically identified index terms typically include some nonsensical terms, we have left these terms in – these terms are one of the problems that information seekers have to cope with in a realistic task-based evaluation.",
            "78": "After conducting initial testing to find out what types of questions subjects founder hard or easy, we spent considerable effort to design a set of 26 questions of varying degrees of difficulty.",
            "79": "To obtain an initial assessment of difficulty, one of the experimenters used ESBI to answer all of the questions and rate each question with regard to how difficult it was to answer using the ESBI system.",
            "80": "For example, the question What are the characteristics of Marchionini's model of browsing? was rated very easy because searching on the string marchionini reveals an index term Marchionini's which is linked to the text sentence: Marchionini's model of browsing considers five interactions among the informationseeking factors of \"task, domain, setting, user characteristics and experience, and system content and interface\" (p.107).",
            "81": "The question What factors determine when users decide to stop browsing? was rated very difficult because searching on stop (or synonyms such as halt, cease, end, terminate, finish, etc).",
            "82": "reveals no helpful index terms, while searching on factors or browsing yields an avalanche of over 500 terms, none with any obvious relevance.",
            "83": "After subjects finished answering each question, they were asked to rate the question in terms of its difficulty.",
            "84": "A positive correlation between judgments of the experimenters and the experimental subjects (Sharp et al., under submission) confirmed that we had successfully devised questions with a range of difficulty.",
            "85": "In general, questions that included terms actually used in the index were judged easier; questions where the user had to devise the index terms were judged harder.",
            "86": "To avoid effects of user learning, questions were presented to subjects in random order; in the one hour experiment, subjects answered an average of about 9 questions.",
            "87": "Although the primary goal of this research is to point the way to improved techniques for automatic creation of index terms, we used human created terms to create a baseline.",
            "88": "For the human index terms, we used the pre-existing back-of-the-book index, which we believe to be of high quality.2 The two techniques for automatic identification were the technical terms algorithm of Justeson and Katz (1995) and the head sorting method (Dagan and Church (1994); Wacholder (1998).",
            "89": "In the implementation of the Justeson and Katz’ algorithm, technical terms are multi-word NPs repeated above some threshold in a corpus; in the head sorting method, technical terms are identified by grouping noun phrases with a common head (e.g.",
            "90": ", health-care workers and asbestos workers), and selecting as terms those NPs whose heads appear in two or more phrases.",
            "91": "Definitionally, technical terms are a proper subset of terms identified by Head Sorting.",
            "92": "Differences in the implementations, especially the preprocessing module, result in there being some terms identified by Termer that were not identified by Head Sorting.",
            "93": "Table 2 shows the number of terms identified by each method.",
            "94": "(*Because some terms are identified by more than one technique, the percentage adds up to more than 100%).",
            "95": "The fewest terms (673) were identified by the human method; in part this reflects the judgment of the indexer and in part it is a result of restrictions on index length in a printed text.",
            "96": "The largest number of terms (7980) was identified by the head sorting method.",
            "97": "This is because it applies looser criteria for determining a term than does the Justeson and Katz algorithm which imposes a very strict standard--no single word can be considered a term, and an NP must be repeated in full to be considered a term.",
            "98": "Wacholder et al.",
            "99": "(2000) showed that when experimental subjects were asked to assess the usefulness of terms for an information access task without actually using the terms for information access showed that the terms identified by the technical term algorithm, which are considerably fewer than the terms identified by head sorting, were overall of higher quality than the terms identified by the head sorting method.",
            "100": "However, the fact that subjects assigned a high rank to many of the terms identified by Head Sorting suggested that the technical term algorithm was failing to pick up many potentially useful index terms.",
            "101": "In preparation for the experiment, all index terms were merged into a single list and duplicates were removed, resulting in a list of nearly 10,000 index terms.",
            "102": "In the experiment, we logged the terms that subjects searched for (i.e.",
            "103": ", entered in a search box) and selected.",
            "104": "In this paper, we report only on the terms that the subjects selected (i.e.",
            "105": ", clicked on).",
            "106": "This is because if a subject entered a single word, or a subpart of a word in the search box, ESBI returned to them a list of index terms; the subject then selected a term to view the context in which it appears in the text.",
            "107": "This term might have been the same term originally searched for or it might have been a superstring.",
            "108": "The terms that subjects selected for searching are interesting in their own right, but are not analyzed here."
          }
        },
        {
          "section": "method",
          "title": "3 Results",
          "sentences": {
            "109": "At the outset of this experiment, we did not know whether it would be possible to discover differences in human preferences for terms in the information access task reported on in this paper.",
            "110": "We therefore started our research with the null hypothesis that all index terms are created equal.",
            "111": "If users selected index terms in roughly the same proportion as the terms occur in the text, the null hypothesis would be proven.",
            "112": "The results strongly discredit the null hypothesis.",
            "113": "Table 3 shows that when measured by percentage of terms selected, subjects selected on over 13.2% of the available human terms, but only 1.73% and 1.43% respectively of the automatically selected terms.",
            "114": "Table 3 also shows that although the human index terms formed only 6% of the total number of index terms, 40% of the terms which were selected by subjects in order to view the context were identified by human indexing.",
            "115": "Although 80% of the index terms were identified by head sorting, only 51% of the terms subjects chose to select had been identified by this method.",
            "116": "(*Because of overlap of terms selected by different techniques, total is greater than 100%) To determine whether the numbers represent statistically significant evidence that the null hypothesis is wrong, we represent the null hypothesis (HT)) as (1) and the falsification of the null hypothesis (HA) as (2).",
            "117": "Pi is the expected percentage of the selected terms that are type i in all the selected terms; µi is the expected percentage if there is no user preference, i.e. the proportion of this term type i in all the terms.",
            "118": "We rewrite the above as (3).",
            "119": "Assuming that X is normally distributed, we can use a one-sample t test on X to decide whether to accept the hypothesis (1).",
            "120": "The two-tailed t test (df =222) produces a p-value of less than.01% for the comparison of the expected and selected proportions of a) human terms and head sorted terms and b) human terms and technical terms.",
            "121": "In contrast, the p-value for the comparison of head-sorted and technical terms was 33.7%, so we draw no conclusions about relative preferences for head sorted and technical terms.",
            "122": "We also considered the possibility that our formulation of questions biased the terms that the subjects selected, perhaps because the words of the questions overlapped more with the terms selected by one of the methods.",
            "123": "3 We took the following steps: As measured by the terms the subjects saw during browsing, 22% were human terms, 62% were head sorted terms and 16% were technical terms.",
            "124": "Using the same reasoning about the null hypothesis as above, the p-value for the comparison of the ratios of human and head sorted terms was less than 0.01%, as was the comparison of the ratios of the human and technical terms.",
            "125": "This supports the validity of the results of the initial test.",
            "126": "In contrast, the p-value for the comparison of the two automatic techniques was 77.3%.",
            "127": "Why did the subjects demonstrate such a strong preference for the human terms?",
            "128": "Table 4 illustrates some important differences between the human terms and the automatically identified terms.",
            "129": "The terms selected on are longer, as measured in number of words, and more complex, as measured by number of prepositions per index terms and by number of content-bearing words.",
            "130": "As shown in Table 5, the difference of these complexity measures between human terms and automatically identified terms are statistically significant.",
            "131": "Since longer terms are more specific than shorter terms (for example, participation in a democracy is longer and more specific than democracy), the results suggest that subjects prefer the more specific terms.",
            "132": "If this result is upheld in future research, it has practical implications for the design of automatic term identification systems.",
            "133": "In this paper, our primary focus is on the question of what makes index terms 'better', as measured by user preferences in a question-answering task.",
            "134": "Also of interest, of course, is what makes index terms 'better' in terms of how accurate the resulting users' answers are.",
            "135": "The problem is that any facile judgment of freetext answer accuracy is bound to be arbitrary and potentially unreliable; we discuss this in detail in [26].",
            "136": "Nevertheless, we address the issue in a preliminary way in the current paper.",
            "137": "We used an ad hoc set of canonical answers to score subjects' answers on a scale of 1 to 3, where 1 stands for 'very accurate', 2 stands for 'partly accurate' and 3 represents 'not at all accurate'.",
            "138": "Using general loglinear regression (Poisson model) under the hypothesis that these two variables are independent of each other, our analysis showed that there is a systematic relationship (significance probability is 0.0504) between source of selected terms and answer accuracy.",
            "139": "Specifically, in cases where subjects used more index terms identified by the human indexer, the answers were more accurate.",
            "140": "On the basis of our initial accuracy judgments, we can therefore draw the preliminary conclusion that terms that were better in that they were preferred by the experimental subjects were also better in that they were associated with better answers.",
            "141": "We plan to conduct a more in-depth analysis of answer accuracy and will report on it in future work.",
            "142": "But the primary question addressed in this paper is how to reliably assess NP chunks and technical terms.",
            "143": "These results constitute experimental evidence that the index terms identified by the human indexer constitute a gold standard, at least for the text used in the experiment.",
            "144": "Any set of index terms, regardless of the technique by which they were created or the criteria by they were selected, can be compared vis a vis their usefulness in the information access task ."
          }
        },
        {
          "section": "method",
          "title": "4 Discussion",
          "sentences": {
            "145": "The contribution of this paper is the description of a task-based gold-standard method for evaluating the usefulness and therefore the quality of NP chunks and technical terms.",
            "146": "In this section, we address a number of questions about this method.",
            "147": "experiments in which terms with errors are include in the set of test terms, the impact of these errors can be measured.",
            "148": "The usefulness of a set of terms presumably is at least in part a function of the impact of the errors, whether the errors are a by-product of the algorithm or the implementation of the algorithm.",
            "149": "2) Could the set of human index terms be used as a gold standard without conducting the human subject experiments?",
            "150": "This of course could be done, but then the terms are being evaluated by a fixed standard – by definition, no set of terms can do better than the gold standard.",
            "151": "This experimental method leaves open the possibility that there is a set of terms that is better than the gold standard.",
            "152": "In this case, of course, the gold standard would no longer be a gold standard -- perhaps we would have to call it a platinum standard.",
            "153": "3) How reproducible is the experiment?",
            "154": "The experiment can be re-run with any set of terms deemed to be representative of the content of the Rice text.",
            "155": "The preparation of the materials for additional texts is admittedly time-consuming.",
            "156": "But over time a sizable corpus of experimental materials in different domains could be built up.",
            "157": "These materials could be used for training as well as for testing.",
            "158": "4) How extensible is the gold standard?",
            "159": "The experimental protocol will be validated only if equally useful index terms can be created for other texts.",
            "160": "We anticipate that they can.",
            "161": "5) How can this research help in the design of real world NLP systems?",
            "162": "This technique can help in assessing the relative usefulness of existing techniques for identifying terms.",
            "163": "It is possible, for example, there already exist techniques for identifying terms that are superior to the two tested here.",
            "164": "If we can find such systems, their algorithms should be preferred.",
            "165": "If not, there remains a need for development of algorithms to identify single word terms and complex phrases.",
            "166": "6) Do the benefits of this evaluation technique outweigh the costs?",
            "167": "Given the fundamental difficulty of evaluating NP chunks and technical terms, task-based evaluation is a promising supplement to evaluation by precision and recall.",
            "168": "These relatively time-consuming human subject experiments surely will not be undertaken by most system developers; ideally, they should be performed by neutral parties who do not have a stake in the outcome.",
            "169": "7) Should automated indexes try to imitate human indexers?",
            "170": "Automated indexes should contain terms that are most easily processed by users.",
            "171": "If the properties of such terms can be reliably discovered, developers of systems that identify terms intended to be processed by people surely should pay attention."
          }
        },
        {
          "section": "conclusions",
          "title": "5 Conclusion",
          "sentences": {
            "172": "In this paper we have reported on a rigorous experimental technique for black-box evaluation of the usefulness of NP chunks and technical terms in an information access task.",
            "173": "Our experiment shows that it is possible to reliably identify human preferences for sets of terms.",
            "174": "The set of human terms created for use in a backof-the-book index serves as a gold standard.",
            "175": "An advantage of the task-based evaluation is that a set of terms could outperform the gold standard; any system that could do this would be a good system indeed.",
            "176": "The two automatic methods that we evaluated performed much less well than the terms created by the human indexer; we plan to evaluate additional techniques for term identification in the hope of identifying automatic methods that identify index terms that people prefer over the human terms.",
            "177": "We also plan to prepare test materials in different domains, and assess in greater depth the properties of the terms that our experimental subjects preferred; our goal is to develop practical guidelines for the identification and selection of technical terms that are optimal for human users.",
            "178": "We will also study the impact of semantic differences between terms on user preferences and investigate whether terms which are preferred for information access are equally suitable for other NLP tasks."
          }
        },
        {
          "section": "acknowledgments",
          "title": "6 Acknowledgements",
          "sentences": {
            "179": "We are grateful to the other members of the Rutgers NLP-I research group, Lu Liu, Mark Sharp, and Xiaojun Yuan, for their valuable contribution to this project.",
            "180": "We also thank Paul Kantor, Judith L. Klavans, Evelyne Tzoukermann, Min Yen Kan, and three anonymous reviewers for their helpful suggestions.",
            "181": "Funding for this research has been provided by the Rutgers University Information Science and Technology Council."
          }
        }
      ],
      "doc_id": 3,
      "actual_doc_id": "N03-1035-parscit-section.xml"
    },
    {
      "content": [
        {
          "section": "abstract",
          "title": "Abstract",
          "sentences": {
            "0": "This paper describes the conversion of a Hidden Markov Model into a sequential transducer that closely approximates the behavior of the stochastic model.",
            "1": "This transformation is especially advantageous for part-of-speech tagging because the resulting transducer can be composed with other transducers that encode correction rules for the most frequent tagging errors.",
            "2": "The speed of tagging is also improved.",
            "3": "The described methods have been implemented and successfully tested on six languages."
          }
        },
        {
          "section": "introduction",
          "title": "1 Introduction",
          "sentences": {
            "4": "Finite-state automata have been successfully applied in many areas of computational linguistics.",
            "5": "This paper describes two algorithms' which approximate a Hidden Markov Model (HMM) used for part-of-speech tagging by a finite-state transducer (FST).",
            "6": "These algorithms may be useful beyond the current description on any kind of analysis of written or spoken language based on both finite-state technology and HMMs, such as corpus analysis, speech recognition, etc.",
            "7": "Both algorithms have been fully implemented.",
            "8": "An HMM used for tagging encodes, like a transducer, a relation between two languages.",
            "9": "One language contains sequences of ambiguity classes obtained by looking up in a lexicon all words of a sentence.",
            "10": "The other language contains sequences of tags obtained by statistically disambiguating the class sequences.",
            "11": "From the outside, an HMM tagger behaves like a sequential transducer that deterministically 'There is a different (unpublished) algorithm by Julian M. Kupiec and John T. Maxwell (p.c.).",
            "12": "maps every class sequence to a tag sequence, e.g.: [DET, PRO] [ADJ, NOUN] [ADJ, NOUN] [END] The aim of the conversion is not to generate FSTs that behave in the same way, or in as similar a way as possible like HMMs, but rather FSTs that perform tagging in as accurate a way as possible.",
            "13": "The motivation to derive these FSTs from HMMs is that HMMs can be trained and converted with little manual effort.",
            "14": "The tagging speed when using transducers is up to five times higher than when using the underlying HMMs.",
            "15": "The main advantage of transforming an HMM is that the resulting transducer can be handled by finite state calculus.",
            "16": "Among others, it can be composed with transducers that encode: • correction rules for the most frequent tagging errors which are automatically generated (Brill, 1992; Roche and Schabes, 1995) or manually written (Chanod and Tapanainen, 1995), in order to significantly improve tagging accuracy2.",
            "17": "These rules may include long-distance dependencies not handled by HMM taggers, and can conveniently be expressed by the replace operator (Kaplan and Kay, 1994; Karttunen, 1995; Kempe and Karttunen, 1996).",
            "18": "These compositions enable complex text analysis to be performed by a single transducer.",
            "19": "An HMM transducer builds on the data (probability matrices) of the underlying HMM.",
            "20": "The accuracy 2Automatically derived rules require less work than manually written ones but are unlikely to yield better results because they would consider relatively limited context and simple relations only.",
            "21": "of this data has an impact on the tagging accuracy of both the HMM itself and the derived transducer.",
            "22": "The training of the HMM can be done on either a tagged or untagged corpus, and is not a topic of this paper since it is exhaustively described in the literature (Bahl and Mercer, 1976; Church, 1988).",
            "23": "An HMM can be identically represented by a weighted FST in a straightforward way.",
            "24": "We are, however, interested in non-weighted transducers."
          }
        },
        {
          "section": "method",
          "title": "2 n-Type Approximation",
          "sentences": {
            "25": "This section presents a method that approximates a (1st order) HMM by a transducer, called n-type approximation3.",
            "26": "Like in an HMM, we take into account initial probabilities it, transition probabilities a and class (i.e.",
            "27": "observation symbol) probabilities b.",
            "28": "We do, however, not estimate probabilities over paths.",
            "29": "The tag of the first word is selected based on its initial and class probability.",
            "30": "The next tag is selected on its transition probability given the first tag, and its class probability, etc.",
            "31": "Unlike in an HMM, once a decision on a tag has been made, it influences the following decisions but is itself irreversible.",
            "32": "A transducer encoding this behaviour can be generated as sketched in figure 1.",
            "33": "In this example we have a set of three classes, c1 with the two tags ti and /12, C2 with the three tags 121,122 and /23, and C3 with one tag t31.",
            "34": "Different classes may contain the same tag, e.g. 112 and 123 may refer to the same tag.",
            "35": "For every possible pair of a class and a tag (e.g.",
            "36": "c1 : t12 or [MIT,NOUN] :NOUN) a state is created and labelled with this same pair (fig.",
            "37": "1). An initial state which does not correspond with any pair, is also created.",
            "38": "All states are final, marked by double circles.",
            "39": "' For every state, as many outgoing arcs are created as there are classes (three in fig.",
            "40": "1). Each such arc for a particular class points to the most probable pair of this same class.",
            "41": "If the arc comes from the initial state, the most probable pair of a class and a tag (destination state) is estimated by: arg maxpi (ci, tik ) = r(ilk) b(ciltik) (2) If the arc comes from a state other than the initial state, the most probable pair is estimated by: arg max p2(ci, ilk) = a(tik tprevious) b(ci itik) (3) In the example (fig.",
            "42": "1) c1 :112 is the most likely pair of class ci, and c2 : t23 the most likely pair of class c2 3Name given by the author.",
            "43": "when coming from the initial state, and c2 :121 the most likely pair of class c2 when coming from the state of c3 :t31.",
            "44": "Every arc is labelled with the same symbol pair as its destination state, with the class symbol in the upper language and the tag symbol in the lower language.",
            "45": "E.g. every arc leading to the state of c1 :112 is labelled with ci :t12.",
            "46": "Finally, all state labels can be deleted since the behaviour described above is encoded in the arc labels and the network structure.",
            "47": "The network can be minimized and determinized.",
            "48": "We call the model an ra-type model, the resulting FST an nl-type transducer and the algorithm leading from the HMM to this transducer, an nl-type approximation of a 1st order HMM.",
            "49": "Adapted to a 2nd order HMM, this algorithm would give an n2-type approximation.",
            "50": "Adapted to a zero order HMM, which means only to use class probabilities b, the algorithm would give an nO-type approximation.",
            "51": "n-Type transducers have deterministic states only."
          }
        },
        {
          "section": "method",
          "title": "3 s-Type Approximation",
          "sentences": {
            "52": "This section presents a method that approximates an HMM by a transducer, called s-type approximation'.",
            "53": "Tagging a sentence based on a 1st order HMM includes finding the most probable tag sequence T given the class sequence C of the sentence.",
            "54": "The joint probability of C and T can be estimated by: The decision on a tag of a particular word cannot be made separately from the other tags.",
            "55": "Tags can influence each other over a long distance via transition probabilities.",
            "56": "Often, however, it is unnecessary to decide on the tags of the whole sentence at once.",
            "57": "In the case of a 1st order HMM, unambiguous classes (containing one tag only), plus the sentence beginning and end positions, constitute barriers to the propagation of HMM probabilities.",
            "58": "Two tags with one or more barriers inbetween do not influence each other's probability.",
            "59": "4 Name given by the author.",
            "60": "To tag a sentence, one can split its class sequence at the barriers into subsequences, then tag them separately and concatenate them again.",
            "61": "The result is equivalent to the one obtained by tagging the sentence as a whole.",
            "62": "We distinguish between initial and middle subsequences.",
            "63": "The final subsequence of a sentence is equivalent to a middle one, if we assume that the sentence end symbol (.",
            "64": "or ! or)? always corresponds to an unambiguous class eu.",
            "65": "This allows us to ignore the meaning of the sentence end position as an HMM barrier because this role is taken by the unambiguous class cu at the sentence end.",
            "66": "An initial subsequence Ci starts with the sentence initial position, has any number (incl.",
            "67": "zero) of ambiguous classes co and ends with the first unambiguous class ce, of the sentence.",
            "68": "It can be described by the regular expression5: The joint probability of an initial class subsequence Ci of length r, together with an initial tag subsequence T2, can be estimated by: A middle subsequence Cm starts immediately after an unambiguous class Cu, has any number (incl.",
            "69": "5Regular expression operators used in this section are explained in the annex.",
            "70": "zero) of ambiguous classes co and ends with the following unambiguous class Cu: For correct probability estimation we have to include the immediately preceding unambiguous class Cu, actually belonging to the preceding subsequence Ci or Cm.",
            "71": "We thereby obtain an extended middle subsequence5: The joint probability of an extended middle class subsequence Cine of length s, together with a tag subsequence T, can be estimated by: To build an s-type transducer, a large number of initial class subsequences Ci and extended middle class subsequences C,e.„ are generated in one of the following two ways: (a) Extraction from a corpus Based on a lexicon and a guesser, we annotate an untagged training corpus with class labels.",
            "72": "From every sentence, we extract the initial class subsequence Ci that ends with the first unambiguous class ct, (eq.",
            "73": "5), and all extended middle subsequences C,.en ranging from any unambiguous class C (in the sentence) to the following unambiguous class (eq.",
            "74": "8). A frequency constraint (threshold) may be imposed on the subsequence selection, so that the only subsequences retained are those that occur at least a certain number of times in the training corpus6.",
            "75": "(b) Generation of possible subsequences Based on the set of classes, we generate all possible initial and extended middle class subsequences, Ci and Cg.",
            "76": ", (eq.",
            "77": "5, 8) up to a defined length.",
            "78": "Every class subsequence Ci or Cg.",
            "79": ", is first disambiguated based on a 1st order HMM, using the Viterbi algorithm (Viterbi, 1967; Rabiner, 1990) for efficiency, and then linked to its most probable tag subsequence Tt or 7;7, by means of the cross product operation6: the first class symbol on the upper side and the first tag symbol on the lower side, will be marked as an extension that does not really belong to the middle sequence but which is necessary to disambiguate it correctly.",
            "80": "Example (12) becomes: We then build the union 'St of all initial subsequences Si and the union '5T72 of all extended middle subsequences.37%, and formulate a preliminary sentence model: uso _ usi usmo* (14) in which all middle subsequences smo are still marked and extended in the sense that all occurrences of all unambiguous classes are mentioned twice: Once unmarked as au at the end of every sequence Ci or Ct, and the second time marked as au° at the beginning of every following sequence Ct.",
            "81": "The upper side of the sentence model u50 describes the complete (but 6The frequency constraint may prevent the encoding of rare subsequences which would encrease the size of the transducer without contributing much to the tagging accuracy.",
            "82": "extended) class sequences of possible sentences, and the lower side of L'S° describes the corresponding (extended) tag sequences.",
            "83": "To ensure a correct concatenation of initial and middle subsequences, we formulate a concatenation constraint for the classes: stating that every middle subsequence must begin with the same marked unambiguous class cu° (e.g.",
            "84": "OIDET]) which occurs unmarked as cts (e.g.",
            "85": "[DET]) at the end of the preceding subsequence since both symbols refer to the same occurrence of this unambiguous class.",
            "86": "Having ensured correct concatenation, we delete all marked classes on the upper side of the relation by means of and all marked tags on the lower side by means of By composing the above relations with the preliminary sentence model, we obtain the final sentence mode16: S= De .o.",
            "87": "R .o.",
            "88": "USO .o.",
            "89": "Dt (18) We call the model an s-type model, the corresponding FST an s-type transducer, and the whole algorithm leading from the HMM to the transducer, an s-type approximation of an HMM.",
            "90": "The s-type transducer tags any corpus which contains only known subsequences, in exactly the same way, i.e. with the same errors, as the corresponding HMM tagger does.",
            "91": "However, since an s-type transducer is incomplete, it cannot tag sentences with one or more class subsequences not contained in the union of the initial or middle subsequences.",
            "92": "An incomplete s-type transducer S can be completed with subsequences from an auxiliary, complete ntype transducer N as follows: First, we extract the union of initial and the union of extended middle subsequences, suSi and 1,-)S,fle from the primary s-type transducer S, and the unions nuSi and „uStme from the auxiliary n-type transducer N. To extract the union L'S'i of initial subsequences we use the following filter: where (cu, t) is the 1-level format7 of the symbol pair cu :t.",
            "93": "The extraction takes place by where the transducer N is first converted into 1level format7, then composed with the filter Fs, (eq.",
            "94": "19). We extract the lower side of this composition, where every sequence of N.1L remains unchanged from the beginning up to the first occurrence of an unambiguous class cu.",
            "95": "Every following symbol is mapped to the empty string by means of [? : [ ]] (eq.",
            "96": "19). Finally, the extracted lower side is again converted into 2-level format7.",
            "97": "The extraction of the union uSr% of extended middle subsequences is performed in a similar way.",
            "98": "We then make the joint unions of initial and extended middle subsequences': In both cases (eq.",
            "99": "21 and 22) we union all subsequences from the principal model S, with all those subsequences from the auxiliary model N that are not in S. Finally, we generate the completed s+n-type transducer from the joint unions of subsequences uSi and `-'5„6, as decribed above (eq.",
            "100": "14-18). A transducer completed in this way, disambiguates all subsequences known to the principal incomplete s-type model, exactly as the underlying HMM does, and all other subsequences as the auxiliary n-type model does ."
          }
        },
        {
          "section": "method",
          "title": "4 An Implemented Finite-StateTagger",
          "sentences": {
            "101": "The implemented tagger requires three transducers which represent a lexicon, a guesser and any above mentioned approximation of an HMM.",
            "102": "All three transducers are sequential, i.e. deterministic on the input side.",
            "103": "Both the lexicon and guesser unambiguously map a surface form of any word that they accept to the corresponding class of tags (fig.",
            "104": "2, col.",
            "105": "1 and 2): 71-Level and 2-level format are explained in the annex.",
            "106": "First, the word is looked for in the lexicon.",
            "107": "If this fails, it is looked for in the guesser.",
            "108": "If this equally fails, it gets the label [UNKNOWN] which associates the word with the tag class of unknown words.",
            "109": "Tag probabilities in this class are approximated by tags of words that appear only once in the training corpus.",
            "110": "As soon as an input token gets labelled with the tag class of sentence end symbols (fig.",
            "111": "2: [SENT7), the tagger stops reading words from the input.",
            "112": "At this point, the tagger has read and stored the words of a whole sentence (fig.",
            "113": "2, col.",
            "114": "1) and generated the corresponding sequence of classes (fig.",
            "115": "2, col.",
            "116": "2). The class sequence is now deterministically mapped to a tag sequence (fig.",
            "117": "2, col.",
            "118": "3) by means of the HMM transducer.",
            "119": "The tagger outputs the stored word and tag sequence of the sentence, and continues in the same way with the remaining sentences of the corpus."
          }
        },
        {
          "section": "evaluation",
          "title": "5 Experiments and Results",
          "sentences": {
            "120": "This section compares different n-type and s-type transducers with each other and with the underlying HMM.",
            "121": "The FSTs perform tagging faster than the HMMs.",
            "122": "Since all transducers are approximations of HMMs, they give a lower tagging accuracy than the corresponding HMMs.",
            "123": "However, improvement in accuracy can be expected since these transducers can be composed with transducers encoding correction rules for frequent errors (sec.",
            "124": "1). Table 1 compares different transducers on an English test case.",
            "125": "The s+nl-type transducer containing all possible subsequences up to a length of three classes is the most accurate (table 1, last line, s+nl-FST (< 3): 95.95 %) but also the largest one.",
            "126": "A similar rate of accuracy at a much lower size can be achieved with the s+nl-type, either with all subsequences up to a length of two classes (s+nl-FST (< 2): 95.06%) or with subsequences occurring at least once in a training corpus of 100 000 words (s+nl-FST (100K, F1): 95.05 %).",
            "127": "Increasing the size of the training corpus and the frequency limit, i.e. the number of times that a subsequence must at least occur in the training corpus in order to be selected (sec.",
            "128": "3.2 a), improves the relation between tagging accuracy and the size of the transducer.",
            "129": "E.g. the s+nl-type transducer that encodes subsequences from a training corpus of 20 000 words (table 1, s+nl-FST (20K, F1): 94.74 %, 927 states, 203 853 arcs), performs less accurate tagging and is bigger than the transducer that encodes subsequences occurring at least eight times in a corpus of 1 000 000 words (table 1, s+nl-FST (1M, F8): 95.09 %, 432 states, 96 712 arcs).",
            "130": "Most transducers in table 1 are faster then the underlying HMM; the nO-type transducer about five times8.",
            "131": "There is a large variation in speed between 'Since nO-type and n1-type transducers have deterministic states only, a particular fast matching algorithm can be used for them.",
            "132": "the different transducers due to their structure and size.",
            "133": "Table 2 compares the tagging accuracy of different transducers and the underlying HMM for different languages.",
            "134": "In these tests the highest accuracy was always obtained by s-type transducers, either with all subsequences up to a length of two classes9 or with subsequences occurring at least once in a corpus of 100 000 words."
          }
        },
        {
          "section": "conclusions",
          "title": "6 Conclusion and Future Research",
          "sentences": {
            "135": "The two methods described in this paper allow the approximation of an HMM used for part-of-speech tagging, by a finite-state transducer.",
            "136": "Both methods have been fully implemented.",
            "137": "The tagging speed of the transducers is up to five times higher than that of the underlying HMM.",
            "138": "The main advantage of transforming an HMM is that the resulting FST can be handled by finite state calculusl° and thus be directly composed with other transducers which encode tag correction rules and/or perform further steps of text analysis.",
            "139": "Future research will mainly focus on this possibility and will include composition with, among others: An HMM transducer can be composed with one or more of these transducers in order to perform complex text analysis using only a single transducer.",
            "140": "We also hope to improve the n-type model by using look-ahead to the following tags11.",
            "141": "\"A large library of finite-state functions is available at Xerox.",
            "142": "liOngoing work has shown that, looking ahead to just one tag is worthless because it makes tagging results highly ambiguous."
          }
        },
        {
          "section": "acknowledgments",
          "title": "Acknowledgements",
          "sentences": {
            "143": "I wish to thank the anonymous reviewers of my paper for their valuable comments and suggestions.",
            "144": "I am grateful to Lauri Karttunen and Gregory Grefenstette (both RXRC Grenoble) for extensive and frequent discussion during the period of my work, as well as to Julian Kupiec (Xerox PARC) and Mehryar Mohri (AT&T Research) for sending me some interesting ideas before I started.",
            "145": "Many thanks to all my colleagues at RXRC Grenoble who helped me in whatever respect, particularly to Anne Schiller, Marc Dymetman and JeanPierre Chanod for discussing parts of the work, and to Irene Maxwell for correcting various versions of the paper."
          }
        }
      ],
      "doc_id": 4,
      "actual_doc_id": "P97-1059-parscit-section.xml"
    },
    {
      "content": [
        {
          "section": "abstract",
          "title": "ABSTRACT",
          "sentences": {
            "0": "This paper describes PEGASUS, a spoken language interface for on-line air travel planning that we have recently developed.",
            "1": "PEGASUS leverages off our spoken language technology development in the ATIS domain, and enables users to book flights using the American Airlines EAASY SABRE system.",
            "2": "The input query is transformed by the speech understanding system to a frame representation that captures its meaning.",
            "3": "The tasks of the System Manager include transforming the semantic representation into an EAASY SABRE command, transmitting it to the application backend, formatting and interpreting the resulting information, and managing the dialogue.",
            "4": "Preliminary evaluation results suggest that users can learn to make productive use of PEGASUS for travel planning, although much work remains to be done."
          }
        },
        {
          "section": "introduction",
          "title": "INTRODUCTION",
          "sentences": {
            "5": "Over the past few years, our group has participated, as a member of the ARPA Human Language Technology (HLT) research community, in the development of spoken language technology in the common domain called Air Travel Information Service, or ATIS [1].",
            "6": "ATIS permits users to verbally query for air travel information, such as flight schedules from one city to another, obtained from a small relational database excised from the Official Airline Guide.",
            "7": "By requiring that all system developers use the same database, it has been possible to compare the performance of various spoken language systems based on their ability to extract the correct information from the database, using a set of prescribed training and test data, and a set of interpretation guidelines.",
            "8": "Indeed, periodic common evaluations have occurred at regular intervals, and steady performance improvements have been observed for all systems [2, 3, 4].",
            "9": "While the ATIS task has been instrumental in the development of technologies that can understand spontaneously generated verbal queries in a limited domain, it does have some shortcomings.",
            "10": "First, the current common evaluation focuses on the correctness of the information extracted from the database without any regard to the system's side of the interchange (e.g.",
            "11": ", clarification queries and helpful suggestions).",
            "12": "Thus it has the effect of discouraging research on dialogue-based systems which, we believe, is a crucial aspect of human computer interactions.",
            "13": "Second, ATIS makes use of a mock-up, static database containing flight and fare information for a small set of cities within the United States and Canada.",
            "14": "It is not a realistic model of the databases actually being used by travel agents and travellers.",
            "15": "In particular, operational flight information systems are much larger and more complex, and, most importantly, they contain information which is dynamic in nature.",
            "16": "The rapid technological progress that we are witnessing gives us hope that spoken language systems capable of performing real tasks will begin to emerge within the decade.",
            "17": "To realize this potential, however, it is important that we begin to develop the technology using real databases, so that we can uncover limitations and gaps in our present research paradigm.",
            "18": "To this end, we started in 1992 to investigate the feasibility of attaching a spoken language interface to an available on-line database.",
            "19": "We selected the American Airlines EAASY SABRE system, which allows subscribers to obtain flight information and make flight reservations via a large dynamic database, accessed through their personal computers over the telephone line.",
            "20": "This system currently has over 700,000 active subscribers, most of whom are travellers, not travel agents.",
            "21": "We selected this database mainly because we believe we can leverage off of our existing Nris system to build an appropriate user-friendly interface.",
            "22": "To communicate with EAASY SABRE in its normal mode of operation, users issue coded queries specifying restrictions such as source, date, and fare code.",
            "23": "If the necessary pieces of information are omitted from the query, the system enters a tightly controlled menu protocol to fill them in.",
            "24": "What we have attempted to accomplish is a replacement of this cumbersome interface with something that permits a more natural dialogue with the computer.",
            "25": "Our system, called PEGASUS, acts as a mediator between the user and the EAASY SABRE system, engaging in a spoken dialogue with the user and postprocessing tables delivered by EAASY SABRE for display on the terminal.",
            "26": "The rest of the paper is organized as follows.",
            "27": "We first describe the PEGASUS system, paying particular attention to the conversion of the parse tree to a semantic representation and the multiple roles played by the System Manager, mediating between the user and the EAASY SABRE back-end.",
            "28": "We then discuss the dialogue management aspects of the system in some detail.",
            "29": "This is followed by some preliminary evaluation results, using data collected from real users planning real trips.",
            "30": "Finally, we summarize lessons learned and present our future plans"
          }
        },
        {
          "section": "method",
          "title": "SYSTEM DESCRIPTION",
          "sentences": {
            "31": "Figure 1 shows a block diagram of PEGASUS.",
            "32": "The speech understanding component makes use of much of the original ATIS system to process user queries [5, 6].",
            "33": "The segment-based SUMMIT speech recognition component [7] produces a list of the top ten sentence hypotheses, which are then filtered by the probabilistic TINA natural language component [8].",
            "34": "The particular version of TINA employs a robust parsing strategy [9] that attempts to piece together parsable fragments, which is often necessary for spontaneous speech containing disfluencies.",
            "35": "A semantic frame representation is used to encode the meaning efficiently.",
            "36": "The entire speech understanding system, with a working vocabulary of approximately 1300 words, performs in near real-time using a high-end workstation with no additional hardware.",
            "37": "As for its performance, our ATIS system achieved the second lowest error rate for both text and speech input in the last two annual ARPA spoken language systems common evaluations measuring the systems' ability to extract relevant information from the database [3, 4].",
            "38": "In the most recent evaluation, our system achieved an error rate of 12.5% and 14.2% on all answerable queries, when the transcription and the speech signal were provided as input, respectively.",
            "39": "Through our previous experience in developing spoken language systems, we have learned that simplicity of form is an important principle in building effective meaning representations.",
            "40": "Our view on the appropriate structural units of a semantic frame has evolved over time.",
            "41": "Our present view is that all major constituents in a sentence can be classified as one of only three distinct categories, which we label as [clause], [topic], and [predicate].",
            "42": "Thus, verbs, adjectives, prepositions and modifier nouns are all considered to be predicates.",
            "43": "Furthermore, grammatical constituents such as &quot;subject&quot; and &quot;direct object&quot; are not explicitly marked in the semantic frame.",
            "44": "We have applied this new formalism successfully across several languages in our VOYAGER domain [10], and we are also using it in PEGASUS.",
            "45": "An example semantic frame for the sentence, &quot;Is there a United flight connecting in Denver,&quot; is shown in Figure 2.",
            "46": "During the design phase of our project, we made a commitment not to alter the interface and protocols of EAASY SABRE.",
            "47": "We see no benefit, nor do we feel competent, in making changes to a proven system used by many users.",
            "48": "In fact, PEGASUS'S interface to EAASY SABRE is identical to that of a user on a PC or a travel agent — EAASY SABRE cannot distinguish between a user speaking a natural utterance (such as &quot;I want to go from Boston to San Francisco on October 21&quot;) or typing to the system in cryptic codes (such as /schedule,BOS,SF0,210CT,1).",
            "49": "Thus the first task of the System Manager is to transform the semantic frame into the appropriate EAASY SABRE command.",
            "50": "When necessary, the System Manager engages in a clarification dialogue with the user until enough information is available to construct a complete EAASY SABRE command, thus preempting EAASY SABRE'S original menu-based clarification protocol.",
            "51": "In response to a command, EAASY SABRE generally returns both formatted tables and additional options available by menu selection (such as &quot;5: Show Seat Assignment&quot; or &quot;12: Show more flights&quot;).",
            "52": "The data stream (i.e.",
            "53": ", raw text) returned from EAASY SABRE must be parsed, filtered, and reformatted by the System Manager for display to the user.",
            "54": "The tables and menu options extracted from the database are temporarily stored in a local cache.",
            "55": "Menu options are selectively made available to the user, thus allowing the system, for example, to map a user's explicit request for seat assignment into the appropriate menu key.",
            "56": "Tables can be postprocessed to apply constraints beyond those available through EAASY SABRE, such as &quot;serving dinner,&quot; &quot;nonstop,&quot; or &quot;leaving in the afternoon.&quot; In addition to providing the displays, the System Manager also provides a paraphrase of the relevant information.",
            "57": "Some users have found this feature to be extremely beneficial to help detect the system's understanding errors.",
            "58": "The user can also type natural language queries to PEGASUS, an appropriate action when speech recognition errors persist."
          }
        },
        {
          "section": "method",
          "title": "DIALOGUE MANAGEMENT",
          "sentences": {
            "59": "Before discussing the dialogue management aspects of PEGASUS, we thought it would be instructive to examine the log of an actual round-trip booking, shown in Figure 3.",
            "60": "In this example, one of the authors used PEGASUS to make a reservation in order to attend a workshop in the San Francisco area.",
            "61": "Like a travel agent, PEGASUS needs to know the source, destination, and date before it can provide the flight information.",
            "62": "The user utilized additional constraints to narrow down the choices before settling on a particular flight.",
            "63": "It took two exchanges to arrive at the appropriate fare, and three more to book the return flight.",
            "64": "The entire booking took nine exchanges, and lasted approximately 5 minutes.",
            "65": "Note that a large fraction of the time is spent waiting for EAASY SABRE to respond.",
            "66": "The dialogue component of PEGASUS is significantly more complicated than that of the original ATIS system [11].",
            "67": "This is in large part due to the fact that it must monitor not only the user's dialogue state and the degree of completion of the booking, but also the state of the EAASY SABRE system.",
            "68": "For instance, it must preprocess fare restrictions, warning the user of limits imposed on return dates for restricted fares, screening selected fares for possible constraint failure, and confirming availability on selected flights before attempting to issue bookings.",
            "69": "Otherwise, the EAASY SABRE system would invoke a complex subdialogue which we wish to avoid.",
            "70": "The system keeps a record of the most recently displayed sets of flights and fares, as well as a ticket frame where slots (e.g.",
            "71": ", fare class) are periodically updated upon user specification.",
            "72": "This information is also displayed to the user as a ticket.",
            "73": "The system must consult all of these sources of information in addition to the user's queries in deciding its next move.",
            "74": "The dialogue is managed as a set of more than thirty distinct dialogue states.",
            "75": "Any particular dialogue exchange consists of a response phase followed by an initiative phase.",
            "76": "During the response phase, the system may have to consult the dialogue state to properly interpret the query.",
            "77": "For instance, there are several different states that can provoke a &quot;yes/no&quot; response, and the system must also be prepared for the user not to comply, but instead to ask a completely independent question.",
            "78": "While processing the user's query the system may need to update the dialogue state, and upon completing the response phase it consults the state to determine what if any initiative action is appropriate at this time.",
            "79": "There are several meta-level commands that have led to a more usable system.",
            "80": "A request for help can be issued at any time, and the response it invokes is dependent upon the current state of the dialogue.",
            "81": "For instance, at the very beginning of a dialogue, a request for help causes the system to provide an organized list of the cities it knows.",
            "82": "There are also two meta-level commands that erase previous information.",
            "83": "A &quot;scratch that&quot; command causes the system to erase the preceding query from context, restoring the discourse content to its former state.",
            "84": "In the event that this query involved a booking request, the system must also issue a cancellation request to EAASY SABRE.",
            "85": "The &quot;scratch that&quot; capability is particularly valuable to recover from damaging recognition errors.",
            "86": "A more drastic &quot;clear history&quot; command allows the user to start completely afresh.",
            "87": "We feel that the availability of these erasure commands allows the system to be more aggressive in taking action, without having to repeatedly ask for confirmation of user requests."
          }
        },
        {
          "section": "evaluation",
          "title": "EVALUATION",
          "sentences": {
            "88": "PEGASUS first came into being in January 1993.",
            "89": "Since then, we have been actively improving and extending its capabilities.",
            "90": "Thus the system is in a constant state of flux — deficiencies are corrected as new capabilities are introduced.",
            "91": "Nevertheless, it is fully functional in the sense that members of our group have been able to use it to make actual travel arrangements since last spring, using naturally spoken English.",
            "92": "Even though it is definitely premature to accurately assess the usefulness of the system, we have recently begun to formally monitor its performance longitudinally by keeping a time-stamped log file of all transactions.",
            "93": "In this section, we will present some very preliminary results on the system's performance since early fall, 1993.",
            "94": "The results are obtained from ten bookings made by eight members of our group in order to satisfy their real travel needs.",
            "95": "All of them represented round-trip bookings from one city to another.",
            "96": "In some cases, the time for travel was important whereas in others, the cheapest airfare was desired.",
            "97": "Seven of the ten bookings were successfully completed.",
            "98": "Statistics on some of the objective measures for the successfully completed bookings are shown in Table 1.",
            "99": "Averaged across all six subjects who completed the bookings successfully, it took almost 25 queries and more than 13 minutes for the subjects to complete a booking.",
            "100": "It is interesting, however, to compare the statistics of the three experienced users2 with the other three, who were using the system for the first time.",
            "101": "Compared to the naive users, the experienced users completed the bookings with considerably less effort — using less than onethird of the number of queries and taking one-fourth the amount of time.",
            "102": "The variations in their performance are also considerably less.",
            "103": "In general, one can expect the system's performance on totally naive subjects to degrade.",
            "104": "On the other hand, the results give us hope that experienced travellers can learn to put PEGASUS to productive use, once they become familiar with its capabilities.",
            "105": "We also examined the log files for the three unsuccessful bookings in order to identify the system's shortcomings.",
            "106": "In one case, the user successfully completed the forward leg of a trip, but the system booked an erroneous return leg, causing him to start over.",
            "107": "He cleared the discourse history, but did not explicitly cancel the booking on EAASY SABRE.",
            "108": "Thus, even though the user successfully booked the flights he wanted, EAASY SABRE was unable to reconcile the double booking on the forward leg.",
            "109": "In the second case, the user initially selected a, fare that was incompatible with his travel plans.",
            "110": "He did not successfully cancel his initial reservation or clear the discourse history.",
            "111": "The, system continued to enforce the restrictions on the previous fare, even though he attempted to rebook with an unrestricted fare.",
            "112": "In the third case, the discount fare selected for the forward leg was not available on the return flight.",
            "113": "Both the second and third users eventually gave up in frustration.",
            "114": "Since mid January, we have begun to save the speech waveform, in addition to the log-file.",
            "115": "We were thus able to also measure the system's speech recognition performance.",
            "116": "The word and sentence recognition error rates for these bookings were found to be 10.6% and 28.6%, respectively."
          }
        },
        {
          "section": "evaluation",
          "title": "DISCUSSION ANDFUTURE PLANS",
          "sentences": {
            "117": "This paper describes our recent effort in developing a spoken language interface to an on-line, dynamic airline reservation system.",
            "118": "By leveraging off our ATIS development effort and paying particular attention to dialogue management, we were able to produce a working interface that enables users to make real flight bookings using spoken language.",
            "119": "PEGASUS is the outcome of a new research strategy that we have adopted, one that strives to develop language-based technologies within the context of real application back-ends, rather than relying on mock-ups, however realistic they might be.",
            "120": "We believe that this strategy will force us to confront some of the critical technical issues that may otherwise elude our attention, such as dialogue modelling and new word detection/learning.",
            "121": "We also believe that the time is ripe for us to begin demonstrating the usefulness of these technologies.",
            "122": "Working on real applications thus has the potential benefit of shortening the interval between technology demonstration and its ultimate use.",
            "123": "Besides, real applications that can help people solve problems will be used by real users, thus providing us with a rich and continuing source of useful data.",
            "124": "While we are encouraged by our initial success with PEGASUS, much work remains to be done.",
            "125": "One of the major deficiencies of the system is its inability to gracefully coerce the user back on track when his/her request cannot be satisfied.",
            "126": "A common problem arises when the cheapest fare that the user specified is not available on the selected return flight.",
            "127": "The user is faced with the multiple choices of modifying his/her choice for the flight, date, or fare.",
            "128": "Rather than leaving the user to explore all these dimensions freely and run the risk of confusion, a more productive solution may be for the system to take control of the dialogue by offering explicit choices.",
            "129": "Of course, the user should still be free to diverge from the computer's goal whenever he/she so chooses.",
            "130": "Until very recently, the system's knowledge has been limited to fewer than sixty major cities in North America, Europe, and Japan.",
            "131": "We have just expanded PEGASUS'S knowledge base to more than 220 major cities worldwide.",
            "132": "Nevertheless, it is still a very small set considering that EAASY SABRE contains flight information for nearly two thousand cities worldwide.",
            "133": "Rather than making all the cities, airports, and airlines available with equal probability at all times, we will explore ways to constrain the search while maintaining full flexibility.",
            "134": "One possibility is to allow a user to customize the system to suit their needs.",
            "135": "Thus, for example, a user could specify the cities and airlines that they care about, in. much the same way they presently specify their frequent flyer number, seating preferences, and credit card information for billing.",
            "136": "The system will need to be supplemented with tools that will enable users to interactively and incrementally add appropriate information.",
            "137": "In addition, the system could also automatically adjust language probabilities based on the user's dialogue history.",
            "138": "At the moment, the system can only book a single seat under the name of the user currently logged onto EAASY SABRE.",
            "139": "In the future, we would like to add the capability of changing the name on the ticket, or booking multiple tickets for the user and accompanying family members, for example.",
            "140": "The present implementation of PEGASUS assumes that information is provided to the user both visually and aurally.",
            "141": "This assumption obviously affects significantly the nature of the responses generated by PEGASUS.",
            "142": "For example, the system will currently say, &quot;Here are the flights from Boston to San Francisco on October 20,&quot; and proceed to display them.",
            "143": "We believe that there will be many occasions in which a user may be communicating with the system by telephone.",
            "144": "In such a case, the information must be presented in a different manner (e.g.",
            "145": ", &quot;There are seventeen direct flights from Boston to San Francisco on October 20.&quot;) The resulting human-computer dialogue will be quite different from that in our current implementation.",
            "146": "We intend to pursue such a &quot;displayless&quot; implementation in the future, eventually leading to the development of telephone-based applications.",
            "147": "Our experience in designing PEGASUS has led us to the realization that considerable care must go into providing mechanisms to easily manage and maintain dialogue coherence.",
            "148": "While our dialogue states are a convenient representation, the current mechanism for controlling them is becoming unwieldy, and therefore needs to be reorganized prior to adding some of the enhancements mentioned here.",
            "149": "Through our experience in developing a preliminary version of PEGASUS, we discovered that the capability to specify the dialogue flow explicitly at some high level is necessary, in order to be able to understand and manage the dialogue effectively.",
            "150": "To that end, we recently redesigned the PEGASUS control strategy, so that dialogue moves conditioned on prior states can be conveniently specified in tabular form.",
            "151": "An example entry from our newest implementation is shown in Table 2.",
            "152": "This entry states that when the user has just completed a successful booking, the system should examine the conditions in the order presented and take the appropriate action when they are met, setting the dialogue state to the new value, if appropriate.",
            "153": "Thus, in our example, once a flight has been booked, the first thing the system does is check to see if there is a first-leg flight associated with the current one (i.e.",
            "154": ", &quot;Has-firstleg?&quot;).",
            "155": "If so, the system performs the actions associated with concluding a booking (e.g.",
            "156": ", summarizing the flight information) and resets the dialogue state to anticipate a completely new exchange.",
            "157": "If the first condition is not met, the system proceeds in the same manner through the others in the order given.",
            "158": "Ultimately, we would like a dialogue framework that is domain independent.",
            "159": "We have begun to define a dialoguedescription language in which different types of user interactions can be represented.",
            "160": "The terminal nodes of the grammar would be associated with user query classes.",
            "161": "User interactions expected within a particular domain would be described in this meta language, and that description would be used by the system to direct the human machine interaction.",
            "162": "There has been some theoretical work on the structure of human-human dialogue [12], but this has not yet led to effective insights for building human-machine interactive systems.",
            "163": "We believe it should be possible to define a hierarchy of dialogue types: for example, the air travel dialogue is an instance of a more general transaction dialogue in which the user acquires information about the choices available, commits to a purchase, perhaps authorizes payment, and verifies the entire transaction.",
            "164": "It should be possible to compile a domain-specific dialogue model from a general transaction dialogue framework and a description of the particular sub-domain."
          }
        },
        {
          "section": "acknowledgments",
          "title": "ACKNOWLEDGEMENTS",
          "sentences": {
            "165": "Part of the work described in this paper is conducted in collaboration with American Airlines SABRE Travel Information Network.",
            "166": "We also benefited from the contributions made by two past members of our group: David Goodine and Lynette Hirschman."
          }
        }
      ],
      "doc_id": 5,
      "actual_doc_id": "H94-1037-parscit-section.xml"
    },
    {
      "content": [
        {
          "section": "abstract",
          "title": "Abstract",
          "sentences": {
            "0": "We describe a method for evaluating a grammar checking application with hand–bracketed parses.",
            "1": "A randomly–selected set of sentences was submitted to a grammar checker in both bracketed and unbracketed formats.",
            "2": "A comparison of the resulting error reports illuminates the relationship between the underlying performance of the parser– grammar system and the error critiques presented to the user."
          }
        },
        {
          "section": "introduction",
          "title": "INTRODUCTION",
          "sentences": {
            "3": "The recent development of broad–coverage natural language processing systems has stimulated work on the evaluation of the syntactic component of such systems, for purposes of basic evaluation and improvement of system performance.",
            "4": "Methods utilizing hand–bracketed corpora (such as the University of Pennsylvania Treebank) as a basis for evaluation metrics have been discussed in Black et al.",
            "5": "(1991), Harrison et al.",
            "6": "(1991), and Black et al.",
            "7": "(1992). Three metrics discussed in those works were the Crossing Parenthesis Score (a count of the number of phrases in the machine produced parse which cross with one or more phrases in the hand parse), Recall (the percentage of phrases in the hand parse that are also in the machine parse), and Precision (the percentage of phrases in the machine parse that are in the hand parse).",
            "8": "We have developed a methodology for using hand–bracketed parses to examine both the internal and external performance of a grammar checker.",
            "9": "The internal performance refers to the behavior of the underlying system—i.e.",
            "10": "the tokenizer, parser, lexicon, and grammar.",
            "11": "The external performance refers to the error critiques generated by the system'.",
            "12": "Our evaluation methodology relies on three separate error reports generated from a corpus of randomly selected sentences: 1) a report based on unbracketed sentences, 2) a report based on optimally bracketed sentences with our current system, and 3) a report based on the optimal bracketings with the system modified to insure the same coverage as the unbracketed corpus.",
            "13": "The bracketed report from the unmodified system tells us something about the coverage of our underlying system in its current state.",
            "14": "The bracketed report from the modified system tells us something about the external accuracy of the error reports presented to the user.",
            "15": "Our underlying system uses a bottom–up, full– ambiguity parser.",
            "16": "Our error detection method relies on including grammar rules for parsing errorful sentences, with error critiques being generated from the occurrence of an error rule in the parse.",
            "17": "Error critiques are based on just one of all the possible parse trees that the system can find for a given sentence.",
            "18": "Our major concern about the underlying system is whether the system has a correct parse for the sentence in question.",
            "19": "We are also concerned about the accuracy of the selected parse, but our current methodology does not directly address that issue, because correct error reports do not depend on having precisely the correct parse.",
            "20": "Consequently, our evaluation of the underlying grammatical coverage is based on a simple metric, namely the parser success rate for satisfying sentence bracketings (i.e.",
            "21": "correct parses).",
            "22": "Either the parser can produce the optimal parse or it can't.",
            "23": "We have a more complex approach to evaluating the performance of the system's ability to detect errors.",
            "24": "Here, we need to look at both the overgeneration and undergeneration of individual error critiques.",
            "25": "What is the rate of spurious critiques, or critiques incorrectly reported, and what is the rate of missed critiques, or critiques not reported.",
            "26": "Therefore we define two additional metrics, which illuminate the spurious and missed critique rates, respectively: Precision: the percentage of correct critiques from the unbracketed corpus.",
            "27": "Recall: the percentage of critiques generated from an ideal bracketed corpus that are also present among those in the unbracketed corpus.",
            "28": "Precision tells us what percentage of reported critiques are reliable, and Recall tells us what percentage of correct critiques have been reported (modulo the coverage)."
          }
        },
        {
          "section": "method",
          "title": "OVERVIEW OF THE APPLICATION",
          "sentences": {
            "29": "The Boeing Simplified English Checker (a.k.a.",
            "30": "the BSEC, cf.",
            "31": "Hoard, Wojcik, and Holzhauser 1992) is a type of grammar and style checker, but it is more accurately described as a `controlled English checker' (cf.",
            "32": "Adriaens 1992).",
            "33": "That is, it reports to users on where a text fails to comply with the aerospace standard for maintenance documentation known as Simplified English (AECMA 1989).",
            "34": "If the system cannot produce a parse, it prints the message &quot;Can't do SE check.&quot; At present, the Checker achieves parses for about 90 percent of the input strings submitted to it.2 The accuracy of the error critiques over that 90 percent varies, but our subjective experience suggests that most sentence reports contain critiques that are useful in that they flag some bona fide failure to comply with Simplified English.",
            "35": "The NLP methodology underlying the BSEC does not rely on the type of pattern matching techniques used to flag errors in more conventional checkers.",
            "36": "It cannot afford simply to ignore sentences that are too complex to handle.",
            "37": "As a controlled sublanguage, Simplified English requires that every word conform to specified usage.",
            "38": "That is, each word must be marked as `allowed' in the lexicon, or it will trigger an error critique.",
            "39": "Since the standard generally requires that words be used in only one part of speech, the BSEC produces a parse tree on which to judge vocabulary usage as well as other types of grammatical violations.3 As one would expect, the BSEC often has to choose between quite a few alternative parse trees, sometimes even hundreds or thousands of them.",
            "40": "Given its reliance on full–ambiguity parse forests and relatively little semantic analysis, we have been somewhat surprised that it works as well as it does.",
            "41": "We know of few grammar and style checkers that rely on the complexity of grammatical analysis that the BSEC does, but IBM's Critique is certainly one of the best known.",
            "42": "In discussing the accuracy of Critique, Richardson and Braden–Harder (1993:86) define it as &quot;the actual 'under the covers' natural language processing involved, and the user's perception.&quot; In other words, there are really two levels upon which to gauge accuracy—that of the internal parser and that of the reports generated.",
            "43": "They add: &quot;Given the state of the art, we may consider it a blessing that it is possible for the latter to be somewhat better than the former.&quot; The BSEC, like Critique, appears to be smarter than it really is at guessing what the writer had in mind for a sentence structure.",
            "44": "Most error critiques are not affected by incorrect phrasal attachment, although grossly incorrect parses lie behind most sentence reports that go sour.",
            "45": "What we have not fully understood in the past is the extent to which parsing accuracy affects error critiques.",
            "46": "What if we could eliminate all the bad parses?",
            "47": "Would that make our system more accurate by reducing incorrect critiques, or would it degrade performance by reducing the overall number of correct critiques reported?",
            "48": "We knew that the system was capable of producing good error reports from relatively bad parses, but how many of those error reports even had a reasonably correct parse available to them?"
          }
        },
        {
          "section": "method",
          "title": "OVERVIEW OF SIMPLIFIEDENGLISH",
          "sentences": {
            "49": "The SE standard consists of a set of grammar, style, format, and vocabulary restrictions, not all of which lend themselves to computational analysis.",
            "50": "A computer program cannot yet support those aspects of the standard that require deep understanding, e.g. the stricture against using a word in any sense other than the approved one, or the requirement to begin paragraphs with the topic sentence.",
            "51": "What a program can do is count the number of words in sentences and compound nouns, detect violations of parts of speech, flag the omission of required words (such as articles) or the presence of banned words (such as auxiliary have and be, etc.).",
            "52": "The overall function of such a program is to present the writer with an independent check on a fair range of Simplified English requirements.",
            "53": "For further details on Simplified English and the BSEC, see Hoard et al.",
            "54": "(1992) and Wojcik et al.",
            "55": "(1990). Although the BSEC detects a wide variety of Simplified English and general writing violations, only the error categories in Table 1 are relevant to this study: Except for illegal comma usage, which is rather uncommon, the above errors are among the most frequent types of errors detected by the BSEC.",
            "56": "To date, The Boeing Company is the only aerospace manufacturer to produce a program that detects such a wide range of Simplified English violations.",
            "57": "In the past, Boeing and other companies have created checkers that report on all words that are potential violations of SE, but such 'word checkers' have no way of avoiding critiques for word usage that is correct.",
            "58": "For example, if the word test is used legally as a noun, the word— checking program will still flag the word as a potential verb—usage error.",
            "59": "The BSEC is the only Simplified English checker in existence that manages to avoid this.4 As Richardson and Braden—Harder (p.",
            "60": "88) pointed out: &quot;We have found...that professionals seem much more forgiving of wrong critiques, as long as the time required to disregard them is minimal.&quot; In fact, the chief complaint of Boeing technical writers who use the BSEC is when it produces too many nuisance errors.",
            "61": "So word—checking programs, while inexpensive and easy to produce, do not address the needs of Simplified English writers."
          }
        },
        {
          "section": "method",
          "title": "THE PARSER UNDERLYING THEBSEC",
          "sentences": {
            "62": "The parser underlying the Checker (cf.",
            "63": "Harrison 1988) is loosely based on GPSG.",
            "64": "The grammar contains over 350 rules, and it has been implemented in Lucid Common Lisp running on Sun workstations.5 Our approach to error critiquing differs from that used by Critique (Jensen, Heidorn, Miller, and Ravin 1993).",
            "65": "Critique uses a two—pass approach that assigns an initial canonical parse in so—called `Chomsky—normal' form.",
            "66": "The second pass produces an altered tree that is annotated for style violations.",
            "67": "No–parses cause the system to attempt a `fitted parse', as a means of producing some information on more serious grammar violations.",
            "68": "As mentioned earlier, the BSEC generates parse forests that represent all possible ambiguities vis–a–vis the grammar.",
            "69": "There is no `canonical' parse, nor have we yet implemented a `fitted parse' strategy to reclaim information available in no–parses.6 Our problem has been the classic one of selecting the best parse from a number of alternatives.",
            "70": "Before the SE Checker was implemented, Boeing's parser had been designed to arrive at a preferred or `fronted' parse tree by weighting grammatical rules and word entries according to whether we deemed them more or less desirable.",
            "71": "This strategy is quite similar to the one described in Heidorn 1993 and other works that he cites.",
            "72": "In the maintenance manual domain, we simply observed the behavior of the BSEC over many sentences and adjusted the weights of rules and words as needed.",
            "73": "To get a better idea of how our approach to fronting works, consider the ambiguity in the following two sentences: In the Simplified English domain, it is more likely that (2) will be an example of passive usage, thus calling for an error report.",
            "74": "To parse (1) as a passive would likely be incorrect in most cases.",
            "75": "We therefore assigned the adjective reading of closed a low weight in order to prefer an adjectival over a verb reading.",
            "76": "Sentence (2) reports a likely event rather than a state, and we therefore weight repaired to be preferred as a passive verb.",
            "77": "Although this method for selecting fronted parse trees sometimes leads to false error critiques, it works well for most cases in our domain."
          }
        },
        {
          "section": "method",
          "title": "BRACKETED INPUT STRINGS",
          "sentences": {
            "78": "In order to coerce our system into accepting only the desired parse tree, we modified it to accept only parses that satisfied bracketed forms.",
            "79": "6. The BSEC has the capability to report on potential word usage violations in no–parses, but the end–users seem to prefer not to use it.",
            "80": "It is often difficult to say whether information will be viewed as help or as clutter in error reports.",
            "81": "For example, the following sentence produces five separate parses because our grammar attaches prepositional phrases to preceding noun phrases and verb phrases in several ways.",
            "82": "The structural ambiguity corresponds to five different interpretations, depending on whether the boy uses a telescope, the hill has a telescope on it, the girl on the hill has a telescope, and so on.",
            "83": "We created a lisp operation called spe, for &quot;string, parse, and evaluate,&quot; which takes an input string and a template.",
            "84": "It returns all possible parse trees that fit the template.",
            "85": "Here is an example of an spe form for (3): The above bracketing restricts the parses to just the parse tree that corresponds to the sense in which the boy saw the girl who is identified as being on the hill that has a telescope.",
            "86": "If run through the BSEC, this tree will produce an error message that is identical to the unbracketed report—viz.",
            "87": "that boy, girl, hill, and telescope are NON–SE words.",
            "88": "In this case, it does not matter which tree is fronted.",
            "89": "As with many sentences checked, the inherent ambiguity in the input string does not affect the error critique.",
            "90": "Recall that some types of ambiguity do affect the error reports--e.g.",
            "91": "passive vs. adjectival participial forms.",
            "92": "Here is how the spe operation was used to disambiguate a sentence from our data: We judged the word permitted to have roughly the same meaning as stative 'permissible' here, and that led us to coerce an adjectival reading in the bracketed input.",
            "93": "If the unbracketed input had resulted in the verb reading, then it would have flagged the sentence as an illegal passive.",
            "94": "It turned out that the BSEC selected the adjective reading in the unbracketed sentence, and there was no difference between the bracketed and unbracketed error critiques in this instance."
          }
        },
        {
          "section": "method",
          "title": "METHODOLOGY",
          "sentences": {
            "95": "We followed this procedure in gathering and analyzing our data: First, we collected a set of data from nightly BSEC batch runs extending over a three month period from August through October 1991.",
            "96": "The data set consisted of approximately 20,000 sentences from 183 documents.",
            "97": "Not all of the documents were intended to be in Simplified English when they were originally written.",
            "98": "We wrote a shell program to extract a percentage—stratified sample from this data.",
            "99": "After extracting a test set, we ended up culling the data for duplicates, tables, and other spurious data that had made it past our initial filter.7 We ended up with 297 sentences in our data set.",
            "100": "We submitted the 297 sentences to the current system and obtained an error report, which we call the unbracketed report.",
            "101": "We then created spe forms for each sentence.",
            "102": "By observing the parse trees with our graphical interface, we verified that the parse tree we wanted was the one produced by the spe operation.",
            "103": "For 49 sentences, our system could not produce the desired tree.",
            "104": "We ran the current system, using the bracketed sentences to produce the unmodified bracketed report.",
            "105": "Next we examined the 24 sentences which did not have parses satisfying their bracketings but did, nevertheless, have parses in the unbracketed report.",
            "106": "We added the lexical information and new grammar rules needed to enable the system to parse these sentences.",
            "107": "Running the resulting system produced the modified bracketed report.",
            "108": "These new parses produced critiques that we used to evaluate the critiques previously produced from the unbracketed corpus.",
            "109": "The comparison of the unbracketed report and the modified bracketed report produced the estimates of Precision and Recall for this sample.",
            "110": "7. The BSEC filters out tables and certain other types of input, but the success rate varies with the type of text."
          }
        },
        {
          "section": "evaluation",
          "title": "RESULTS",
          "sentences": {
            "111": "Our 297—sentence corpus had the following characteristics.",
            "112": "The length of the sentences ranged between three words and 32 words.",
            "113": "The median sentence length was 12 words, and the mean was 13.8 words.8 Table 2 shows the aggregated outcomes for the three reports.",
            "114": "The table shows the coverage of the system and the impact of the spurious parses.",
            "115": "The coverage is reflected in the Unmodified Bracketed column, where 248 parses indicates a coverage of 84 percent for the underlying system in this domain.",
            "116": "The table also reveals that there were 24 spurious parses in the unbracketed corpus, corresponding to no valid parse tree in our grammar.",
            "117": "The Modified Bracketed column shows the effect on the report generator of forcing the system to have the same coverage as the unbracketed run.",
            "118": "Table 3 shows by type the errors detected in instances where errors were reported.",
            "119": "The Spurious Error column indicates the number of errors from the unbracketed sentences which we judged to be bad.",
            "120": "The Missed Errors column indicates errors which were missed in the unbracketed report, but which showed up in the modified bracketed 8.",
            "121": "Since most of the sentences in our corpus were intended to be in Simplified English, it is not surprising that they tended to be under the 20 word limit imposed by the standard.",
            "122": "report. The modified bracketed report contained only 'actual' Simplified English errors.",
            "123": "For this data, the estimate of Precision (rate of correct error critiques for unbracketed data) is (302-64)/302, or 79 percent.",
            "124": "We estimate that this precision rate is accurate to within 5 percent with 95 percent confidence.",
            "125": "Our estimate of Recall (rate of correct critiques from the set of possible critiques) is (267-29)/267, or 89 percent.",
            "126": "We estimate that this Recall rate is accurate to within 4 percent with 95 percent confidence.",
            "127": "It is instructive to look at a report that contains an incorrectly identified error.",
            "128": "The following report resulted from our unbracketed test run: The bracketed run produced a no—parse for this sentence because of an inadequacy in our grammar that blocked fi// from parsing as a verb.",
            "129": "Since it parsed as a noun in the unbracketed run, the system complained thatfi// was allowed as a verb.",
            "130": "In our statistics, we counted the fi// Noun error as an incorrect POS error and the requires Verb error as a correct one.",
            "131": "This critique contains two POS errors, one TWO—COMMAND error, and two MISSING ARTICLE error.",
            "132": "Four of the five error critiques are accurate."
          }
        },
        {
          "section": "discussions",
          "title": "DISCUSSION",
          "sentences": {
            "133": "We learned several things about our system through this exercise.",
            "134": "First, we learned that the act of comparing unbracketed and unmodified bracketed sentences revealed worse performance in the underlying system than we anticipated.",
            "135": "We had expected there to be a few more no—parses with unmodified bracketing, but not so many more.",
            "136": "Second, the methodology helped us to detect some obscure bugs in the system.",
            "137": "For example, the TWO—COMMAND and NOUN CLUSTER errors were not being flagged properly in the unmodified bracketed set because of bugs in the report generator.",
            "138": "These bugs had not been noticed because the errors were being flagged properly in some sentences.",
            "139": "When a system gets as large and complicated as ours, especially when it generates hundreds or thousands of parse trees for some sentences, it becomes very difficult to detect errors that only show up sporadically and infrequently in the data.",
            "140": "Our new methodology provided us with a window on that aspect of system performance.",
            "141": "Perhaps a more interesting observation concerns the relationship between our system and one like Critique, which relies on no—parses to trigger a fitted parse 'damage repair' phase.",
            "142": "We believe that the fitted—parse strategy is a good one, although we have not yet felt a strong need to implement it.",
            "143": "The reason is that our system generates such rich parse forests that strings which ought to trigger no—parses quite frequently end up triggering 'weird' parses.",
            "144": "That is, they trigger parses that are grammatical from a strictly syntactic perspective, but inappropriate for the words in their accustomed meanings.",
            "145": "A fitted parse strategy would not work with these cases, because the system has no way of detecting weirdness.",
            "146": "Oddly enough, the existence of weird parses often has the same effect in error reports as parse fitting in that they generate error critiques which are useful.",
            "147": "The more ambiguity a syntactic system generates, the less likely it is to need a fitted parse strategy to handle unexpected input.",
            "148": "The reason for this is that the number of grammatically correct, but 'senseless' parses is large enough to get a parse that would otherwise be ruled out on semantic grounds.",
            "149": "Our plans for the use of this methodology are as follows.",
            "150": "First, we intend to change our current system to improve deficiencies and lack of coverage revealed by this exercise.",
            "151": "In effect, we plan to use the current test corpus as a training corpus in the next phase.",
            "152": "Before deploying the changes, we will collect a new test corpus and repeat our method of evaluation.",
            "153": "We are very interested in seeing how this new cycle of development will affect the figures of coverage, Precision, and Recall on the next evaluation."
          }
        }
      ],
      "doc_id": 6,
      "actual_doc_id": "P93-1006-parscit-section.xml"
    },
    {
      "content": [
        {
          "section": "abstract",
          "title": "Abstract",
          "sentences": {
            "0": "This paper presents a semantic interpretation of adjectival modification in terms of the Generative Lexicon.",
            "1": "It highlights the elements which can be borrowed from the GL and develops limitations and extensions.",
            "2": "We show how elements of the Qualia structure can be incorporated into semantic composition rules to make explicit the semantics of the combination adjective + noun."
          }
        },
        {
          "section": "keywords",
          "title": "1 Aims",
          "sentences": {
            "3": "Investigations within the generative perspective aim at modelling, by means of a small number of rules, principles and constraints, linguistic phenomena at a high level of abstraction, level which seems to be appropriate for research on multi-linguism and language learning.",
            "4": "Among works within the generative perspect ive, one of the most, innovative is the Generative Lexicon (GL) (Pustejovsky 91, 95) which introduces an abstract model opposed to sense enumeration lexicons.",
            "5": "The GL is based (1) on the close cooperation of three lexical semantic structures: the argument structure, the aspectual structure and the Qualia structure (with four roles: Telic, Agentive, Constitutive and Formal), (2) on a detailed type theory and a type coercion inference rule and (3) on a refined theory of compositionality.",
            "6": "The Generative Lexicon investigates the problem of polysemy and of the nulltiplicity of usages from a core sense of a lexeme and shows how these usages can be analyzed in terms of possible type shiftings w.r.t. the type expected by the core usage.",
            "7": "Type shifting is modelled by a specific inference mechanism: type coercion.",
            "8": "In this paper, the following points are addressed: of its existence, in particular for the &apos;relic role, (explored e.g. in the EuroWordNet project, the European WordNet).",
            "9": "Qualias are well-designed and useful for nouns, but look more artificial for other lexical categories.",
            "10": "We show that it is the telic role of nouns which is the most useful.",
            "11": "We also show how the internal structure or this role can be made more precise and its use more reliable and accurate by means of types and how it can be partitioned by means of types into ontological domains for modelling some forms of metaphors.",
            "12": "tion system, which makes them more general.",
            "13": "This paper is devoted to adjectival modification (see also (Bouillon 97, 98)).",
            "14": "The goal is to study the use and impact of the Qualia structure of the modified noun in the determination of the semantic representation of the association Noun + Adjective.",
            "15": "To illustrate this study, we have chosen one of the most polysemic French adjectives: bon (good), which covers most of the main situations.",
            "16": "Other adjectives, often cited in the GL literature, such as sad, fast, difficult or noisy have been studied and confirm tins analysis.",
            "17": "We observed also many similarities within semantic families of adjectives."
          }
        },
        {
          "section": "introduction",
          "title": "2 Conceptual versus Lexicographic",
          "sentences": {
            "18": "In this section, we outline the differences but also the cooperation between conceptual and lexicographic analysis of the semantics of lexical items to build a lexicon suitable for the development of generative devices.",
            "19": "We have considered a sample of technical texts in French from various origins and used a simple tagging and extraction system developed for our needs.",
            "20": "We have considered a total of 386 pages of text, with a total of 193 1.46 word occurences, among which, we have ii 598 occurences of adjectives.",
            "21": "These occurences correspond to 754 different adjectives, among which 720 are restrictive adjectives.",
            "22": "We will only consider this latter set.",
            "23": "A srnall number of adjectives appear frequently: interval nb.",
            "24": "of adjectives concerned &gt; 300 5 &gt; 300 and --&lt; 150 12 &gt; 150 and --‹ 50 81 This means that 98 adjectives appear relatively frequently in texts, i.e. only about 13.6% of the total.",
            "25": "In terms of occurences, these adjectives cover 11887 occurences, i.e. about 81% of the occurences.",
            "26": "Adjectives from eight main &apos;semantic&apos; families appear frequently.",
            "27": "These families do not correspond exactly to those defined by (Dixon 91) (see also an introduction in (Raskin et al. 95)), which look too vague (figures have been rounded up or down to the closest integer): In terms of (po ysemic power&apos;, evaluative, locational, and shapes are the families which are the most polysemic, with a ratio of an average of 3.8 senses per adjective.",
            "28": "Nationalities, technical and aspectual adjectives are munch less polysemic.",
            "29": "The GL approach requires a conceptual analysis of adjectives in order to focus on a relatively small number of senses.",
            "30": "The idea is to isolate generic conceptual &apos;behaviors&apos;, while taking also into account the constraints on linguistic realizations as in the lexicographic approach.",
            "31": "The principle that we attempt at validating is to define a &apos;deep&apos; LCS representation for each predicative lexical item, which is generic enough to accomodate variations within a sense and precise enough to be meaningful and discriminatory w.r.t. other wordsenses.",
            "32": "be able to represent sense variations in an efficient and reliable way, the variable or underspecified elements should be &apos;low level&apos; elements such as functions or paths.",
            "33": "Semantic fields may also be altered, e.g. going from location to psychological or to epistemological (Pinker 93).",
            "34": "Such an approach is being validated on various semantic families of verbs.",
            "35": "The variable elements seem to belong to various ontologies (a crucial topic under intense investigation), such as the ontology of events (active, sleeping, terminated, etc.), of people&apos;s quilities, etc.",
            "36": "In this short document, for the purpose of illustration, let us consider the adjective bon (corresponding quite well to good), which is one of the most polysemic adjective: 25 senses identified in WordNet (e.g.",
            "37": "(Fellbaurn 93)).",
            "38": "In fact, bon can be combined with almost any noun in French, and as (Katz 66) pointed out, good would need as many different readings as there are functions for objects.",
            "39": "We have identified the following senses and sense variations (metaphors and metonymies in particular, expressed as in (Lakoff 80)): eyes).",
            "40": "Metaphors abound: e.g.: &apos;communication acts as tools&apos;: une bonne plaisanterie/mise an point (a good joke), &apos;function for tool&apos; (un bon odorat), &apos;paths as tools&apos; (a good road).",
            "41": "Metonymies are rather unusual since if X is a part of Y, a good X does not entail a good Y 2.",
            "42": "2. Positive evaluation of moral, psychological, physical or intellectual qualities in humans: bonne personne, bon musicien, (good person, good musician).",
            "43": "The basic sense concerns professions and related activites or humans as a whole: it is the ability of someone to realize something for professions, and, for humans, the high level of their moral qualities (an enumeration can be given or a kind of higher-order, typed expression).",
            "44": "This second sense could be viewed as a large metaphor of the first, with a structurepreserving transposition to a different ontology: from tools to professional or moral skills.",
            "45": "There are some &apos;light&apos; metaphors such as: &apos;social positions or ranks as professions&apos; (a good boss/ father /fr-iend / citizen), and a large number of metonymies: &apos;image for person, image being a part of a person&apos; (a good reputation), &apos;tool for profession&apos; (a good scalpel), &apos;place for profession&apos; (a good restaurant).",
            "46": "These metaphors have a good degree of systematicity.",
            "47": "3. Intensifier of one or more properties of the noun, producing an idea of pleasure and satisfaction (this is different for sense 5) 3: noun(+edible): good meal/dish/taste, tasty, with metonymies such as &apos;container for containee&apos; (a good bottle/ glass), noun ( I le- a r t ): good filin/book/paiiiting, valuable.",
            "48": "with metonymies such as &apos;physical support for contents&apos; (good CD), noun(±sinelling): good odor, noun(-1-psyclio): good relation/ experience noun(-Flunnan relations): good neighbours.",
            "49": "Note that bon can only be used with neutral or positive nouns, we indeed do not have ill French *good ennetnies, *good humidity with the sense outlined here.",
            "50": "1In the combination noun -4- adjective, the noun is the element that undergo the metaphor.",
            "51": "The adjective being a predicate, it is its relation to the noun it modifies which is metaphorical, similarly to the relation verb-noun.",
            "52": "The semantics of the noun remains a priori unaltered.",
            "53": "&apos;This needs refinements: there are some weak forms of upward inheritance in the part-of relation: e.g. if the body of a car is red, then the car is said to be red.",
            "54": "3 Norms are being defined for about 600 top-most nodes of a general purpose ontology in different projects and research groups (e.g.",
            "55": "NMS11, NI, Eagles EEC project), they will be used as soon as available.",
            "56": "amount/salary, a good wind.",
            "57": "In this case, good means a slightly more than the unit/measure indicated or above the average (for terms which are not measure units such as wind or salary).",
            "58": "This sense being quite different since it is basically a quantifier, it won&apos;t, be studied hereafter.",
            "59": "5. Idea of exactness, accuracy, correctness, validity, freshness, etc.: un bon raisonneinent/calcul = exact, accurate (a good deduction/computation), good note/ticket = valid, a good meat = fresh or eatable, a good use, appropriate, good knowledge efficient, large and of good quality.",
            "60": "The meaning of bon is therefore underdetermined.",
            "61": "Depending on the noun, the semantics of bon is slightly different, this is not really a case of co-composition.",
            "62": "It is the semantic type of the noun and that of the selected predicate in the telic role of the noun which determine the meaning of the adjective in this particular NP.",
            "63": "We call this phenomenon, by comparison with selective binding, selective projection, because the meaning is projected front the noun&apos;s telic role.",
            "64": "Sense 5 is substantially different from sense 1: it is basically boolean (e.g.",
            "65": "exact or not), there is no idea of tool, function or even activity.",
            "66": "Bora appears in a large number of fixed or semi-fixed forms such as: le bon goat, le bon SellS, lc bon temps, urnc bonne giffle.",
            "67": "Almost, the same behavior is observed for all evaluative adjectives such as excellent, terrific, bad or lousy in French.",
            "68": "For example, for -ma-ova-ms (bad), senses 2 and 3 are identical, sense 4 is only applicable to amounts (maimais salaire), not to units and sense 5 is almost identical, it, conveys the idea of erroneous deduction, invalid ticket, bad use and rotting meat.",
            "69": "Note that in WordNet, bad has only 14 senses, whereas good has 25 senses, with no clear justification.",
            "70": "We have carried out a comparison of our conceptual analysis with the lexicographic analysis in WordNet.",
            "71": "We have compared manually a subset of 54 adjectives among the above mentioned frequently used adjectives.",
            "72": "Among these adjectives, 30 are polyscone in our approach while 44 belong to several synsets in WordNet: largely (for highly polysemic adjectives), for which our approach identifies much less senses.",
            "73": "Each of the senses of bon has many facets and interpretations depending on the noun it modifies.",
            "74": "As for verbs or nouns (Busa 97), polymorphic types are used to represent the semantics of the expected nouns, viewed as arguments of the adjective predicate.",
            "75": "The semantic representation associated with a sense is therefore underspecified and tuned to reflect this polymorphism.",
            "76": "The scope of underspecified elements must however be bounded and precisely defined by &apos;lexical&apos; types and by additional constraints.",
            "77": "The generative expansion of underspecified fields can be defined from lexical items using a fix-point semantics approach (Saint-llizier 96).",
            "78": "2.6 Towards an automatic acquisition of conceptual descriptions Some on-line resources and dictionaries may efficiently contribute to this task.",
            "79": "We have considered several mono- and bi-lingual dictionaries in order to evaluate convergences.",
            "80": "Only those structured on a conceptual basis are worth considering.",
            "81": "Among them, the Harrap&apos;s German-French dictionary is very nicely structured in a conceptual perspective, providing translations on an accurate semantic basis.",
            "82": "Senses are slightly more expanded than in the GL approach to account for translation variations, but, closely related senses can be grouped to form the senses defined above.",
            "83": "Another source of knowledge for English is Corelex 4, which is just being made accessible.",
            "84": "It contains word definitions specifically designed for the GL.",
            "85": "Its evaluation is about to start."
          }
        },
        {
          "section": "method",
          "title": "3 Generative Devices and SemanticCornposition",
          "sentences": {
            "86": "Let us now analyze from a GL point of view the meanings of the adjective bon.",
            "87": "In (Pustejovsky 95), to deal with the compound adjective+noun, a predicate in the telic of the noun is considered.",
            "88": "For example, fast, modifying a noun such as typist, is represented as follows:.Xe [type&apos; (e, x) A f ast(e)] where e denotes an event.",
            "89": "This formula says that the event of typing is fast.",
            "90": "A similar representation is given for long, in a long record.",
            "91": "This approach is appropriate to represent temporal notions in a coarsegrained way, i.e. the event is said to be fast (with e.g. potential inferences on its expected duration) or long.",
            "92": "But this approach is not viable for bon, and many other adjectives with little or no temporal dimension.",
            "93": "In: Ac [type&apos; (e,x) A good(e)] it is not the typing event which is &apos;good&apos; but the way the typing has been performed (certainly fast, but also with no typos, good layout, etc.).",
            "94": "A precise event should not be considered in isolation, but the representation should express that, in general, someone types well, allowing exceptions (some average or bad typing events).",
            "95": "This involves a quantification, more or less explicit, over typing events of x.",
            "96": "Finally, bon being polysemous, a single representation is not sufficient to accomodate all the senses.",
            "97": "As introduced in section 1, the semantic representation framework we consider here is the LCS.",
            "98": "The nature of its primitives and its low-level granularity seem to be appropriate for our current purpose.",
            "99": "Underdetermined structures are represented by a typed A-calculus.",
            "100": "This first sense applies to any noun of type tool, machine or technique: a good car, a good screwdriver.",
            "101": "The semantic representation of bon requires a predicate from the telic role of the Qualia structure of the noun.",
            "102": "It is the set (potentially infinite) of those predicates that characterizes the polymorphism.",
            "103": "We have here a typical situation of selective binding (Pustejovsky 91), where the representation of the adjective is a priori largely underspecified.",
            "104": "Let us assume that any noun which can be modified by bon has a telic role in which the main function(s) of the object is described (e.g.",
            "105": "execute programmes for a computer, run for a car 5), then the semantics of the compound adjective + noun can be defined as follows: Let N be a noun of semantic type a, and of Qualia: where T denotes the set of predicates associated with the telic role of the noun N. Let Y the variable associated with N and let us assume that T is a list of predicates of the form Then the LCS-based representation of bon is: A Y : a, AF,[tat, BE+char,-Hdent ([thing Y ], [-Fprop ABILITY —TO(h(Y,...))= high ])] . which means that the entity denoted by the noun works well, expressed by the evaluation function ABILITY-TO and the value &apos;high&apos;.",
            "106": "This type of low-level function abounds in the LCS, this principle is introduced in (Jackendoff 97).",
            "107": "Note that the second argument of the predicate Fi does not need to be explicit (we use the Prolog notation `_&apos; for these positions).",
            "108": "The Qualia allows us to introduce in a direct, way a pragmatic or interpretative dimension via the instanciation of The constant &apos;high&apos; can be replaced by a more accurate representation, e.g.",
            "109": "&apos;above average&apos;, but the problem of evaluating a functionality remains open.",
            "110": "More generally, the introduction of low level functions, such as ABILITY-TO, and specific values, such as &apos;low&apos;, should be introduced in a principled way, following the definition of ontologies of different domains, e.g. action, intensities, etc.",
            "111": "This is quite challenging, but necessary for any accurate semantic framework.",
            "112": "Note finally that instead of quantifying over events, bon is described as a state: the functionalities of the object remain good, even when it is not used effectively.",
            "113": "If several functionalities are at stake, we may have a conjunction or a more complex combination of functions From a compositional point of view, the combination Adjective + Noun is treated as follows, where R is the semantic representation of the adjective, T, the contents of the telic role of the Quaint of the noun N of type n, r, a particular element of T, and Y, the variable associated with the noun: sem-composition ( Adj ((B).",
            "114": ",Noun (Qualia(T)) :, ( Y, _) E T, (N(Y) A II( Y )(Fi(Y, _))).",
            "115": "The open position in R(Y) is instanciated by reduction.",
            "116": "The selection of Fi is simple: kr basic tools, there is probably only one predicate in the Qualia (screw-driver screw), for more complex nouns, there is an ambiguity which is reflected by the non-deterministic choice of but probably organized with preferences, which should be added in the Qualia.",
            "117": "It is the constraint on the type of 17 that restricts the application of that semantic composition rule.",
            "118": "This notation is particularly simple and convenient.",
            "119": "Metaphors are treated in a direct way: the constraint on the type of Y can be enlarged to: AY : A, mclaphor(11, a) and the remainder of the semantic composition rule and semantic krinula remains unchanged.",
            "120": "We have, for example: metaphor(communication — act, tool) (joke).",
            "121": "met aphor(communication — path, tool) (road).",
            "122": "which is paraphrased as &apos;communication path viewed as a tool&apos;.",
            "123": "We have evaluated that, in French, there are about 12 frequent forms of metaphors for this sense.",
            "124": "The study of this first sense suggests that the introduction of a hierarchy of preferences would be a useful extension to the &apos;folic role, reflecting forms of prototypicality among predicates.",
            "125": "3.2 Sense 2: Bon restricted to cognitive or moral qualities Another sense of bon modifies nouns of type profession or human.",
            "126": "The treatment is the sante as in the above section, but the selection of the predicate(s) r = Fi(X,Y) in the telic of the noun&apos;s qualia must be restricted to properties related to the moral behavior (makes-charity, has-compassion, has-integrity) when the noun is a person; and to some psychological attitudes and cognitive capabilities when the noun denotes a profession (e.g.",
            "127": "a good composer).",
            "128": "Alternatively, some of these properties could be found in the constitutive role (approximately the part-of relation), if properties can be parts of entities.",
            "129": "The typing of the predicates in the Qualia roles can be done in two ways, (1) by means of labels identifying the different facets of a role, as in (Bergler 91) for report verbs, but these facets are often quite ad &apos;hoc and hard to define, or (2) by means of types directly associated with each predicate.",
            "130": "These types can, for example, directly reflect different verb semantic classes as those defined in (Levitt 93) or (Saint-Dizier 96) on a syntactic basis, or the major ontological classes of WordNet or EuroWordNet and their respective subdivisions.",
            "131": "This solution is preferable, since it does not involve any additional development of the Telic role, but simply the adjunction of types from a separate, pre-defined ontology.",
            "132": "The WordNet or EuroWordNet types also seem to be quite easy to handle and well-adapted to the phenomena we model.",
            "133": "This remains to be validated on a large scale.",
            "134": "An LCS representation for this sense of bon is, assuming the following types for Pi: : human, F, : action -- related — to -pro f cssion V moral — behavior, Y :ci, When several predicates are at stake, a set of F1 (Y, _) can be considered in the representation, or the statement is ambiguous.",
            "135": "Metonymies such as a good scalpel are resolved by the general rule: &apos;tools for professions&apos;.",
            "136": "This information could be in a knowledge base or, alternatively, it can be infered from the &apos;relic role of the tool: any instrument has a predicate in its telic role that describes its use: the type of the first argument of the predicate is directly related to the profession that uses it.",
            "137": "For example, scalpel has in its telic role: cut(X : surgeon V biologist, Y : body).",
            "138": "When the profession is identified, the standard procedure for determining the meaning of the compound can be applied.",
            "139": "Metonymies using the part-of relation are quite simple to resolve using the constitutive role, as in the CL.",
            "140": "Another main role of bon is to einphasize a quality of the object denoted by the noun.",
            "141": "As shown in section 2, there is a certain action associated with the telic of the modified noun that produces a certain pleasure.",
            "142": "For example, watching a good film entails a certain pleasure.",
            "143": "Let us consider again a noun N of type a (e.g.",
            "144": "edible object) associated with the variable Y. The entity (human) undergoing the pleasure is not.",
            "145": "explicit in the NP, it is represented by X, and included in the scope of a A-abstraction.",
            "146": "Let Fi(X, Y) be the predicate selected in the telic role of N. The LCS representation is then: We have here another form of representation for bon, where Fi is a CAUSE.",
            "147": "The term &apos;pleasure&apos; is an element of an ontology describing e.g. mental attitudes and feelings.",
            "148": "It is relatively generic and can be replaced by a more precise term, via selective projection (see below for sense 5), depending on the nature of the pleasure.",
            "149": "An alternative representation describes a path towards the value &apos;pleasure&apos;, giving an idea of progression: Notice that this sense of bon does not imply an idea of quantity: a good meal does not entail that the meal is big, a good temperature does not entail that the temperature is high, but rather mild.",
            "150": "The semantic composition rule is similar as in 3.1.",
            "151": "The metonymy &apos;container for containee (a good bottle) is resolved by a type shifting on Y. Y may be of type 13 if: 3 Z : a, Y : container A container — or(1/, Z).",
            "152": "Inferences are identical for e.g. a good CD.",
            "153": "We have here a.",
            "154": "situation of selective projection: the exact meaning of bon is projected from the type of the modified noun and the type of the predicate selected in the noun&apos;s Telic role.",
            "155": "For example, if the noun is of type bank — note V ticket and the type of the predicate selected in the noun&apos;s &apos;relic role is pay V give — access — to, then the meaning of bon is &apos;valid&apos;: AX :bank — note V ticket, [state BE-Echar,-Hdentathing X 1, [place AT-Fehar,-Fident([4-propValid(X)])M.",
            "156": "The constraint on the type of the telic role is stated in the semantic composition rule: : pay V give — access — to ET, (N(X) A n( X ) It is necessary to have both a constraint.",
            "157": "on the noun and on the predicate(s) in the telic role: (I) the type of the predicate in the telic role is certainly not a sufficient constraint, e.g. every noun&apos;s telic role in which there is the predicate pay cannot be combined with bon with sense 5; (2) the constraint on the type of the noun is also not sufficient, e.g. a medecine is a kind of food, but we don&apos;t eat it ."
          }
        },
        {
          "section": "method",
          "title": "4 Representing the core meaning ofa word-sense",
          "sentences": {
            "158": "The work presented here has shown the necessity of describing the semantics of a lexical item at a relatively &apos;deep&apos; level, in order to make explicit the meaning elements subject to alterations in the sense.",
            "159": "variations shown above.",
            "160": "It turns out, so far, that these elements can be represented by LCS primitives and a few functions and values, assumed to belong to general-purpose, and often commonly-admitted, ontologies.",
            "161": "This remains an assumption since this type of ontological knowledge is still under development, but the elements used are relatively simple and standard.",
            "162": "Besides ontologies, and not very far from them, we also find information contained in the noun&apos;s Qualias, but in a less structured way, making selection more difficult.",
            "163": "Core meaning definition requires a good analysis of a word-sense and of its behavior in different contexts.",
            "164": "This is however not.",
            "165": "so difficult to elaborate once the formalism is stabilized.",
            "166": "Also, we noted that semantically close words share a lot, making descriptions easier.",
            "167": "This is in particular true for verbs.",
            "168": "Besides adjectives, we have also studied a.",
            "169": "number of different types of verbs, as e.g. the verb cooper (cut), often used as an example in the literature.",
            "170": "Its core representation would be the following: A I, J [event CAUSE([tnotg 1], [event GDA( X, [path Y with the following values for the core sense: A = +/oc ; X = [thin, PART — 041) ] AW AY — FROMAaplace, LOCATION — OF( For the metaphor: &apos;to cut a conversation/ a fihn, etc...&apos;, the values for the above variables become: where ACTIVE(J) is an elementary property of an ontology describing the status of events.",
            "171": "A conversation is viewed as a flow which becomes non-active.",
            "172": "A similar treatment is observed for other types of metaphors, with elliptic forms, such as cover Venn/ l&apos;electricite/ les credits, also viewed as flows.",
            "173": "The property AVAILABLE(J) will then be used, which is at a comparable abstract level in an ontology than ACTIVE(J)."
          }
        },
        {
          "section": "method",
          "title": "5 Long-distance compositionality",
          "sentences": {
            "174": "The NP a good meat is related to senses 2 or 5, it therefore includes in its domain of meanings structures presented in sections 3.2 and 3.4.",
            "175": "Instead of choosing one solution solution (a generate and test strategy), a set can be provided (as in constraint programming).",
            "176": "Now, if we have an NP of the form: one viande bonne a consommer, then the parsing of consommer will provoque the selection of sense 5 (and subsense &apos;fresh/consumable&apos; via selective projection) because of the type of consommer.",
            "177": "If, conversely, we have one viande bonne a deguster, then, since deguster is of type &apos;eat.enjoy&apos; (a dotted type in the GL), sense 2 is selected.",
            "178": "The space of meanings is restricted when additional information is found.",
            "179": "A second case involves default reasoning (as in (Pernelle 98)).",
            "180": "ln on bon couteau pour sculpter (a good knife to carve), by default, the action that the knife performs well is that protypically found in its telic role.",
            "181": "But, if&apos; a less prototypical action is found explicitly in the, sentence, then this latter is prefered and incorporated into the semantic representation instead of the default case.",
            "182": "Indeed, the telic role describes prototypical actions, since the others are often unpredictable.",
            "183": "The default meaning of bon is kept and &apos;frozen&apos; until the whole sentence has been parsed.",
            "184": "If there is no contracdiction with that sense, then it is assigned to the adjective, otherwise, it is discarded in favor of the sense explicitly found in the sentence.",
            "185": "Finally, we consider the expressions Y makes a good X, Y is a good X as collocations where good is not fully treated (-:onipositionally."
          }
        }
      ],
      "doc_id": 7,
      "actual_doc_id": "C98-2182-parscit-section.xml"
    },
    {
      "content": [
        {
          "section": "abstract",
          "title": "Abstract",
          "sentences": {
            "0": "In this paper it will be shown how unification grammars can be used to build a reversible machine translation system.",
            "1": "Unification grammars are often used to define the relation between strings and meaning representations in a declarative way.",
            "2": "Such grammars are sometimes used in a bidirectional way, thus the same grammar is used for both parsing and generation.",
            "3": "In this paper I will show how to use bidirectional unification grammars to define r(st.i:r.sible relations between language dependent meaning representations.",
            "4": "Furthermore it is shown how to obtain a completely reversible MT system using a.",
            "5": "series of (bidirectional) unification grammars."
          }
        },
        {
          "section": "method",
          "title": "I Introduction",
          "sentences": {
            "6": "The notion of a reversible MT system was first expressed by tandsbergen [11].",
            "7": "Such a system will in 1;rinciple produce a set of possible translations, by employing linguistic knowledge only.",
            "8": "Choosing the best translation from the set of linguistically possible translations will usually require other sources of knowledge, either incorporated in the system or provided (interactively) by the user.",
            "9": "The relation &apos;possible translation&apos; is symmetric whereas the relation &apos;best translation&apos; is not.",
            "10": "Thus an MT system may consist of a reversible core, implementing the symmetric relation &apos;possible translation&apos;, and additional components (not necessarily reversible) to select the best translation.",
            "11": "Not only is it possible to build reversible (modules of) MT systems; it has also been claimed that reversible systems are preferable.",
            "12": "For example Isabelle [6] claims that.",
            "13": "reversible MT systems are to be preferred to others because in reversible MT systems a better understanding of the translation relation is achieved; such systems will eventually exhibit better practical performance.",
            "14": "Moreover, the arguments in favour of using bidirectional grammars in NLP, such as those given in 11, 8] carry over to translation as well.",
            "15": "Because of the declarative nature of unification- and logic grammar formalisms grammars written in these formalisms are increasingly used In a bidirectional way, thus the same grammar is used for both parsing and generation.",
            "16": "Some recent developments are reported in [3, 24, 16, 21, 2, 18, 19, 22, 20].",
            "17": "In this paper I will show how to use such bidirectional unification grammars to build a completely reversible, multilingual, MT system.",
            "18": "For each language there is a unification grammar that defines a reversible relation between strings and language dependent meaning representations (logical forms).",
            "19": "Moreover, for each language pair (or set of languages) there is a unification grammar that defines a reversible relation between such language dependent logical forms.",
            "20": "Translation is thus defined by a series of three unification grammars.",
            "21": "A specific version of the system that is described here is implemented as the core of the experimental MiMo2 translation system [23].",
            "22": "This system aims at translating international news items on Teletext.",
            "23": "Apart from unification grammars the system uses a bidirectional two-level orthography component.",
            "24": "Language dependent meanings are represented as.sintple predicate argument structures with softie extra labels indicating &apos;universal&apos; meaning such as tense and aspect.",
            "25": "The current system (November 1989) includes grammars for Dutch, Spanish and English.",
            "26": "The paper is set up as follows.",
            "27": "In section 2, I will give some examples that show how bidirectional unification grammars can be used to define relations between logical forms of different languages.",
            "28": "In section 3, reversibility is defined in terms of symmetry and computability.",
            "29": "Possible approaches to obtain reversibility are discussed.",
            "30": "In section 4, 1 will compare the current approach with some other approaches in the unification based translation paradigm and discuss some problems and future directions ."
          }
        },
        {
          "section": "method",
          "title": "2 Unification-based Transfer",
          "sentences": {
            "31": "In this section I will give some examples of the use of a unification grammar (in PATR II [17] notation) to define the relation between language dependent logical forms.",
            "32": "For illustrative purposes I will assume logical forms are represented by feature structures consisting of the attributes pred, argi, arg2 together with some attributes representing &apos;universal&apos; meanings such as tense, aspect, number and parson; I will not touch upon issues such as quantification and modification.",
            "33": "The logical forms of English and Spanish are labeled by the attributes gb and.sp respectively.",
            "34": "As an example the logical form of &apos;The army opened lire at the civilians&apos; is represented as in figure 1.",
            "35": "Such feature structures will often be related in a straightforward way to a Spanish equivalent, except for the value of the prod attributes.",
            "36": "A very simple rule in PAM II style may look as in figure 2.",
            "37": "This rule simply states that the translation of a logical form is composed of the translation of its arguments.",
            "38": "If the rule applies to the feature structure in 1 the three daughters of the rule will be instantiated as in figure 3, and the value of sp will be bound to the sp values of these daughters.",
            "39": "An example of the rule for the first daughter will be a lexical entry and looks as in figure 4.",
            "40": "The simple English expression &apos;army&apos; has to be translated as a complex expression in.",
            "41": "Spanish: fuerza militar&apos;.",
            "42": "The rule will look as in 5 where it is assumed that the construction is analyzed in Spanish as an ordinary noun-adjective construction, and where the logical form of the adjective takes the logical form of the noun as its argument.",
            "43": "The translation for &apos;civilian&apos; is defined in a similar rule (although the translation of &apos;number&apos; is different).",
            "44": "Note that this example of complex transfer is similar to the famous &apos;schimmel - grey horse&apos; cases.",
            "45": "As a result of the rule In the foregoing examples the relation between logical forms is rather straightforward.",
            "46": "Note however that the full power of a unification grammar can be used to settle more difficult translation cases, because different attributes can be used to represent the &apos;translational syntax&apos;.",
            "47": "For instance we can build a tree as value of the attribute tree to represent the derivational history of the translation process.",
            "48": "Or we can &apos;thread&apos; information through different nodes to be able to make translations dependent on each other.",
            "49": "Translation parameters such as style and subject field can be percolated as attributes of nodes to obtain consistent translations; but these attributes themselves need not be translated ."
          }
        },
        {
          "section": "method",
          "title": "3 Reversible UnificationGrammars",
          "sentences": {
            "50": "A unification grammar defined in formalisms such as PATE.",
            "51": "II and DCG [12] usually defines a relation between a string of words and a logical form.",
            "52": "In signbased approaches such as UCG [26] and IIPSG [14] this string of words is not assigned a privileged status but is the value of one of the attributes of a feature structure.",
            "53": "I will assume a formalism similar to PATE.",
            "54": "but without the context-free base; the string is represented as the value of one of the attributes of a feature structure.",
            "55": "Thus more generally, unification grammars define relations between the values of two (or morel) attributes - for example the relation between the value of the attributes string and if, or between the value of the attributes sp and gb; these relations are all relations between feature structures.",
            "56": "I will call a binary relation reversible if the relation is symmetric and computable.",
            "57": "Both symmetry and computability will be explained in the following subsections.",
            "58": "A grammar G is reversible for a relation R if 1? is reversible and defined by G. For example, a grammar that relates strings to logical forms is reversible if both the parsing and generation problem is computable, and the relation between strings and logical forms is symmetric; the parsing problem is computable if for a given string all corresponding logical forms can be enumerated by some terminating procedure; such a procedure should halt if the given string does not have a corresponding logical form.",
            "59": "Thus: reversible = symmetric + computable.",
            "60": "Note that reversibility as defined here is different from bidirectionality.",
            "61": "The latter merely says that grammars are to be used in two directions, but does not state how the two directions relate.",
            "62": "It is easy to see that a composition of reversible relations is a a reversible relation too; i.e. if some feature structure Ii is related to some feature structure fn via the reversible relations Ri(fi, fi+i), each defined by some reversible grammar Gi, then le (fi, In) is reversible.",
            "63": "Thus an MT system that defines a relation R(.",
            "64": "a, St) via the relations R1(s,,, la), 112(1,, It) and R3(11, Si) is reversible if R1,2,3 are reversible.",
            "65": "A relation R C A xB is symmetric ill R(a, b) implies 1?(b, a&apos;) where a and a&apos; are equivalent.",
            "66": "For an MT system we want to define &apos;equivalence&apos; in such a way that the translation relation is a symmetric relation between strings.",
            "67": "However, strings are feature structures thus we must define equivalence for feature structures to obtain this effect.",
            "68": "Unification grammars as they are commonly used implement a rather weak notion of equivalence between feature structures: feature structures a and b are equivalent if they can Definition 1 (Weak equivalence) Two feature structures fi, f2 are weakly equivalent iff f2 exists.",
            "69": "If feature structures are taken to stand for all their ground instances this yields an acceptable version of synysetry.",
            "70": "Moreover, under the assumption that 1Note that it is possible to define a unification grammar that relates several language dependent logical forms; in this approach a multilingual transfer system consists of only one transfer grammar.",
            "71": "feature structures which represent strings are always ground (i.e.",
            "72": "these feature structures cannot be extended), this results in a symmetric relation between (feature structures that represent) strings.",
            "73": "It is also possible to define a &apos;strong&apos; notion of equivalence for feature structures that does not rely on this assumption.",
            "74": "Definition 2 (Strong equivalence) Two feature structures f2 are strongly equivalent (fi f2) if 12 E fi and ft E 12.",
            "75": "A grammar that defines a computable relation between two attributes under the strong definition of equivalence might be called strongly reversible.",
            "76": "Similarly a weakly reversible grammar is reversible under a weak definition of equivalence.",
            "77": "Again these results can be generalized to a series of unification grammars.",
            "78": "The strong version of equivalence can be motivated on the ground that it may be easier to obtain computability; this is the topic of the next subsection.",
            "79": "In section 3.2 I will discuss possible relaxations of the strong version of equivalence to obtain &apos;mildly&apos; reversible grammars.",
            "80": "A relation RC A xB is computable if for a given a E A the set {b E B1R(a,b)} can be enumerated by some terminating procedure.",
            "81": "To discuss computability it is useful to look a bit more careful at the relations we are interested in.",
            "82": "These relations are all binary relations between feature structures.",
            "83": "However, in the case of the relation between strings and logical forms, strings will always be related to logical forms and logical forms will be related to strings.",
            "84": "Similarly for the relation between Dutch and Spanish logical forms.",
            "85": "Clearly, the domain and range of the relation is structured and can be partioned into two sets A and B, for example the set of feature structures representing strings and the set of feature structures representing logical forms.",
            "86": "The relation RCAUBxAUBcan be partitioned similarly into the relations rCAx 13 and its inverse, r1 C B x A. The problem to compute R is now replaced by two problems: the computation of r and For example the problem to compute the relation between logical forms and strings consists of the parsing- and generation problem.",
            "87": "It is now possible to incorporate the notion of equivalence, to obtain a definition of a parser, generator and translator.",
            "88": "For example, an algorithm that computes the foregoing relation r will enumerate for a given features structure /1 all feature structures 12, such that r(f3, f2) and fi and h are equivalent.",
            "89": "In the case of strong equivalence this implies that Ii C f3 (completeness), and h C Ii (coherence).",
            "90": "In other words, the input should not be extended (coherence) and should completely be derived (completeness).",
            "91": "This usage of the terms completeness and coherence was introduced in [24].",
            "92": "In the following I will discuss ways to obtain computability of one such partition.",
            "93": "It is well known that relations defined by unrestricted unification grammars are not computable in general as such grammars have Turing power [13]; it is thus not decidable whether the relation is defined for some given input.",
            "94": "Usually some constraint on grammars is defined to remedy this.",
            "95": "For example the off-line-parsability constraint [13, 5] ensures that the recognition problem is solvable.",
            "96": "Moreover this constraint also implies that the parsing problem as defined here is computable; i.e. the proof procedure will always terminate (because the constraint implies that there is a limit to the depth of possible parse trees for all strings of a given length).",
            "97": "However the off-line-parsability constraint assumes a context-free base of the formalism.",
            "98": "A generalization of the off-line-parsability constraint for any binary relation defined by unification grammars will consist of three parts; the first and third of these parts are usually implicit in the case of parsing.",
            "99": "First, the value of the input must be built in a wellbehaved compositional way.",
            "100": "For example in the case of parsing: each daughter of a rule dominates part of the string dominated by the mother of that rule.",
            "101": "Similarly for transfer and generation: each daughter of a rule has a value for If that is part of the value of If of the mother.",
            "102": "Second, a special condition is defined for rules where the input value of the mother is the same as the input value of one of the daughters.",
            "103": "For parsing such rules have exactly one daughter.",
            "104": "A chain of applications of such rules is disallowed by some constraint or other; this is the core of most definitions of the offline parsability.constraint.",
            "105": "For example in [13] such a chain is disallowed as the principal functor of a term may only occur once in a chain.",
            "106": "For a slightly more general definition, cf.",
            "107": "[5]. For generation and transfer a similar constraint can be defined.",
            "108": "In the terminology of [18, 19] the &apos;head&apos; of a rule is a daughter with the same logical form as its mother.",
            "109": "A chain of these heads must be disallowed.",
            "110": "Third, the input should not get extended during the proof procedure.",
            "111": "In the case of parsing this is achieved easily because the input is ground 2.",
            "112": "For generation and transfer this is not necessarily the case.",
            "113": "This is the point where the usefulness of the coherence condition comes in; the coherence requirement explicitly states that extension of the input is not allowed.",
            "114": "For this reason strong reversiblity may be easier to obtain than weak reversibility.",
            "115": "In the next subsection I will discuss two relaxations of strong symmetry that will not affect the computability properties discussed here.",
            "116": "Generalizing the terminology introduced by [13] a proof procedure is strongly stable if it always terminates for grammars that adhere to a generalized off-line parsability constraint.",
            "117": "In [15] a general proof procedure for DCG based on head-driven generation [18, 19, 22] is defined that is strongly stable for a specific instantiation of the generalized off-line parsability constraint.",
            "118": "2Note that this is the reason that most DCG parsers expect that the input value of the string has an atomic tail, i.e. parse([ john, kisses, maryb p) will work fine, but parseqjohn, kisses, mary14 X) will cause problems.",
            "119": "it is easy to see that the completeness and coherence requirements make life hard for the rulewriter as she/he needs to know exactly what the possible values of inputs are for some component.",
            "120": "It is possible to relax the completeness and coherence requirement in two ways that will not affect the reversibility properties between strings.",
            "121": "The usefulness of these relaxations depends on the analyses a user wishes to define.",
            "122": "The first relaxation assumes that there is a sort system defined for feature structures that makes it possible to make a distinction between cyclic and noncyclic attributes (cf.",
            "123": "[5]). For the moment a noncyclic attribute may be defined as an attribute with a finite number of possible values (i.e.",
            "124": "it is not recursive).",
            "125": "For example the attributes aryl and arg2 will be cyclic whereas number will be non-cyclic.",
            "126": "The completeness and coherence condition is restricted to cyclic attributes.",
            "127": "As the proof procedure can only further instantiate non-cyclic attributes no termination problems occur because there are only a finite number of possibilities to do tins.",
            "128": "The definition of &apos;equivalence&apos; for feature structures is now slightly changed.",
            "129": "To define this properly it is necessary to define the notion noncyclic extension.",
            "130": "A non-cyclic extension of a feature structure only instantiates non-cyclic attributes.",
            "131": "This results in the following definition of equivalence: Definition 3 (Non-cyclic equivalent) Two feature structures ft, f2 are non-cyclic equivalent if ff21 where A are non-cyclic extensions of fn.",
            "132": "It will be clear that the usefulness of this definition depends heavily on the style of grammar writing that is used.",
            "133": "Note that it is of course also possible to declare for each non-cyclic attribute whether the completeness and coherence requirements hold.",
            "134": "The second relaxation is not without ramifications for the organization of a transfer grammar.",
            "135": "This relaxation has to do with reentrancies in feature structures.",
            "136": "Some constructions such as control verbs and relative clauses may be represented using reentrancies; for example &apos;the soldiers tried to shoot the president&apos; may be represented by a feature structure where the first argument of &apos;try&apos; is reentrant with the first argument of &apos;shoot&apos;, cf.",
            "137": "figure 7.",
            "138": "The translation of such logical forms to Dutch equivalents can be defined as in rule 8.",
            "139": "In this rule the reentrancy is explicitly mentioned for two reasons.",
            "140": "The first reason is simply that in the case of different possible translations of aryl we want the same translation for both aryl and the embedded aryl.",
            "141": "Note that the translation of &apos;soldier&apos; into Dutch can be both `soldaat&apos; or `militair&apos;.",
            "142": "Tithe reentrancy is not mentioned the system has to try to generate from four different Dutch logical forms, two of which without matching aryl&apos;s.",
            "143": "The reentrancy is also mentioned because this is required by the completeness condition.",
            "144": "It is possible to relax the completeness and coherence condition with respect to these reentrancies, again without affecting the reversibility properties of the system by slightly changing the definition of equivalence.",
            "145": "There is a tradeoff between simplicity of the transfer grammar (in the presence of this relaxation) and the efficiency of the system.",
            "146": "In the case of this relaxation the system will eventually find the good translations, but it may take a while.",
            "147": "On the other hand, if we are to mention all (possibly unbounded) reentrancies explicitly then the transfer grammar will have to be complicated by a threading mechanism to derive such reentrancies.",
            "148": "Again, the specific use of reentrancies in the logical forms that are defined will determine whether this relaxation is desired or not."
          }
        },
        {
          "section": "method",
          "title": "4 Final remarks",
          "sentences": {
            "149": "The objective to build a reversible MT system using a series of unification grammars is similar to the objective of the CRITTER system as expressed in [3, 7], and the work of Zajac in [25].",
            "150": "Instead of using unification grammars CRITTER uses logic grammars; Zajac uses a type system including an inheritance mechanism to define transfer-like rules.",
            "151": "In these two approaches less attention is being paid to an exact definition of reversibility; although our work may be compatible with these approaches.",
            "152": "A somewhat different approach is advocated in [9].",
            "153": "In that approach a system is described where an LFC grammar for some source language is augmented with equations that define (part of) the target level representations.",
            "154": "A generator derives from this partial description a string according to some LEG grammar of the target language.",
            "155": "Instead of a series of three grammars this architecture thus assumes two grammars, one of which both defines the source language and the relation with the target language.",
            "156": "The translation relation is not only defined between logical forms but may relate all levels of representation ( c-structure, f-structure, a-structure).",
            "157": "Although in this approach monolingual grammars may be used in a bidirectional way it is unclear whether the translation equations can be used bidirectionally 3. An important problem for the approach advocated here is the problem of logical form equivalence.",
            "158": "Shieber [16] noted that unification grammars usually define a relation between strings and some canonical logical form.",
            "159": "Depending on the nature of logical forms that are being used, several representations of a logical form may have the same `meaning&apos;; just as in first order predicate calculus the formulas pV q and qV p are logically equivalent; a unification grammar will not know of these equivalences and, consequently, all equivalences have to be defined separately (if such equivalents are thought of as being translational equivalents); for example in a transfer grammar two rules may be defined to translate p V q into both p&apos; V q&apos; and q&apos; V p&apos; if these formulas are thought of as being equivalent.",
            "160": "Of course this technique can only be applied if the number of equivalences is finite.",
            "161": "It is not possible to define that p is equivalent with for any even number of The approach discussed so far can be extended just as unification grammars for parsing and generation have been extended.",
            "162": "Apart from equational constraints it will be useful to add others such as disjunction and negation.",
            "163": "Moreover it seems useful to allow some version of universal constraints or some inheritance mechanism to be able to express generalizations and exceptions more easily ."
          }
        },
        {
          "section": "acknowledgments",
          "title": "Acknowledgements",
          "sentences": {
            "164": "I want to thank joke Dorrepaal, Pim van der Eijk, Maria Florenza, Dirk lleylen, Steven Krauwer, Jan Landsbergen, Michael Moortga,t, Herbert Ruessink and Louis des Tombe.",
            "165": "I was supported by the European Community and the NBI3I through the Eurotra project."
          }
        }
      ],
      "doc_id": 8,
      "actual_doc_id": "C90-2052-parscit-section.xml"
    },
    {
      "content": [
        {
          "section": "abstract",
          "title": "Abstract",
          "sentences": {
            "0": "We describe the second installment of the Challenge on Generating Instructions in Virtual Environments (GIVE-2), a shared task for the NLG community which took place in 2009-10.",
            "1": "We evaluated seven NLG systems by connecting them to 1825 users over the Internet, and report the results of this evaluation in terms of objective and subjective measures."
          }
        },
        {
          "section": "introduction",
          "title": "1 Introduction",
          "sentences": {
            "2": "This paper reports on the methodology and results of the Second Challenge on Generating Instructions in Virtual Environments (GIVE-2), which we ran from August 2009 to May 2010.",
            "3": "GIVE is a shared task for the NLG community which we ran for the first time in 2008-09 (Koller et al., 2010).",
            "4": "An NLG system in this task must generate instructions which guide a human user in solving a treasure-hunt task in a virtual 3D world, in real time.",
            "5": "For the evaluation, we connect these NLG systems to users over the Internet, which makes it possible to collect large amounts of evaluation data cheaply.",
            "6": "While the GIVE-1 challenge was a success, in that it evaluated five NLG systems on data from 1143 game runs in the virtual environments, it was limited in that users could only move and turn in discrete steps in the virtual environments.",
            "7": "This made the NLG task easier than intended; one of the best-performing GIVE-1 systems generated instructions of the form “move three steps forward”.",
            "8": "The primary change in GIVE-2 compared to GIVE-1 is that users could now move and turn freely, which makes expressions like “three steps” meaningless, and makes it hard to predict the precise effect of instructing a user to “turn left”.",
            "9": "We evaluated seven NLG systems from six institutions in GIVE-2 over a period of three months from February to May 2010.",
            "10": "During this time, we collected 1825 games that were played by users from 39 countries, which is an increase of over 50% over the data we collected in GIVE1.",
            "11": "We evaluated each system both on objective measures (success rate, completion time, etc).",
            "12": "and subjective measures which were collected by asking the users to fill in a questionnaire.",
            "13": "We completely revised the questionnaire for the second challenge, which now consists of relatively fine-grained questions that can be combined into more high-level groups for reporting.",
            "14": "We also introduced several new objective measures, including the point in the game in which users lost or cancelled, and an experimental “back-to-base” task intended to measure how much users learned about the virtual world while interacting with the NLG system.",
            "15": "Plan of the paper.",
            "16": "The paper is structured as follows.",
            "17": "In Section 2, we describe and motivate the GIVE-2 Challenge.",
            "18": "In section 3, we describe the evaluation method and infrastructure.",
            "19": "Section 4 reports on the evaluation results.",
            "20": "Finally, we conclude and discuss future work in Section 5."
          }
        },
        {
          "section": "method",
          "title": "2 The GIVE Challenge",
          "sentences": {
            "21": "GIVE-2 is the second installment of the GIVE Challenge (“Generating Instructions in Virtual Environments”), which we ran for the first time in 2008-09.",
            "22": "In the GIVE scenario, subjects try to solve a treasure hunt in a virtual 3D world that they have not seen before.",
            "23": "The computer has a complete symbolic representation of the virtual world.",
            "24": "The challenge for the NLG system is to generate, in real time, natural-language instructions that will guide the users to the successful completion of their task.",
            "25": "Users participating in the GIVE evaluation start the 3D game from our website at www.",
            "26": "give-challenge.org. They then see a 3D game window as in Fig.",
            "27": "1, which displays instructions and allows them to move around in the world and manipulate objects.",
            "28": "The first room is a tutorial room where users learn how to interact with the system; they then enter one of three evaluation worlds, where instructions for solving the treasure hunt are generated by an NLG system.",
            "29": "Users can either finish a game successfully, lose it by triggering an alarm, or cancel the game.",
            "30": "This result is stored in a database for later analysis, along with a complete log of the game.",
            "31": "In each game world we used in GIVE-2, players must pick up a trophy, which is in a wall safe behind a picture.",
            "32": "In order to access the trophy, they must first push a button to move the picture to the side, and then push another sequence of buttons to open the safe.",
            "33": "One floor tile is alarmed, and players lose the game if they step on this tile without deactivating the alarm first.",
            "34": "There are also a number of distractor buttons which either do nothing when pressed or set off an alarm.",
            "35": "These distractor buttons are intended to make the game harder and, more importantly, to require appropriate reference to objects in the game world.",
            "36": "Finally, game worlds contained a number of objects such as chairs and flowers that did not bear on the task, but were available for use as landmarks in spatial descriptions generated by the NLG systems.",
            "37": "The crucial difference between this task and the (very similar) GIVE-1 task was that in GIVE2, players could move and turn freely in the virtual world.",
            "38": "This is in contrast to GIVE-1, where players could only turn by 90 degree increments, and jump forward and backward by discrete steps.",
            "39": "This feature of the way the game controls were set up made it possible for some systems to do very well in GIVE-1 with only minimal intelligence, using exclusively instructions such as “turn right” and “move three steps forward”.",
            "40": "Such instructions are unrealistic – they could not be carried over to instruction-giving in the real world –, and our aim was to make GIVE harder for systems that relied on them."
          }
        },
        {
          "section": "method",
          "title": "3 Method",
          "sentences": {
            "41": "Following the approach from the GIVE-1 Challenge (Koller et al., 2010), we connected the NLG systems to users over the Internet.",
            "42": "In each game run, one user and one NLG system were paired up, with the system trying to guide the user to success in a specific game world.",
            "43": "We adapted the GIVE-1 software to the GIVE-2 setting.",
            "44": "The GIVE software infrastructure (Koller et al., 2009a) consists of three different modules: The client, which is the program which the user runs on their machine to interact with the virtual world (see Fig.",
            "45": "1); a collection of NLG servers, which generate instructions in real-time and send them to the client; and a matchmaker, which chooses a random NLG server and virtual world for each incoming connection from a client and stores the game results in a database.",
            "46": "The most visible change compared to GIVE-1 was to modify the client so it permitted free movement in the virtual world.",
            "47": "This change further necessitated a number of modifications to the internal representation of the world.",
            "48": "To support the development of virtual worlds for GIVE, we changed the file format for world descriptions to be much more readable, and provided an automatic tool for displaying virtual worlds graphically (see the screenshots in Fig.",
            "49": "2). Participants were recruited using email distribution lists and press releases posted on the Internet and in traditional newspapers.",
            "50": "We further advertised GIVE at the Cebit computer expo as part of the Saarland University booth.",
            "51": "Recruiting anonymous experimental subjects over the Internet carries known risks (Gosling et al., 2004), but we showed in GIVE-1 that the results obtained for the GIVE Challenge are comparable and more informative than those obtained from a laboratoryWorld 1 World 2 World 3 based experiment (Koller et al., 2009b).",
            "52": "We also tried to leverage social networks for recruiting participants by implementing and advertising a Facebook application.",
            "53": "Because of a software bug, only about 50 participants could be recruited in this way.",
            "54": "Thus tapping the true potential of social networks for recruiting participants remains a task for the next installment of GIVE.",
            "55": "Fig. 2 shows the three virtual worlds we used in the GIVE-2 evaluation.",
            "56": "Overall, the worlds were more difficult than the worlds used in GIVE-1, where some NLG-systems had success rates around 80% in some of the worlds.",
            "57": "As for GIVE-1, the three worlds were designed to pose different challenges to the NLG systems.",
            "58": "World 1 was intended to be more similar to the development world and last year’s worlds.",
            "59": "It did have rooms with more than one button of the same color, however, these buttons were not located close together.",
            "60": "World 2 contained several situations which required more sophisticated referring expressions, such as rooms with several buttons of the same color (some of them close together) and a grid of buttons.",
            "61": "Finally, World 3 was designed to exercise the systems’ navigation instructions: one room contained a “maze” of alarm tiles, and another room two long rows of buttons hidden in “booths” so that they were not all visible at the same time.",
            "62": "After the GIVE-2 Challenge was publicized in June 2009, fifteen researchers and research teams declared their interest in participating.",
            "63": "We distributed a first version of the software to these teams in August 2009.",
            "64": "In the end, six teams submitted NLG systems (two more than in GIVE-1); one team submitted two independent NLG systems, bringing the total number of NLG systems up to seven (two more than in GIVE-1).",
            "65": "These were connected to a central matchmaker that ran for a bit under three months, from 23 February to 17 May 2010.",
            "66": "Seven NLG systems were evaluated in GIVE-2: Detailed descriptions of these systems as well as each team’s own analysis of the evaluation results can be found at http://www. give-challenge.org/research."
          }
        },
        {
          "section": "evaluation",
          "title": "4 Results",
          "sentences": {
            "67": "We now report the results of GIVE-2.",
            "68": "We start with some basic demographics; then we discuss objective and subjective evaluation measures.",
            "69": "The data for the objective measures are extracted from the logs of the interactions; whereas the data for the subjective measures are obtained from a questionnaire which asked subjects to rate various aspects of the NLG system they interacted with.",
            "70": "Notice that some of our evaluation measures are in tension with each other: For instance, a system which gives very low-level instructions may allow the user to complete the task more quickly (there is less chance of user errors), but it will require more instructions than a system that aggregates these.",
            "71": "This is intentional, and emphasizes our desire to make GIVE a friendly comparative challenge rather than a competition with a clear winner.",
            "72": "Over the course of three months, we collected 1825 valid games.",
            "73": "This is an increase of almost 60% over the number of valid games we collected in GIVE-1.",
            "74": "A game counted as valid if the game client did not crash, the game was not marked as a test game by the developers, and the player completed the tutorial.",
            "75": "Of these games, 79.0% were played by males and 9.6% by females; a further 11.4% did not specify their gender.",
            "76": "These numbers are comparable to GIVE-1.",
            "77": "About 42% of users connected from an IP address in Germany; 12% from the US, 8% from France, 6% from Great Britain, and the rest from 35 further countries.",
            "78": "About 91% of the participants who answered the question self-rated their English language proficiency as “good” or better.",
            "79": "About 65% of users connected from various versions of Windows, the rest were split about evenly between Linux and MacOS.",
            "80": "The objective measures are summarize in Fig.",
            "81": "3. In addition to calculating the percentage of games users completed successfully when being guided by the different systems, we measured the time until task completion, the distance traveled until task completion, and the number of actions (such as pushing a button to open a door) executed.",
            "82": "Furthermore, we counted how many instructions users received from each system, and how many words these instructions contained on average.",
            "83": "All objective measures were collected completely unobtrusively, without requiring any action on the user’s part.",
            "84": "To ensure comparability, we only counted successfully completed games.",
            "85": "Fig. 4 shows the results of these objective measures.",
            "86": "Task success is reported as the percentage of successfully completed games.",
            "87": "The other measures are reported as the mean number of seconds/distance units/actions/instructions/words per instruction, respectively.",
            "88": "The figure also assigns systems to groups A, B, etc. for each evaluation measure.",
            "89": "For example, users interacting with systems in group A had a higher task success rate, needed less time, etc. than users interacting with systems in group B. If two systems do not share the same letter, the difference between these two systems is significant with p &lt; 0.05.",
            "90": "Significance was tested using a χ2-test for task success and ANOVAs for the other objective measures.",
            "91": "These were followed by post-hoc tests (pairwise χ2 and Tukey) to compare the NLG systems pairwise.",
            "92": "In terms of task success, the systems fall pretty neatly into four groups.",
            "93": "Note that systems D and T had very low task success rates.",
            "94": "That means that, for these systems, the results for the other objective measures may not be reliable because they are based on just a handful of games.",
            "95": "Another aspect in which systems clearly differed is how many words they used per instruction.",
            "96": "Interestingly, the three systems with the best task success rates also produced the most succinct instructions.",
            "97": "The distinctions between systems in terms of the other measures is less clear.",
            "98": "The subjective measures were obtained from responses to a questionnaire that was presented to users after each game.",
            "99": "The questionnaire asked users to rate different statements about the NLG system using a continuous slider.",
            "100": "The slider position was translated to a number between -100 and 100.",
            "101": "Figs. 7 and 6 show the statements that users were asked to rate as well as the results.",
            "102": "These results are based on all games, independent of the success.",
            "103": "We report the mean rating for each item, and, as before, systems that do not share a letter, were found to be significantly different (p &lt; 0.05).",
            "104": "We used ANOVAs and post-hoc Tukey tests to test for significance.",
            "105": "Note that some items make a positive statement about the NLG system (e.g.",
            "106": ", Q1) and some make a negative statement (e.g.",
            "107": ", Q2).",
            "108": "For negative statements, we report the reversed scores, so that in Figs.",
            "109": "7 and 6 greater numbers are always better, and systems in group A are always better than systems in group B. In addition to the items Q1–Q22, the questionnaire contained a statement about the overall instruction quality: “Overall, the system gave me good directions.” Furthermore notice that the other items fall into two categories: items that assess the quality of the instructions (Q1–Q15) and items that assess the emotional affect of the interaction (Q16–Q22).",
            "110": "The ratings in these categories can be aggregated into just two ratings by summing over them.",
            "111": "Fig. 5 shows the results for the overall question and the aggregated ratings for quality measures and emotional affect measures.",
            "112": "The three systems with the highest task success rate get rated highest for overall instruction quality.",
            "113": "The aggregated quality measure also singles out the same group of three systems.",
            "114": "In addition to the differences between NLG systems, some other factors also influence the outcomes of our objective and subjective measures.",
            "115": "As in GIVE-1, we find that there is a significant difference in task success rate for different evaluation worlds and between users with different levels of English proficiency.",
            "116": "Fig. 8 illustrates the effect of the different evaluation worlds on the task success rate for different systems, and Fig.",
            "117": "9 shows the effect that a player’s English skills have on the task success rate.",
            "118": "As in GIVE-1, some systems seem to be more robust than others with respect to changes in these factors.",
            "119": "None of the other factors we looked at (gender, age, and computer expertise) have a significant effect on the task success rate.",
            "120": "With a few exceptions the other objective measures were not influenced by these demographic factors either.",
            "121": "However, we do find a significant effect of age on the time and number of actions a player needs to retrieve the trophy: younger players are faster and need fewer actions.",
            "122": "And we find that women travel a significantly shorter distance than men on their way to the trophy.",
            "123": "Interestingly, we do not find D H M NA NM S T D H M NA NM S T Q1: The system used words and phrases that were easy to understand.",
            "124": "a significant effect of gender on the time players need to retrieve the trophy as in GIVE-1 (although the mean duration is somewhat higher for female than for male players; 481 vs. 438 seconds)."
          }
        },
        {
          "section": "conclusions",
          "title": "5 Conclusion",
          "sentences": {
            "125": "In this paper, we have described the setup and results of the Second GIVE Challenge.",
            "126": "Altogether, we collected 1825 valid games for seven NLG systems over a period of three months.",
            "127": "Given that this is a 50% increase over GIVE-1, we feel that this further justifies our basic experimental methodology.",
            "128": "As we are writing this, we are preparing detailed results and analyses for each participating team, which we hope will help them understand and improve the performance of their systems.",
            "129": "The success rate is substantially worse in GIVE2 than in GIVE-1.",
            "130": "This is probably due to the Figure 10: Points at which players lose/cancel.",
            "131": "harder task (free movement) explained in Section 2 and to the more complex evaluation worlds (see Section 3.3).",
            "132": "It was our intention to make GIVE-2 more difficult, although we did not anticipate such a dramatic drop in performance.",
            "133": "GIVE2.5 next year will use the same task as GIVE-2 and we hope to see an increase in task success as the participating research teams learn from this year’s results.",
            "134": "It is also noticeable that players gave mostly negative ratings in response to statements about immersion and engagement (Q16-Q20).",
            "135": "We discussed last year how to make the task more engaging on the one hand and how to manage expectations on the other hand, but none of the suggested solutions ended up being implemented.",
            "136": "It seems that we need to revisit this issue.",
            "137": "Another indication that the task may not be able to capture participants is that the vast majority of cancelled and lost games end in the very beginning.",
            "138": "To analyze at what point players lose or give up, we divide the game into phases demarcated by manipulations of buttons that belong to the 6button safe sequence.",
            "139": "Fig. 10 illustrates in which phase of the game players lose or cancel.",
            "140": "We are currently preparing the GIVE-2.5 Challenge, which will take place in 2010-11.",
            "141": "GIVE-2.5 will be very similar to GIVE-2, so that GIVE-2 systems will be able to participate with only minor changes.",
            "142": "In order to support the development of GIVE-2.5 systems, we have collected a multilingual corpus of written English and German instructions in the GIVE-2 environment (Gargett et al., 2010).",
            "143": "We expect that GIVE-3 will then extend the GIVE task substantially, perhaps in the direction of full dialogue or of multimodal interaction.",
            "144": "Acknowledgments. GIVE-2 was only possible through the support and hard work of a number of colleagues, especially Konstantina Garoufi (who handled the website and other publicity-related issues), Ielka van der Sluis (who contributed to the design of the GIVE-2 questionnaire), and several student assistants who programmed parts of the GIVE-2 system.",
            "145": "We thank the press offices of Saarland University, the University of Edinburgh, and Macquarie University for their helpful press releases.",
            "146": "We also thank the organizers of Generation Challenges 2010 and INLG 2010 for their support and the opportunity to present our results, and the seven participating research teams for their contributions."
          }
        }
      ],
      "doc_id": 9,
      "actual_doc_id": "W10-4233-parscit-section.xml"
    },
    {
      "content": [
        {
          "section": "abstract",
          "title": "Abstract",
          "sentences": {
            "0": "The present work advances the accuracy and training speed of discriminative parsing.",
            "1": "Our discriminative parsing method has no generative component, yet surpasses a generative baseline on constituent parsing, and does so with minimal linguistic cleverness.",
            "2": "Our model can incorporate arbitrary features of the input and parse state, and performs feature selection incrementally over an exponential feature space during training.",
            "3": "We demonstrate the flexibility of our approach by testing it with several parsing strategies and various feature sets.",
            "4": "Our implementation is freely available at: http://nlp.cs.nyu.edu/parser/."
          }
        },
        {
          "section": "introduction",
          "title": "1 Introduction",
          "sentences": {
            "5": "Discriminative machine learning methods have improved accuracy on many NLP tasks, including POS-tagging, shallow parsing, relation extraction, and machine translation.",
            "6": "Some advances have also been made on full syntactic constituent parsing.",
            "7": "Successful discriminative parsers have relied on generative models to reduce training time and raise accuracy above generative baselines (Collins & Roark, 2004; Henderson, 2004; Taskar et al., 2004).",
            "8": "However, relying on information from a generative model might prevent these approaches from realizing the accuracy gains achieved by discriminative methods on other NLP tasks.",
            "9": "Another problem is training speed: Discriminative parsers are notoriously slow to train.",
            "10": "In the present work, we make progress towards overcoming these obstacles.",
            "11": "We propose a flexible, end-to-end discriminative method for training parsers, demonstrating techniques that might also be useful for other structured prediction problems.",
            "12": "The proposed method does model selection without ad-hoc smoothing or frequency-based feature cutoffs.",
            "13": "It requires no heuristics or human effort to optimize the single important hyper-parameter.",
            "14": "The training regime can use all available information from the entire parse history.",
            "15": "The learning algorithm projects the hand-provided features into a compound feature space and performs incremental feature selection over this large feature space.",
            "16": "The resulting parser achieves higher accuracy than a generative baseline, despite not using a generative model as a feature.",
            "17": "Section 2 describes the parsing algorithm.",
            "18": "Section 3 presents the learning method.",
            "19": "Section 4 presents experiments with discriminative parsers built using these methods.",
            "20": "Section 5 compares our approach to related work."
          }
        },
        {
          "section": "method",
          "title": "2 Parsing Algorithm",
          "sentences": {
            "21": "The following terms will help to explain our work.",
            "22": "A span is a range over contiguous words in the input.",
            "23": "Spans cross if they overlap but neither contains the other.",
            "24": "An item is a (span, label) pair.",
            "25": "A state is a partial parse, i.e. a set of items, none of whose spans may cross.",
            "26": "A parse inference is a (state, item) pair, i.e. a state and an item to be added to it.",
            "27": "The frontier of a state consists of the items with no parents yet.",
            "28": "The children of a candidate inference are the frontier items below the item to be inferred, and the head of a candidate inference is the child item chosen by English head rules (Collins, 1999, pp.",
            "29": "238–240). A parse path is a sequence of parse inferences.",
            "30": "For some input sentence and training parse tree, a state is correct if the parser can infer zero or more additional items to obtain the training parse tree, and an inference Given input sentence s, the parser searches for parse pˆ out of the possible parses P(s): Section 3.1 describes how to compute cΘ(i).",
            "31": "Because cΘ(i) ∈ R+, the cost of a partial parse monotonically increases as we add items to it.",
            "32": "The parsing algorithm considers a succession of states.",
            "33": "The initial state contains terminal items, whose labels are the POS tags given by the tagger of Ratnaparkhi (1996).",
            "34": "Each time we pop a state from the agenda, cΘ computes the costs for the candidate bottom-up inferences generated from that state.",
            "35": "Each candidate inference results in a successor state to be placed on the agenda.",
            "36": "The cost function cΘ can consider arbitrary properties of the input and parse state.",
            "37": "We are not aware of any tractable solution to Equation 1, such as dynamic programming.",
            "38": "Therefore, the parser finds pˆ using a variant of uniform-cost search.",
            "39": "The parser implements the search using an agenda that stores entire states instead of single items.",
            "40": "Each time a state is popped from the agenda, the parser uses depth-first search starting from the state that was popped until it (greedily) finds a complete parse.",
            "41": "In preliminary experiments, this search strategy was faster than standard uniformcost search (Russell & Norvig, 1995)."
          }
        },
        {
          "section": "method",
          "title": "3 Training Method",
          "sentences": {
            "42": "Our training set I consists of candidate inferences from the parse trees in the training data.",
            "43": "From each training inference i ∈ I we generate the tuple hX(i), y(i), b(i)i.",
            "44": "X(i) is a feature vector describing i, with each element in {0, 1}.",
            "45": "We will use Xf(i) to refer to the element of X(i) that pertains to feature f.",
            "46": "y(i) = +1 if i is correct, and y(i) = −1 if not.",
            "47": "Some training examples might be more important than others, so each is given a bias b(i) ∈ R+, as detailed in Section 3.3.",
            "48": "The goal during training is to induce a hypothesis hΘ(i), which is a real-valued inference scoring function.",
            "49": "In the present work, hΘ is a linear model parameterized by a real vector Θ, which has one The sign of hΘ(i) predicts the y-value of i and the magnitude gives the confidence in this prediction.",
            "50": "The training procedure optimizes Θ to minimize the expected risk RΘ over training set I. RΘ is the objective function, a combination of loss function LΘ and regularization term ΩΘ: The loss of the inference set decomposes into the loss of individual inferences: In principle, lΘ can be any loss function, but in the present work we use the log-loss (Collins et al., 2002): ΩΘ in Equation 4 is a regularizer, which penalizes complex models to reduce overfitting and generalization error.",
            "51": "We use the `1 penalty: where λ is a parameter that controls the strength of the regularizer.",
            "52": "This choice of objective RΘ is motivated by Ng (2004), who suggests that, given a learning setting where the number of irrelevant features is exponential in the number of training examples, we can nonetheless learn effectively by building decision trees to minimize the `1regularized log-loss.",
            "53": "On the other hand, Ng (2004) suggests that most of the learning algorithms commonly used by discriminative parsers will overfit when exponentially many irrelevant features are present.1 Learning over an exponential feature space is the very setting we have in mind.",
            "54": "A priori, we define only a set A of simple atomic features (given in Section 4).",
            "55": "The learner then induces compound features, each of which is a conjunction of possibly negated atomic features.",
            "56": "Each atomic feature can have one of three values (yes/no/don’t care), so the size of the compound feature space is 3|A|, exponential in the number of atomic features.",
            "57": "It was also exponential in the number of training examples in our experiments (|A |≈ |I|).",
            "58": "We use an ensemble of confidence-rated decision trees (Schapire & Singer, 1999) to represent hΘ.2 The path from the root to each node n in a decision tree corresponds to some compound feature f, and we write ϕ(n) = f.",
            "59": "To score an inference i using a decision tree, we percolate the inference’s features X(i) down to a leaf n and return confidence Θϕ(n).",
            "60": "An inference i percolates down to node n iff Xϕ(n) = 1.",
            "61": "Each leaf node n keeps track of the parameter value Θϕ(n).3 The score hΘ(i) given to an inference i by the whole ensemble is the sum of the confidences returned by the trees in the ensemble.",
            "62": "Listing 1 Outline of training algorithm.",
            "63": "Listing 1 presents our training algorithm.",
            "64": "At the beginning of training, the ensemble is empty, Θ = 0, and the `1 parameter λ is set to ∞ (Steps 1.2 and 1.3).",
            "65": "We train until the objective cannot be further reduced for the current choice of λ.",
            "66": "We then determine the accuracy of the parser on a held-out development set using the previous λ value (before it was decreased), and stop training when this 2Turian and Melamed (2005) reported that decision trees applied to parsing have higher accuracy and training speed than decision stumps, so we build full decision trees rather than stumps.",
            "67": "3Any given compound feature can appear in more than one tree, but each leaf node has a distinct confidence value.",
            "68": "For simplicity, we ignore this possibility in our discussion.",
            "69": "accuracy reaches a plateau (Step 1.4).",
            "70": "Otherwise, we relax the regularization penalty by decreasing λ (Steps 1.6 and 1.7) and continue training.",
            "71": "In this way, instead of choosing the best λ heuristically, we can optimize it during a single training run (Turian & Melamed, 2005).",
            "72": "Each training iteration (Steps 1.5–1.13) has several steps.",
            "73": "First, we choose some compound features that have high magnitude gradient with respect to the objective function.",
            "74": "We do this by building a new decision tree, whose leaves represent the chosen compound features (Steps 1.5– 1.9).",
            "75": "Second, we confidence-rate each leaf to minimize the objective over the examples that percolate down to that leaf (Steps 1.10–1.12).",
            "76": "Finally, we append the decision tree to the ensemble and update parameter vector Θ accordingly (Step 1.13).",
            "77": "In this manner, compound feature selection is performed incrementally during training, as opposed to a priori.",
            "78": "Our strategy minimizing the objective RΘ(I) (Equation 4) is a variant of steepest descent (Perkins et al., 2003).",
            "79": "To compute the gradient of the unpenalized loss LΘ with respect to the parameter Θf of feature f, we have: Using Equation 6, we define the weight of an example i under the current model as the rate at which loss decreases as the margin of i increases: We define the gain of feature f as: Equation 14 has this form because the gradient of the penalty term is undefined at Θf = 0.",
            "80": "This discontinuity is why `1 regularization tends to produce sparse models.",
            "81": "If GΘ(I; f) = 0, then the objective RΘ(I) is at its minimum with respect to parameter Θf.",
            "82": "Otherwise, GΘ(I; f) is the magnitude of the gradient of the objective as we adjust of in the appropriate direction.",
            "83": "To build each decision tree, we begin with a root node.",
            "84": "The root node corresponds to a dummy “always true” feature.",
            "85": "We recursively split nodes by choosing a splitting feature that will allow us to increase the gain.",
            "86": "Node n with corresponding compound feature cp(n) = f can be split by atomic feature a if: Go(I; f ∧ a) + Go(I; f ∧ tea) > Go(I; f) (15) If no atomic feature satisfies the splitting criterion in Equation 15, then n becomes a leaf node of the decision tree and ocp(n) becomes one of the values to be optimized during the parameter update step.",
            "87": "Otherwise, we choose atomic feature aˆ to split node n: Parameter update is done sequentially on only the most recently added compound features, which correspond to the leaves of the new decision tree.",
            "88": "After the entire tree is built, we percolate examples down to their appropriate leaf nodes.",
            "89": "We then choose for each leaf node n the parameter ocp(n) that minimizes the objective over the examples in that leaf.",
            "90": "A convenient property of decision trees is that the leaves’ compound features are mutually exclusive.",
            "91": "Their parameters can be directly optimized independently of each other using a line search over the objective.",
            "92": "We choose a single correct path from each training parse tree, and the training examples correspond to all candidate inferences considered in every state along this path.4 In the deterministic setting there is only one correct path, so example generation is identical to that of Sagae and Lavie (2005).",
            "93": "If parsing proceeds non-deterministically then there might be multiple paths that lead to the same final parse, so we choose one randomly.",
            "94": "This method of generating training examples does not require a working parser and can be run prior to any training.",
            "95": "The disadvantage of this approach is that it minimizes the error of the parser at correct states only.",
            "96": "It does not account for compounded error or teach the parser to recover from mistakes gracefully.",
            "97": "Turian and Melamed (2005) observed that uniform example biases b(i) produced lower accuracy as training progressed, because the induced classifiers minimized the error per example.",
            "98": "To minimize the error per state, we assign every training state equal value and share half the value uniformly among the negative examples for the examples generated from that state and the other half uniformly among the positive examples.",
            "99": "We parallelize training by inducing 26 label classifiers (one for each non-terminal label in the Penn Treebank).",
            "100": "Parallelization might not uniformly reduce training time because different label classifiers train at different rates.",
            "101": "However, parallelization uniformly reduces memory usage because each label classifier trains only on inferences whose consequent item has that label."
          }
        },
        {
          "section": "method",
          "title": "4 Experiments",
          "sentences": {
            "102": "Discriminative parsers are notoriously slow to train.",
            "103": "For example, Taskar et al.",
            "104": "(2004) took several months to train on the < 15 word sentences in the English Penn Treebank (Dan Klein, p.c.).",
            "105": "The present work makes progress towards faster discriminative parser training: our slowest classifier took fewer than 5 days to train.",
            "106": "Even so, it would have taken much longer to train on the entire treebank.",
            "107": "We follow Taskar et al.",
            "108": "(2004) in training and testing on < 15 word sentences in the English Penn Treebank (Taylor et al., 2003).",
            "109": "We used sections 02–21 for training, section 22 for development, and section 23 for testing, preprocessed as per Table 1.",
            "110": "We evaluated our parser using the standard PARSEVAL measures (Black et al., 1991): labelled precision, labelled recall, and labelled F-measure (Prec.",
            "111": ", Rec.",
            "112": ", and F1, respectively), which are based on the number of nonterminal items in the parser’s output that match those in the gold-standard parse.5 As mentioned in Section 2, items are inferred bottom-up and the parser cannot infer any item that crosses an item already in the state.",
            "113": "Although there are O(n2) possible (span, label) pairs over a frontier containing n items, we reduce this to the � 5 · n inferences that have at most five children.6 Table 1 Steps for preprocessing the data.",
            "114": "Starred steps are performed only when parse trees are available in the data (e.g.",
            "115": "not on test data).",
            "116": "aAs pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation might discard useful information.",
            "117": "Collins and Roark (2004) saw a LFMS improvement of 0.8% over their baseline discriminative parser after adding punctuation features, one of which encoded the sentence-final punctuation.",
            "118": "To ensure the parser does not enter an infinite loop, no two items in a state can have both the same span and the same label.",
            "119": "Given these restrictions on candidate inferences, there were roughly 40 million training examples generated in the training set.",
            "120": "These were partitioned among the 26 constituent label classifiers.",
            "121": "Building a decision tree (Steps 1.5–1.9 in Listing 1) using the entire example set I can be very expensive.",
            "122": "We estimate loss gradients (Equation 13) using a sample of the inference set, which gives a 100-fold increase in training speed (Turian & Melamed, 2006).",
            "123": "Our atomic feature set A contains 300K features, each of the form “is there an item in group J whose label/headword/headtag/headtagclass is ‘X’?”.7 Possible values of ‘X’ for each predicate are collected from the training data.",
            "124": "For 1 < n < 3, possible values for J are: The left and right context items are the frontier items to the left and right of the children of the candidate inference, respectively.",
            "125": "To demonstrate the flexibility of our learning procedure, we trained three different parsers: left-to-right (l2r), right-to-left (r2l), ment set have more than five children.",
            "126": "7The predicate headtagclass is a supertype of the headtag.",
            "127": "Given our compound features, these are not strictly necessary, but they accelerate training.",
            "128": "An example is “proper noun,” which contains the POS tags given to singular and plural proper nouns.",
            "129": "Space constraints prevent enumeration of the headtagclasses, which are instead provided at the URL given in the abstract.",
            "130": "Table 2 Results on the development set, training and testing using only < 15 word sentences.",
            "131": "A active % Rec.",
            "132": "% Prec.",
            "133": "F1 features l2r 0.040 11.9K 89.86 89.63 89.74 b.u. 0.020 13.7K 89.92 89.84 89.88 r2l 0.014 14.0K 90.66 89.81 90.23 and non-deterministic bottom-up (b.u.).",
            "134": "The non-deterministic parser was allowed to choose any bottom-up inference.",
            "135": "The other two parsers were deterministic: bottom-up inferences had to be performed strictly left-to-right or rightto-left, respectively.",
            "136": "We stopped training when each parser had 15K active features.",
            "137": "Figure 1 shows the accuracy of the different runs over the development set as training progressed.",
            "138": "Table 2 gives the PARSEVAL scores of these parsers at their optimal Z&apos;1 penalty setting.",
            "139": "We found that the perplexity of the r2l model was low so that, in 85% of the sentences, its greedy parse was the optimal one.",
            "140": "The l2r parser does poorly because its decisions were more difficult than those of the other parsers.",
            "141": "If it inferred far-right items, it was more likely to prevent correct subsequent inferences that were to the left.",
            "142": "But if it inferred far-left items, then it went against the right-branching tendency of English sentences.",
            "143": "The left-to-right parser would likely improve if we were to use a left-corner transform (Collins & Roark, 2004).",
            "144": "Parsers in the literature typically choose some local threshold on the amount of search, such as a maximum beam width.",
            "145": "With an accurate scoring function, restricting the search space using a fixed beam width might be unnecessary.",
            "146": "Instead, we imposed a global threshold on exploration of the search space.",
            "147": "Specifically, if the Figure 1 F1 scores on the development set of the Penn Treebank, using only ≤ 15 word sentences.",
            "148": "The x-axis shows the number of non-zero parameters in each parser, summed over all classifiers.",
            "149": "total number of non-zero parameters parser has found some complete parse and has explored at least 100K states (i.e.",
            "150": "scored at least 100K inferences), search stopped prematurely and the parser would return the (possibly sub-optimal) current best complete parse.",
            "151": "The l2r and r2l parsers never exceeded this threshold, and always found the optimal complete parse.",
            "152": "However, the non-deterministic bottom-up parser’s search was cut-short in 28% of the sentences.",
            "153": "The nondeterministic parser can reach each parse state through many different paths, so it searches a larger space than a deterministic parser, with more redundancy.",
            "154": "To gain a better understanding of the weaknesses of our parser, we examined a sample of 50 development sentences that the r2l parser did not get entirely correct.",
            "155": "Roughly half the errors were due to noise and genuine ambiguity.",
            "156": "The remaining errors fell into three types, occurring with roughly the same frequency: Figure 2 F1 scores of right-to-left parsers with different atomic feature sets on the development set of the Penn Treebank, using only ≤ 15 word sentences.",
            "157": "total number of non-zero parameters We compared our right-to-left parser with the baseline set of atomic features to one with a far richer atomic feature set, including unbounded context features, length features, and features of the terminal items.",
            "158": "This “kitchen sink” parser merely has access to many more item groups J, described in Table 3.",
            "159": "All features are all of the form given earlier, except for length features (Eisner & Smith, 2005).",
            "160": "Length features compute the size of one of the groups of items in the indented list in Table 3.",
            "161": "The feature determines if this length is equal to/greater than to n, 0 ≤ n ≤ 15.",
            "162": "The kitchen sink parser had 1.1 million atomic features, 3.7 times the number available in the baseline.",
            "163": "In future work, we plan to try linguistically more sophisticated features (Charniak & Johnson, 2005) as well as sub-tree features (Bod, 2003; Kudo et al., 2005).",
            "164": "Figure 2 shows the accuracy of the right-toleft parsers with different atomic feature sets over the development set as training progressed.",
            "165": "Even though the baseline training made progress more quickly than the kitchen sink, the kitchen sink’s F1 surpassed the baseline’s F1 early in training, and at 6.3K active parameters it achieved a development set F1 of 90.55%.",
            "166": "To situate our results in the literature, we compare our results to those reported by Taskar et al.",
            "167": "(2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on ≤ 15 word sentences.",
            "168": "We also compare our parser to a representative non-discriminative Table 4 Results of parsers on the test set, training and testing using only < 15 word sentences.",
            "169": "parser (Bikel, 2004)8, the only one that we were able to train and test under exactly the same experimental conditions (including the use of POS tags from the tagger of Ratnaparkhi (1996)).",
            "170": "Table 4 shows the PARSEVAL results of these four parsers on the test set."
          }
        },
        {
          "section": "method",
          "title": "5 Comparison with Related Work",
          "sentences": {
            "171": "Our parsing approach is based upon a single endto-end discriminative learning machine.",
            "172": "Collins and Roark (2004) and Taskar et al.",
            "173": "(2004) beat the generative baseline only after using the standard trick of using the output from a generative model as a feature.",
            "174": "Henderson (2004) finds that discriminative training was too slow, and reports accuracy higher than generative models by discriminatively reranking the output of his generative model.",
            "175": "Unlike these state-of-the-art discriminative parsers, our method does not (yet) use any information from a generative model to improve training speed or accuracy.",
            "176": "As far as we know, we present the first discriminative parser that does not use information from a generative model to beat a 8Bikel (2004) is a “clean room” reimplementation of the Collins (1999) model with comparable accuracy.",
            "177": "generative baseline (the Collins model).",
            "178": "The main limitation of our work is that we can do training reasonably quickly only on short sentences because a sentence with n words generates O(n2) training inferences in total.",
            "179": "Although generating training examples in advance without a working parser (Turian & Melamed, 2005) is much faster than using inference (Collins & Roark, 2004; Henderson, 2004; Taskar et al., 2004), our training time can probably be decreased further by choosing a parsing strategy with a lower branching factor.",
            "180": "Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O(n) training examples.",
            "181": "An advantage of our approach is its flexibility.",
            "182": "As our experiments showed, it is quite simple to substitute in different parsing strategies.",
            "183": "Although we used very little linguistic information (the head rules and the POS tag classes), our model could also start with more sophisticated task-specific features in its atomic feature set.",
            "184": "Atomic features that access arbitrary information are represented directly without the need for an induced intermediate representation (cf.",
            "185": "Henderson, 2004).",
            "186": "Other papers (Clark & Curran, 2004; Kaplan et al., 2004, e.g). have applied log-linear models to parsing.",
            "187": "These works are based upon conditional models, which include a normalization term.",
            "188": "However, our loss function forgoes normalization, which means that it is easily decomposed into the loss of individual inferences (Equation 5).",
            "189": "Decomposition of the loss allows the objective to be optimized in parallel.",
            "190": "This might be an advantage for larger structured prediction problems where there are more opportunities for parallelization, for example machine translation.",
            "191": "The only important hyper-parameter in our method is the t1 penalty factor.",
            "192": "We optimize it as part of the training process, choosing the value that maximizes accuracy on a held-out development set.",
            "193": "This technique stands in contrast to more ad-hoc methods for choosing hyper-parameters, which may require prior knowledge or additional experimentation."
          }
        },
        {
          "section": "conclusions",
          "title": "6 Conclusion",
          "sentences": {
            "194": "Our work has made advances in both accuracy and training speed of discriminative parsing.",
            "195": "As far as we know, we present the first discriminative parser that surpasses a generative baseline on constituent parsing without using a generative component, and it does so with minimal linguistic cleverness.",
            "196": "Our approach performs feature selection incrementally over an exponential feature space during training.",
            "197": "Our experiments suggest that the learning algorithm is overfitting-resistant, as hypothesized by Ng (2004).",
            "198": "If this is the case, it would reduce the effort required for feature engineering.",
            "199": "An engineer can merely design a set of atomic features whose powerset contains the requisite information.",
            "200": "Then, the learning algorithm can perform feature selection over the compound feature space, avoiding irrelevant compound features.",
            "201": "In future work, we shall make some standard improvements.",
            "202": "Our parser should infer its own POS tags to improve accuracy.",
            "203": "A shift-reduce parsing strategy will generate fewer training inferences, and might lead to shorter training times.",
            "204": "Lastly, we plan to give the model linguistically more sophisticated features.",
            "205": "We also hope to apply the model to other structured prediction tasks, such as syntax-driven machine translation."
          }
        },
        {
          "section": "acknowledgments",
          "title": "Acknowledgments",
          "sentences": {
            "206": "The authors would like to thank Chris Pike, Cynthia Rudin, and Ben Wellington, as well as the anonymous reviewers, for their helpful comments and constructive criticism.",
            "207": "This research was sponsored by NSF grants #0238406 and #0415933."
          }
        }
      ],
      "doc_id": 10,
      "actual_doc_id": "P06-1110-parscit-section.xml"
    },
    {
      "content": [
        {
          "section": "abstract",
          "title": "AN ASSESSMENT OF SEMANTIC INFORMATION AUTOMATICALLYEXTRACTED FROM MACHINE READABLE DICTIONARIES",
          "sentences": {
          }
        },
        {
          "section": "introduction",
          "title": "ABSTRACT",
          "sentences": {
            "0": "In this paper we provide a quantitative evaluation of information automatically extracted from machine readable dictionaries.",
            "1": "Our results show that for any one dictionary, 55-70% of the extracted information is garbled in some way.",
            "2": "However, we show that these results can be dramatically reduced to about 6% by combining the information extracted from five dictionaries.",
            "3": "It therefore appears that even if individual dictionaries are an unreliable source of semantic information, multiple dictionaries can play an important role in building large lexical-semantic databases."
          }
        },
        {
          "section": "method",
          "title": "I. INTRODUCTION",
          "sentences": {
            "4": "In recent years, it has become increasingly clear that the limited size of existing computational lexicons and the poverty of the semantic information they contain represents one of the primary bottlenecks in the development of realistic natural language processing (NLP) systems.",
            "5": "The need for extensive lexical and semantic databases is evident in the recent initiation of a number of projects to construct massive generic lexicons for NLP (project GENELEX in Europe or EDR in Japan).",
            "6": "The manual construction of large lexical-semantic databases demands enormous human resources, and there is a growing body of research into the possibility of automatically extracting at least a part of the required lexical and semantic information from everyday dictionaries.",
            "7": "Everyday dictionaries are obviously not structured in a way that enables their immediate use in NLP systems, but several studies have shown that relatively simple procedures can be used to extract taxonomies and various other semantic relations (for example, Amster, 1980; Calzolari, 1984; Chodorow, Byrd, and Heidorn, 1985; Markowitz, Ahlswede, and Evens, 1986; Byrd et al., 1987; Nakarnura and Nagao, 1988; Vdronis and Ide, 1990i Klavans, Chodorow, and Wacholder, 1990; Wilks et al., 1990).",
            "8": "However, it remains to be seen whether information automatically extracted from dictionaries is sufficiently complete and coherent to be actually usable in NLP systems.",
            "9": "Although there is concern over the quality of automatically extracted lexical information, very few empirical studies have attempted to assess it systematically, and those that have done so have been restricted to consideration of the quality of grammatical information (e.g.",
            "10": ", Akkerman, Maserecuw, and Meijs, 1985).",
            "11": "No evaluation of automatically extracted semantic information has been published.",
            "12": "The authors would like to thank Lisa Lassck and Anne Gilman for their contribution to this work.",
            "13": "In this paper, we report thc results of a quantitative evaluation of automatically extracted semantic data.",
            "14": "Our results show that for any one dictionary, 55-70% of the extracted information is garbled in some way.",
            "15": "These results at first call into doubt the validity of automatic extraction from dictionaries.",
            "16": "However, in section 4 we show that these results can be dramatically reduced to about 6% by several means--most significantly, by combining the• information extracted from five dictionaries.",
            "17": "It therefore appears that even if individual dictionaries are an unreliable source of semantic information, multiple dictionaries can play an important role in building large lexical-semantic databases."
          }
        },
        {
          "section": "method",
          "title": "2. METHODOLOGY",
          "sentences": {
            "18": "Our strategy involves automatically extracting hypernyms from five English dictionaries for a limited corpus.",
            "19": "To determine where problems exist, the resulting hierarchies for each dictionary are compared to an &quot;ideal&quot; hierarchy constructed by hand.",
            "20": "The five dictionaries compared were: the Collins English Dictionary (CED), the Oxford Advanced Learner&apos;s Dictionary (OALD), the COBUILD Dictionary, the Long man&apos;s Dictionary of Contemporary English (LDOCE) and the Webster&apos;s 9th Dictionary (W9).",
            "21": "We begin with the most straightforward case in order to determine an upper bound for the results.",
            "22": "We deal with words within a domain which poses few modelling problems, and we focus on hyperonymy, which is probably the least arguable semantic relation and has been shown to be the easiest to extract.",
            "23": "If the results are poor under such favorable constraints, we can foresee that they will be poorer for more complex (abstract) domains and less clearly cut relations.",
            "24": "An ideal hierarchy probably does not exist for the entire dictionary; however, a fair degree of consensus seems possible for carefully chosen terms within a very restricted domain.",
            "25": "We have therefore selected a corpus of one hundred kitchen utensil terms, each representing a concrete, individual object--for example, cup, fork, saucepan, decanter, etc.",
            "26": "All of the terms are count nouns.",
            "27": "Mass nouns, which can cause problems, have been excluded (for example, the mass noun cutlery is not a hypernym of kn(e).",
            "28": "Other idiosyncratic cases, such as chopsticks (where it is not clear lithe utensil is one object or a pair of objects) have also been eliminated from the corpus.",
            "29": "This makes it easy to apply, simple tests for hyperonymy, which, for instance, enable us to say that Y is a hypernym of X if &quot;this is an X&quot; entails but is not entailed by &quot;this is a Y&quot; (Lyons, 1963).",
            "30": "Chodorow, Byrd, and Hcidorn (1985) proposed a heuristic for extracting hypernyms which exploits the fact that definitions for nouns typically give a hypernyrn - 227 term as the head of the defining noun phrase.",
            "31": "Consider the following examples: spoon a metal, wooden, or plastic utensil...",
            "32": "[CEDI In very general terms, the heuristic consists of extracting the word which precedes the first preposition, relative pronoun, or participle encountered in the definition text.",
            "33": "When this word is &quot;empty&quot; (e.g.",
            "34": "one, any, kind, class) the true hypernym is the head of the noun phrase following the preposition of: Automatically extracted hierarchies arc necessarily tangled (Amster, 1980) because many words arc polysemous.",
            "35": "For example, in the CED, the word pan has the following senses (among others): pan l.a a wide metal vessel...",
            "36": "ICED) pan2 1 the leaf of the betel tree...",
            "37": "ICED) The CED also gives pan as the hypemym for saucepan, which taken together yields the hierarchy in figure 1.a.",
            "38": "The tangled hierarchy is problematic because, following the path upwards from saucepan, we find that saucepan can be a kind of leaf.",
            "39": "This is clearly erroneous.",
            "40": "A hierarchy utilizing senses rather than words would not be tangled, as shown in figure 1.b.",
            "41": "In our study, the hierarchy was disambiguated by hand.",
            "42": "Sense disambiguation in dictionary definitions is a difficult problem, and we will not address it here; this problem is the focus of much current research and is considered in depth elsewhere (e.g.",
            "43": ", Byrd et al., 1987; Byrd, 1989; Veronis and Ide, 1990; Klavans, Chodorow, and Wacholder, 1990; Wilks et al., 1990)."
          }
        },
        {
          "section": "evaluation",
          "title": "3. EVALUATION",
          "sentences": {
            "44": "Hierarchies constructed with methods such as those outlined in section 2 show, upon close inspection, several serious problems.",
            "45": "In this section, we describe the most pervasive problems and give their frequency in our five dictionaries.",
            "46": "The problems fall into two general types: those which arise because information in the dictionary is incomplete, and those which are the result of a lack of distinction among terms and the lack of a one-to-one mapping between terms and concepts, especially at the highest levels of the hierarchy.",
            "47": "The information in dictionaries is incomplete for two main reasons.",
            "48": "First, since a dictionary is typically the product of several lexicographers&apos; efforts and is constructed, revised, and updated over many years, there exist inconsistencies in the criteria by which the hypernyms given in definition texts arc chosen.",
            "49": "In addition, space and readability restrictions, on the one hand, and syntactic restrictions on phrasing, on the other, may dictate that certain information is unspecified in definition texts or left to be implied by other parts of the definition.",
            "50": "11.1. Attachment too high : 21-34% The most pervasive problem in automatically extracted hierarchies is the attachment of terms too high in the hierarchy.",
            "51": "It occurs in 21-34% of the definitions in our sample from the five dictionaries (figure 8).",
            "52": "For example, while pan and bottle are vessels in the CED, cup and bowl are simply containers, the hypernym of vessel.",
            "53": "Obviously, &quot;this is a cup&quot; and &quot;this is a bowl&quot; both entail (and are not entailed by) &quot;this is a vessel&quot;.",
            "54": "Further, other dictionaries give vessel as the hypcmym for cup and bowl.",
            "55": "Therefore, the attachment of cup and bowl to the higher-level term container seems to be an inconsistency within the CED.",
            "56": "The problem of attachment too high in the hierarchy occurs relatively randomly within a given dictionary.",
            "57": "In dictionaries with a controlled definition vocabulary (such as the LDOCE), the problem of attachment at high levels of the hierarchy results also from a lack of terms from which to choose.",
            "58": "For example, ladle and dipper are both attached to spoon in the LDOCE, although &quot;this is a dipper&quot; entails and is not entailed by &quot;this is a ladle&quot;.",
            "59": "There is no way that dipper could be defined as a ladle (as, for instance, in the CED), since ladle is not in the defining vocabulary.",
            "60": "As a result, hierarchies extracted from the LDOCE are consistently flat (figure 7).",
            "61": "3.1.2. Absent hypernyms : 0-3% In some cases, strategies like that of Chodorow, Byrd and Heidorn yield incorrect hypernyms, as in the following definitions: grill A grill is a part of a cooker...",
            "62": "[COBUILD] corkscrew a pointed spiral piece of metal...",
            "63": "[W9) dinner service a complete set of plates and dishes...",
            "64": "[LDOCE, not included in our corpus] The words part, piece, set, are clearly not hypernyms of the defined concepts: it is virtually meaningless to say that grill is a kind of part, or that corkscrew is a kind of piece.",
            "65": "In these cases, the head of the noun phrase serves to mark another relation: part-whole, member-class, etc.",
            "66": "It is easy to reject these and similar words (member, series, etc).",
            "67": "as hypernyms, since they form a closed list (Klavans, Chodorow, and Wacholder, 1990).",
            "68": "However, excluding these words leaves us with no hypernym.",
            "69": "We call these &quot;absent hypernyms&quot;; they occur in 0-3% of the definitions in our sample corpus (figure 8).",
            "70": "The absence of a hypernym in a given definition text does not necessarily imply that no hypernym exists.",
            "71": "For example, &quot;this is a corkscrew&quot; clearly entails (and is not entailed by) &quot;this is a device&quot; (the hypernym given by the COBUILD and the CED).",
            "72": "In many cases, the lack of a hypernym seems to be the result of concern over space and/or readability.",
            "73": "We can imagine, for example, that the definition for corkscrew could be more fully specified as &quot;a device consisting of a pointed spiral piece of metal...&quot; In such cases, lexicographers rely on the reader&apos;s ability to deduce that something made of metal, with a handle, used for pulling corks, can be called a device.",
            "74": "However, for some terms, such as cutlery or dinner service, it is not clear that a hypernym exists.",
            "75": "Note that we have voluntarily excluded problematic terms of this kind from our corpus, in ordet to restrict our evaluation to the best case.",
            "76": "Another problem results from the necessary choices that lexicographers must make in an attempt to specify a vessel leaf - 228 single superordinatc, when concepts in the real world overlap freely.",
            "77": "For instance, a saucepan can be said to be a pot as well as a pan. &quot;This is a saucepan&quot; entails both &quot;this is a pot&quot; (the hypcmym given by the CED and W9) as well as &quot;this is a pan&quot; (the hypernym given by the LDOCE, OALD, and COBUILD).",
            "78": "On the other hand, &quot;this is a pot&quot; does not entail and is not entailed by &quot;this is a pan&quot;, which is to say that pot and pan are not synonyms, nor is one the hypernym of the other.",
            "79": "In terms of classes, pan and pot are distinct but overlapping, and saucepan is a subset of their intersection (figure 2.a).",
            "80": "This is no longer a strict hierarchy since it includes merging branches (figure 2.b).",
            "81": "We will call it an &quot;overlapping hierarchy&quot;.",
            "82": "Although a tree representation of such a hierarchy is impossible, it presents no problems on either logical or computational grounds.",
            "83": "Assuming the above relations, it would be more logically correct to phrase the definition of saucepan as &quot;a pan AND a pot...&quot;.",
            "84": "However, lexicographers never use &quot;and&quot; in this way, but usually give only one of the alternatives.",
            "85": "For example, each of the five dictionaries in our study chooses either pot or pan as the genus term for saucepan.",
            "86": "When this occurs, one of the hypemyms is missing.",
            "87": "This problem arises in our sample corpus relatively frequently, 8-14% of the time depending on the dictionary (figure 8).",
            "88": "At the higher levels of the hierarchy, terms necessarily become more general, and they often become less clearly defined.",
            "89": "For example, most people will agree on whether some object falls into the category fork or spoon, but there is much less agreement on what objects are implements or utensils.",
            "90": "In addition, at the higher levels some concepts simply lack a term to designate them exactly.",
            "91": "As a result; there is confusion at the higher levels of hierarchies implicit in dictionary definitions.",
            "92": "For 7-10% of the terms in our corpus, definitions give a list of head nouns separated by the conjunction or, as in the following: utensil an implement, tool or container...",
            "93": "!Mt In this case, none of the three alternatives is a hypemym of utensil.",
            "94": "First, it is clearly not true that &quot;this is a utensil&quot; entails &quot;this is a container&quot;.",
            "95": "For the other two, it is not clear whether or not &quot;this is a utensil&quot; entails &quot;this is a tool&quot; and &quot;this is an implement&quot;, and it is even less clear that the reverse entailments do not apply.",
            "96": "Regarding the three terms as hypernyms of utensil would produce the hierarchy in figure 3.",
            "97": "However, by enumerating the paths upwards from spatula (defined as a utensil), we see that spatula is a kind of container, which is obviously incorrect.",
            "98": "This solution amounts to regarding the class of utensils as the intersection of the classes of implements, tools, and containers.",
            "99": "Regarding the conjunction or as denoting the union of these classes would be more correct on logical grounds, since if X is included in A or X is included in B, then X is included in A u B. This relation cannot be fitted into a tree, but it can be pictured as in figure 4.",
            "100": "However, this does not help to determine whether spatula is an implement, tool, or container, or some subset of the three.",
            "101": "In any case, lexicographers do not use or with a consistent, mathematical meaning.",
            "102": "Or-conjoined heads appear not to be usable in constructing hierarchical trees without considerable manipulation and addition of information.",
            "103": "It is well known that circularity exists in dictionary definitions, especially when concepts are high up in the hierarchy.",
            "104": "For instance, consider the definitions below: tool an implement, such as a hammer...",
            "105": "ICED] implement a piece of equipment; tool or utensil.",
            "106": "ICED] utensil an implement, tool or container...",
            "107": "ICED] Circular definitions yield hierarchies containing loops (figure 5.a).",
            "108": "Unlike merging branches, loops have no interpretation in terms of classes.",
            "109": "A loop asserts both that A is a sub-class of B and B is a sub-class of A, which yields A,= B. This is why Amster (1980) suggests merging circularly-defined concepts and regarding them as synonyms (figure 5.b).",
            "110": "However, in most cases this solution leads to erroneous results; it is clear, for example, that many implements, tools, and utensils (e.g.",
            "111": ", spatula) are not containers.",
            "112": "This problem is similar to the one cited above in section 3.2.1.",
            "113": "If dictionary definitions are to be interpreted in terms of set theoretical relations, a more complex mathematical treatment is required.",
            "114": "The definitions above can be represented by the following relations: tool C implement Implement c (equipment t..",
            "115": "tool u utensil) utensil c (implement u tool u container) which, once solved, do not equate tool, implement, and utensil, but instead define the overlapping classes in figure 6.",
            "116": "This representation is clearly more sound on logical grounds.",
            "117": "It still does not indicate exactly where spatula should appear (since we have no indication that it is not a container), but at least it shows that there may be some utensils which are not containers.",
            "118": "Although this representation is more intuitively accurate than the representation in figure 5.b, ultimately it goes too far in delineating the relations among terms.",
            "119": "In actual use, the distinctions among terms are much less clear-cut than figure 6 implies.",
            "120": "For instance, the figure indicates that all tools that are containers are also implements, but it is certainly not clear that humans would agree to this or use the terms in a manner consistent with this specification.",
            "121": "Dictionaries Altogether, the problems described in the sections above yield a 55-70% error rate in automatically extracted hierarchies.",
            "122": "Given that we have attempted to consider the most favorable case, it appears that any single dictionary, taken in isolation, is a poor source of automatically extracted semantic information.",
            "123": "This is made more evident in figure 7, which demonstrates the marked differences in hierarchies extracted from the CED and LDOCE for a small subset of our corpus.",
            "124": "A summary of our results appears in figure 8 ."
          }
        },
        {
          "section": "evaluation",
          "title": "4. REFINING",
          "sentences": {
            "125": "We have concluded that hierarchies extracted using strategies such as that of Chodorow, Byrd, and Hcidorn are seriously flawed, and are therefore likely to be unusable in NLP systems.",
            "126": "However, in this section we discuss various means to refine automatically extracted hierarchies, most of which can be performed automatically.",
            "127": "It is possible to use information provided in the differentiae of definition texts to refine hierarchies; for example, in the definition vessel any object USED AS a container...",
            "128": "ICED] the automatically extracted hypernym is object.",
            "129": "However, some additional processing of the definition text enables the extraction of container following the phrase &quot;used as&quot;.",
            "130": "It is also possible to use other definitions.",
            "131": "For example, the CED does not specify that knife and spoon are implements, but this information is provided in the definition of cutlery: cutlery implements used for eating SUCII AS knives, forks, and spoons.",
            "132": "ICED] The extraction of information from differentiae demands some extra parsing, which may be difficult for complex definitions.",
            "133": "Also, further research is required to determine which phrases function as markers for which kind of information, and to determine how consistent their use is.",
            "134": "More importantly, such information is sporadic, and its extraction may require more effort than the results warrant.",
            "135": "We therefore seek more &quot;brute force&quot; methods to improve automatically extracted hierarchies.",
            "136": "One of the most promising strategies for refining extracted information is the use of information from several dictionaries.",
            "137": "Hierarchies derived from individual dictionaries suffer from incompleteness, but it is extremely unlikely that the same information is consistently missing from all dictionaries.",
            "138": "For instance, the CED attaches cup to container, which is too high in the hierarchy, while the W9 attaches it lower, to vessel.",
            "139": "It is therefore possible to use taxonomic information from several dictionaries to fill in absent hypernyms, missing links, and to rectify cases of too high attachment.",
            "140": "To investigate this possibility, we merged the information extracted from the five English dictionaries in our database.",
            "141": "The individual data for the five dictionaries was organized in a table, as in figure 9.",
            "142": "Merging these hierarchies into a single hierarchy was accomplished automatically by applying a simple algorithm, which scans the table line-by-line, as follows: b) take the remaining cell or cells as the hypernym(s).",
            "143": "This algorithm must be applied recursively, since, for example, it may not yet be known when evaluating basin that container is a hypernym of vessel, and vessel is a hypernym of bowl, until those terms arc themselves processed.",
            "144": "Therefore, several passes through the table are required.",
            "145": "Note that if after applying the algorithm several terms are left as hypernyms for a given word, we effectively create an overlap in the hierarchy.",
            "146": "For example, saucepen is attached to both pot and pan, and fork is attached to tool, implement, and instrument.",
            "147": "We evaluate the quality of the resulting combined hierarchy using the same strategy applied in section 3.",
            "148": "It is interesting to note that in the merged hierarchy, all the absent hypernym problems (including absence due to or-heads) have been eliminated, since in every ease at least one of the five dictionaries gives a valid hypcmym.",
            "149": "In addition, almost all of the attachments too high in the hierarchy and missing overlaps have disappeared, although a few cases remain (5% and 1%, respectively).",
            "150": "None of the dictionaries, for instance, gives pot as the hypemym of teapot, although three of the five dictionaries give pot as the hypernym of coffeepot.",
            "151": "A larger dictionary database would enable the elimination of many of these remaining imperfections (for example, New Penguin English Dictionary, not included in our database, gives pot as a hypernym of teapot).",
            "152": "Merging dictionaries on a large scale assumes that it is possible to automatically map senses across them.",
            "153": "For our small sample, we mapped senses among dictionaries by hand.",
            "154": "We describe elsewhere a promising method to automatically accomplish sense mapping, using a spreading activation algorithm Ode and Vdronis, 1990).",
            "155": "There remain a number of circularly-defined hypemyms in the combined taxonomy, which demand additional consideration on theoretical grounds.",
            "156": "Circularly-defined terms tend to appear when lexicographers lack terms to designate certain concepts.",
            "157": "The fact that &quot;it is not impossible for what is intuitively recognized as a conceptual category to be without a label&quot; has already been noted (Cruse, 1986, p.",
            "158": "147). The lack of a specific term for a recognizable concept tends to occur more frequently at the higher levels of the hierarchy (and at the very lowest and most specific levels as well--e.g.",
            "159": ", there is no term to designate forks with two prongs).",
            "160": "This is probably because any language includes the most terms at the generic level (Brown, 1958), that is, the level of everyday, ordinary terms for objects and living things (dog, pencil, house, etc.).",
            "161": "Circularity, as well as the use of or-conjoined terms at the high levels of the hierarchy, results largely from the lexicographers&apos; efforts to approximate the terms they lack.",
            "162": "For example, there is no clear term to denote that category of objects which fall under any of the terms utensil, tool, implement, instrument, although this concept seems to exist.",
            "163": "Clearly, these terms are not strictly synonymous—there are, for example, utensils that one would not call tools (e.g.",
            "164": ", a colander).",
            "165": "If a term, let us say X, for the concept existed, then the definitions for utensil, tool, implement, and instrument - 231 could simply read &quot;an X that...&quot;.",
            "166": "Since this is not the case, lexicographers define each term with a list Including the others, which enables the delineation of a concept which encompasses all of them.",
            "167": "One way to resolve difficulties at the higher levels of extracted hierarchies is to introduce &quot;covert categories&quot;, that is, concepts which do not correspond to any particular word.",
            "168": "We therefore do not merge circular terms into a single concept, but instead create a common &quot;covert&quot; hypernym for all of them.",
            "169": "In this way, tool, utensil, implement; and instrument each appear in the hierarchy as kinds of INSTRUMENTALOBJECT (covert categories names are capitalized).",
            "170": "We need a means to determine when and where covert categories are necessary.",
            "171": "Circularities in dictionary definitions clearly indicate the presence of covert categories.",
            "172": "However, we obviously cannot use a single dictionary to determine them, because the loops contained in one dictionary rarely include all of the terms that may be involved in the &quot;constellation&quot; representing a given covert category.",
            "173": "For instance, the CED contains the loop tool-implement-utensil, while the COBUILD contains a loop for tool-instrument; this provides strong evidence that all four terms should be involved in a constellation.",
            "174": "Supporting information can he derived by looking at the hyponyms for each of the candidate terms in different dictionaries.",
            "175": "The word fork, for example, is defined as tool (COBUILD), implement (CED, OALD, W9), and instrument (LDOCE), while spoon is defined as object (COB UILD), utensil (CED, OALD), tool (LDOCE) and implement (W9),which adds further support to the idea that tool, utensil, instrument, and implement belong to the same constellation.",
            "176": "Even if it is relatively easy to automatically detect circularities, the final determination of which covert categories to create and the terms that are involved in them must be done manually.",
            "177": "However, this task is not as daunting as it may first appear, since it involves only the higher levels of the hierarchy, and likely involves a relatively small number of covert categories.",
            "178": "By merging five dictionaries, all but 6% of the problems found in individual dictionaries were eliminated (figure 8).",
            "179": "This result is made clear in figure 10, which includes the same small subset of the sample corpus as in the individual hierarchies given in figure 7.",
            "180": "Although there remain a few imperfections, the combined hierarchy is much more accurate and complete, and therefore more useful, than the hierarchy derived from any one of the dictionaries alone."
          }
        },
        {
          "section": "conclusions",
          "title": "5. CONCLUSION",
          "sentences": {
            "181": "The results of our study show that dictionaries can be a reliable source of automatically extracted semantic information.",
            "182": "Merging information from several dictionaries improved the quality of extracted information to an acceptable level.",
            "183": "However, these results were obtained for a selected corpus representing a best case situation.",
            "184": "It is likely that different results will be obtained for larger, less restricted cases.",
            "185": "Our results suggest that this is an encouraging line of research to pursue for refining automatically extracted information."
          }
        },
        {
          "section": "references",
          "title": "REFERENCES",
          "sentences": {
            "186": "saucepan frying) plate bowl cup tureen Five diction:tires"
          }
        }
      ],
      "doc_id": 11,
      "actual_doc_id": "E91-1040-parscit-section.xml"
    },
    {
      "content": [
        {
          "section": "abstract",
          "title": "ABSTRACT",
          "sentences": {
            "0": "In modeling the structure of task-related discourse using plans, it is important to distinguish between plans that the agent has adopted and is pursuing and those that are only being considered and explored, since the kinds of utterances arising from a particular domain plan and the patterns of reference to domain plans and movement within the plan tree are quite different in the two cases.",
            "1": "This paper presents a three-level discourse model that uses separate domain and exploration layers, in addition to a layer of discourse metaplans, allowing these distinct behavior patterns and the plan adoption and reconsideration moves they imply to be recognized and modeled."
          }
        },
        {
          "section": "keywords",
          "title": "DISCOURSE MODEL LEVELS",
          "sentences": {
            "2": "In task-related discourse, much of the discourse structure derives directly from the task structure, so that a model of the agent&apos;s plans can serve as a useful discourse model, with discourse segment boundaries mapping to sub-plan boundaries and the like.",
            "3": "This simple model works well in applications like expert-apprentice dialogues, where a novice agent is currently pursuing a single domain plan.",
            "4": "Since the discourse tracks the domain plan so closely in such cases, it is fairly easy to make the links between the agent&apos;s queries and the relevant domain plans.",
            "5": "But this single-level model is not rich enough to handle phenomena like clarification subdialogues or plan revision, as seen in the work of Litman, Carberry, and others.",
            "6": "Litman&apos;s model [Lit85, LA87, LA90] employs a stack of discourse metaplans on top of the base domain plan, producing a two-level model that can handle clarification subdialogues and other discourse phenomena that go beyond the domain plans.",
            "7": "Carberry [Car90] adds an independent stack of discourse goals, for similar reasons.",
            "8": "In earlier work [R.am89a], I explored the use of a different kind of metaplan to model what I called the problem-solving level, where the agent is exploring possible plans, rather than pursuing an adopted one.",
            "9": "Such plan exploration contexts, which can include comparison between alternative plans or consideration of plans in hypothetical circumstances, are quite different from adopted domain plan execution contexts, both in terms of the reference patterns to domain plans and in terms of the kinds of utterances that are generated.",
            "10": "This paper describes an effort to combine these earlier approaches into a three-level model where the discourse metaplans can be rooted in either the exploration or domain plan levels, so that both kinds of plan-related behavior can be modeled.",
            "11": "Such a model can capture the differences between the plan exploration level and the domain level in terms of the appropriate plan recognition and query generation processes, thus broadening the range of discourse phenomena that can be modeled.",
            "12": "It also allows us to model shifts between levels, as the agent explores, adopts, and reconsiders particular plans."
          }
        },
        {
          "section": "introduction",
          "title": "THE NATUREOF PLAN EXPLORATION",
          "sentences": {
            "13": "Cohen and Levesque [CL90] have recently pointed out the theoretical problems that arise, while using a planning system to model a rational agent, from failing to distinguish between a system&apos;s plans and its intentions, since agents frequently form plans that they never adopt.",
            "14": "It is this same distinction that motivates the division proposed here of the domain-plan-related portion of the discourse model into separate levels representing first those domain plans and goals which the agent has adopted and is pursuing, here called the domain layer, and second those which the agent is exploring but has not adopted, the exploration layer.",
            "15": "While the same domain plans give structure to these two levels, the resulting discourse phenomena including the space of relevant queries based on those plans and the patterns of references to plans in the plan tree are quite different."
          }
        },
        {
          "section": "method",
          "title": "QUERY TYPES",
          "sentences": {
            "16": "One clear difference can be seen in the content of utterances arising on those different layers from the same underlying domain plan.",
            "17": "For example, in a banking context, consider these two queries: What is the interest rate on your passbook account?",
            "18": "To whom do I make out the check for the initial deposit on a passbook account?",
            "19": "The interest rate query is an example of a query based in an exploration context, since the interest rate on an account affects its desirability compared to other accounts but has no instrumental relevance to any of the plan steps involved in opening an account.",
            "20": "The query about the check payee, on the other hand, plays a local instrumental role in the open-account plan, but has no relevance outside of that particular subplan.",
            "21": "Some queries, of course, can arise in either kind of context, For example, What&apos;s the minimum initial deposit for a passbook account? could be either an exploration level query from an agent weighing the comparative advantages of a passbook account versus a CD, or it could be a domain level query from an agent who had already decided to open a passbook account, and who needed to know how large a check to write to open it.",
            "22": "Thus the context model for that query is ambiguous between the two interpretations.",
            "23": "There are also whole classes of queries that may be generated on the plan exploration level but that do not arise when agents are pursuing an adopted domain plan, as when an agent asks queries about the possible plans for a goal or about possible fillers for a variable within a plan.",
            "24": "For example, What does it take to open an account? asks about the subactions in a plan being explored, and asks for possible fillers of the account-class variable.",
            "25": "Such queries imply that the agent does not have any fully-instantiated adopted plan in mind."
          }
        },
        {
          "section": "method",
          "title": "DOMAIN PLAN REFERENCES",
          "sentences": {
            "26": "Another difference between the exploration and domain levels comes in the patterns of references to domain plans.",
            "27": "An agent pursuing an adopted domain plan has a single subplan in focus and typically shifts that focus in an orderly way related to the sequential steps in that plan.",
            "28": "On the other hand, at the exploration level, the possible patterns of movement are much less constrained, and conflicting alternative plans or multiple hypothetical plans may be explored simultaneously.",
            "29": "Exploration metaplans can capture these more complex patterns.",
            "30": "For example, agents frequently generate queries that compare particular features of alternative plans for the same goal.",
            "31": "For instance, after asking about the interest rate on passbook accounts, the agent might naturally ask about the rate for CD&apos;s.",
            "32": "This kind of comparison can be modeled by a compare-by-feature plan exploration metaplan, which represents the close discourse connection between the similar features of the two different plans.",
            "33": "Such feature by feature comparison would be hard to capture in a model based directly on the domain plans, since the focus would have to jump back and forth between the two alternatives.",
            "34": "At each step, such a model would seem to predict further queries about the current plan as more likely than a jump back to a query about the other plan, while at the exploration level, we can have plan comparison metaplans that capture either a plan by plan or a feature by feature approach.",
            "35": "A different kind of complex domain plan reference can occur in a hypothetical exploration query, where the agent explores plans in a context that includes projected states of affairs that are different from her own current world model.",
            "36": "Of course there is a sense in which every exploration level query is hypothetical, since it concerns the preconditions or effects of executing a plan to which the agent is not yet committed, but the issue here concerns hypothetical queries that assume more than the adoption of the single plan being explored.",
            "37": "While modeling arbitrary hypotheticals requires more than a planning system, there are cases where the hypothetical element in the agent&apos;s question can be expressed by assuming the adoption of some other plan in addition to the one currently explored.",
            "38": "For example, for a hypothetical query like If I put $1000 in a 1-year CD and withdrew it in a month, what would be the penalty? it seems that an exploration level metaplan could use the purchase-CD plan to create the hypothetical context in which the query about the withdrawal plan penalty is to be understood.",
            "39": "Thus the domain plan references in exploration utterances often do not correspond closely to the shape of the domain level plan tree.",
            "40": "Exploration metaplans can supply alternative structures that better capture the more complex reference patterns involved in examples like feature comparisons or hypothetical queries."
          }
        },
        {
          "section": "method",
          "title": "MOVES BETWEEN LEVELS",
          "sentences": {
            "41": "Finally, distinguishing between domain plan and exploration behavior is important so that the system can recognize when the agent moves from one level to the other.",
            "42": "If an agent has been asking evaluative queries and then proceeds to ask a pure domain level query about one of those plan options, the system should recognize that the agent has adopted that particular plan and is now actively pursuing it, rather than continuing to evaluate alternatives.",
            "43": "Such an adoption of a particular subplan establishes the expectation that the agent will continue to pursue it, perhaps asking further domain level queries, either until it is completed and focus moves on to a new plan or until a plan blockage or second thoughts on the agent&apos;s part trigger a reconsideration move back to the exploration level.",
            "44": "We can see the importance of this distinction in the care taken by human agents to make clear the level at which they are operating.",
            "45": "Agents use mood, cue phrases, and direct informing acts to keep their expert advisor informed as to whether they are speaking of adopted plans or of ones only being explored, so that the expert&apos;s plan tracking and responses can be fully cooperative."
          }
        },
        {
          "section": "method",
          "title": "STRUCTURE OF THE MODELTHE DOMAIN LEVEL",
          "sentences": {
            "46": "The base level in the model is always a domain plan structure representing the plans and goals that the agent is believed to have adopted and to be currently pursuing.",
            "47": "These plans are organized (adapting a technique from Kautz [Kau85]) into a classification hierarchy based on their effects, so that the subplan children of a class of plans represent alternative possible strategies for achieving the given effects.",
            "48": "The plans also include traditional action links outlining their action decomposition in terms of more primitive plans.",
            "49": "There are two classes of predicates in a plan definition: preconditions which must be true for the plan to be executed but which can be recursively planned for, and constraints (using Litman&apos;s word—Carberry calls them &quot;applicability conditions&quot;) which cannot be recursively planned for.",
            "50": "Each predicate is also classified as to its relevance, where internally relevant predicates are those whose bindings must be known in order to execute the plan and external predicates are those whose bindings are relevant when evaluating the plan from the outside.",
            "51": "Thus, using the earlier examples, the payee identity for write-check is only internally relevant, the interest rate for open-savings-account is only externally relevant, and the minimum initial deposit feature is relevant both internally and externally.",
            "52": "This heuristic classification of predicates is used to indicate which ones can be expanded at the domain vs. exploration levels."
          }
        },
        {
          "section": "method",
          "title": "THE EXPLORATION LEVEL",
          "sentences": {
            "53": "The basic exploration metaplan is explore-plan itself, which takes an instantiated domain plan as its single parameter.",
            "54": "The expected default exploration pattern simply follows the domain plan tree shape, exploring the subplans and actions beneath that plan, using the explore-subplan and explore-subaction metaplans.",
            "55": "This default behavior is compiled in by linking each exploration level node to the explore-plan nodes for the subplans and subactions of the domain plan it references.",
            "56": "Thus when the system models a move to the exploration level from a given domain plan node, the entire subtree of possible plans and actions beneath that node is also instantiated beneath the initial exploration level node.",
            "57": "The more complex exploration level strategies are encoded as metalevel subplans and subactions of explore-plan.",
            "58": "For example, compare-subplans is a subplan of explore-plan, and compare-by-feature is in turn one subplan of compare-subplans.",
            "59": "The system works from this library of exploration metaplans to create trees of possible contexts beneath each explore-plan node that model these alternative strategies of plan exploration."
          }
        },
        {
          "section": "method",
          "title": "THE DISCOURSE LEVEL",
          "sentences": {
            "60": "The metaplan structure directly underlying an utterance is always a discourse metaplan, though it may be as simple as an ask-value metaplan (like Litman&apos;s identify-parameter) directly based in the current domain plan context.",
            "61": "As in Litman&apos;s approach, phenomena like clarification subdialogues can be handled by further layers of discourse metaplans that introduce additional structure above the domain plan.",
            "62": "In this three-level model, these discourse layer metaplans can also be based on exploration level plans.",
            "63": "In testing for a match between a given query and a discourse context like ask-value, the discourse metaplans have access to the set of relevant predicates from the base context.",
            "64": "In determining that set, the system uses the appropriate relevance criteria depending on whether the base context is at the domain or exploration level.",
            "65": "There are also particular discourse plans, such as the ask-fillers plan, that require that their base context be at the exploration level."
          }
        },
        {
          "section": "method",
          "title": "OPERATION OF THE MODEL",
          "sentences": {
            "66": "For each utterance, the system begins from the previous context(s) and searches for a discourse node (based either on a domain or exploration node) that matches the utterance.",
            "67": "In the following example, an initial domain level context is assumed, with the default top-level goal being that deduced from the situation of the agent entering a bank and approaching the receptionist, namely, (conduct-banking-activity ?agent) as a subplan of (manage-money ?agent).",
            "68": "The matching context for the initial query, is seen in Figure 1, with asterisks used to mark the current focused path.",
            "69": "No match in the assumed context is found to this particular query using discourse metaplans based in domain plans, although one can imagine other contexts in which this query could be a step in pursuing an adopted plan, as in a journalist compiling a consumer&apos;s report on various banks.",
            "70": "But using the normal plans for banking customers, this query matches only on the exploration level, where the agent is exploring the plan of opening a savings account.",
            "71": "Note that an exploration level match could also be found by assuming that the move to the exploration level occurs at open-savings-account, suggesting that the agent has adopted not just the plan conduct-banking-activity, but also the more specific plan open-savings-account.",
            "72": "The system finds both matches in such cases, but heuristically prefers the one which makes the weakest assumptions about plans and goals adopted by the agent, thus preferring the model where the open-savingsaccount plan is only being explored.",
            "73": "Suppose the agent continues with the query This is matched by a discourse plan based on exploring one of the subplans of open-savingsaccount, which was the previous exploration level context, as seen in Figure 2 at #1.",
            "74": "The system also explores the possibility of matching to a discourse plan based in a domain level plan, which would imply the agent&apos;s adoption of the plan.",
            "75": "However, the interest rate feature has only external relevance, and thus cannot match queries on the domain level.",
            "76": "This query does finds a second match as the beginning of a compare-by-feature (at #2), but the heuristics prefer the match that is closer to the previous context, while discouraging the one-legged comparison.",
            "77": "The agent&apos;s next query, And the rate for the investment account?",
            "78": "*( can also be matched in two different ways, as seen in Figure 3.",
            "79": "One way (at #1) is based in an explore-plan for open-investment-account, suggesting that the agent has simply turned from exploring one plan to exploring an alternative one.",
            "80": "But this query also matches (at #2) as a second leg of the compare-by-feature subplan of exploreplan, where the query is part of the comparison between the two kinds of savings accounts based on the interest rate offered.",
            "81": "Since that serves as a close continuation of the feature comparison interpretation of the previous query, the latter interpretation is preferred.",
            "82": "The following two queries How big is the initial deposit for the passbook account? can be matched by a sibling compare-by-feature subtree, as seen in Figure 4.",
            "83": "This approach is thus able to represent the logical feature-by-feature structure of such a comparison, rather than having to bounce back and forth between explorations of the two subplan trees.",
            "84": "The next query, OK, who do I see to open a passbook account? makes a substantial change in the context, as shown in Figure 5.",
            "85": "Since the choice of the bank personnel for opening an account is an internal feature that can only be queried on the domain level, the only matches to this query are ones that imply that the agent has adopted the plan that she was previously exploring.",
            "86": "Modeling that adoption, the parallel path in the domain tree to the path that was being explored becomes the current domain context, and the matching discourse plan is based there.",
            "87": "The cue phrase &quot;OK&quot;, of course, is a further signal of this change in level, though not one the system can yet make use of.",
            "88": "In spite of that plan adoption, the agent can later reopen an exploration context concerning a subplan by saying, for example, She could also raise a query that implies a reconsideration of the previous plan adoption by saying I forgot to ask whether there are any maintenance charges on this account.",
            "89": "which would reestablish an exploration context of choosing between the passbook and investment accounts."
          }
        },
        {
          "section": "method",
          "title": "COMPARISON WITHEXISTING WORK",
          "sentences": {
            "90": "The general framework of using domain plans to model discourse structure is one that has been widely pursued and shown to be fruitful for various purposes [A1179, AP80, Car84, Car85, GS85, Sid85].",
            "91": "Important extensions have been made more recently in plan classification (Kau85) and in modeling plans in terms of beliefs, so as to be able to handle incorrect plans [Po186, Po190).",
            "92": "The most direct precursor of the model presented here is Litman and Allen&apos;s work [Lit85, LA87, LA901, which combines a domain plan model with discourse metaplans in a way that can model utterances arising from either the normal flow of domain plans, clarification subdialogues, or cases of domain plan modification.",
            "93": "Like exploration metaplans, their discourse plans can handle examples that do not mirror the execution structure of the domain plan.",
            "94": "Their system, however, makes the assumption that the agent is pursuing a single domain plan.",
            "95": "While the agent can modify a plan, there is no way to capture an agent&apos;s exploration of a number of different domain plan possibilities, the use of varying exploration strategies, or the differences between utterances that are based on exploration plans vs. those based on domain plans.",
            "96": "Carberry developed a model [Car90) that is similar to Litman&apos;s in combining domain plans with a discourse component, although this model&apos;s discourse plans operate on a separate stack rather than as a second layer of the domain plan model.",
            "97": "While the mechanisms of her model cover a wide variety of discourse goals, they make no distinction between domain and exploration plans.",
            "98": "They are thus also limited to following a single domain plan context at a time.",
            "99": "In earlier work [Ram89a], I presented a model that accounts for the plan refining and query aspects of plan exploration by using a tree of planbuilding metaplans, and much of that structure is incorporated in this model.",
            "100": "However, that version uses only a single layer of plan-building metaplans, so that it is strictly limited to plan exploration discourse.",
            "101": "It thus cannot model queries arising directly from the domain level, nor can it model the moves of plan adoption or reconsideration when the agent switches levels.",
            "102": "The plan-building trees in that earlier version are also limited to following the structure of the domain plans, and so are unable to represent comparison by features or other alternative exploration strategies, and that earlier model also lacks a separate discourse component.",
            "103": "Lambert and Carberry [Lam90, LC91] are currently working on a new, three-level approach that has much in common with the one presented here.",
            "104": "One interesting difference is that the three levels in their model form a hierarchy, with discourse plans always rooted in exploration plans.",
            "105": "While this may be appropriate for information-seeking discourse, allowing discourse plans to be rooted directly in domain plans can provide a natural way of representing utterances based directly on adopted plans.",
            "106": "Overall, their model makes significant contributions on the discourse level, allowing for the recognition of a wide range of discourse plans like expressing surprise or warning.",
            "107": "In contrast, the main focus in this work has been on the exploration level, modeling alternative exploration strategies, and plan adoption and reconsideration.",
            "108": "It would be fruitful to try to combine the two approaches."
          }
        },
        {
          "section": "method",
          "title": "IMPLEMENTATION ANDFUTURE WORK",
          "sentences": {
            "109": "The model presented here has been implemented in a system called Pragma (redone from the earlier Pragma system [Ram891))) which handles the examples covered in the paper.",
            "110": "Since the focus is on modeling plan exploration strategies, the initial context is directly input in the form of a domain plan with its parameter values, and the queries are input as meaning representations.",
            "111": "The output after each query is the updated set of context models.",
            "112": "The system has been exercised in the banking and course registration domains, though it is only populated with enough domain plans to serve as a testbed for the plan exploration strategies.",
            "113": "The exploration level is the most developed, including metaplans for constraining or instantiating plan variables and for exploring or comparing subplans using various strategies.",
            "114": "The discourse level currently includes only the metaplans ask-value, askplans, and ask-fillers.",
            "115": "Important next steps include expanding the collection of exploration level metaplans from the samples worked out so far to better characterize the full range of plan exploration strategies that people actually use, validating that collection against real data.",
            "116": "It would be particularly interesting to add coverage for the hypothetical queries discussed above, where the assumed event is another known domain plan.",
            "117": "The coverage of discourse level metaplans should be expanded, to better explore their interaction with exploration plans.",
            "118": "The system should also be made sensitive to other indicators for recognizing moves between the exploration and domain levels besides the class of predicate queried, including verb mood, cue phrases, and direct inform statements by the agent."
          }
        },
        {
          "section": "conclusions",
          "title": "CONCLUSIONS",
          "sentences": {
            "119": "This work suggests that plan exploration metaplans can be a useful and domain independent way of expanding the range of discourse phenomena that can be captured based on a model of the agent&apos;s domain plans.",
            "120": "While the more complex exploration strategies complicate the plan recognition task of connecting discourse phenomena with the underlying domain plans, exploration metaplans can successfully model those strategies and also allow us to recognize the moves of plan exploration, adoption, and reconsideration."
          }
        }
      ],
      "doc_id": 12,
      "actual_doc_id": "P91-1006-parscit-section.xml"
    },
    {
      "content": [
        {
          "section": "abstract",
          "title": "Abstract",
          "sentences": {
            "0": "We present a new approach for mapping natural language sentences to their formal meaning representations using stringkernel-based classifiers.",
            "1": "Our system learns these classifiers for every production in the formal language grammar.",
            "2": "Meaning representations for novel natural language sentences are obtained by finding the most probable semantic parse using these string classifiers.",
            "3": "Our experiments on two realworld data sets show that this approach compares favorably to other existing systems and is particularly robust to noise."
          }
        },
        {
          "section": "introduction",
          "title": "1 Introduction",
          "sentences": {
            "4": "Computational systems that learn to transform natural language sentences into formal meaning representations have important practical applications in enabling user-friendly natural language communication with computers.",
            "5": "However, most of the research in natural language processing (NLP) has been focused on lower-level tasks like syntactic parsing, word-sense disambiguation, information extraction etc.",
            "6": "In this paper, we have considered the important task of doing deep semantic parsing to map sentences into their computer-executable meaning representations.",
            "7": "Previous work on learning semantic parsers either employ rule-based algorithms (Tang and Mooney, 2001; Kate et al., 2005), or use statistical feature-based methods (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006).",
            "8": "In this paper, we present a novel kernel-based statistical method for learning semantic parsers.",
            "9": "Kernel methods (Cristianini and Shawe-Taylor, 2000) are particularly suitable for semantic parsing because it involves mapping phrases of natural language (NL) sentences to semantic concepts in a meaning representation language (MRL).",
            "10": "Given that natural languages are so flexible, there are various ways in which one can express the same semantic concept.",
            "11": "It is difficult for rule-based methods or even statistical featurebased methods to capture the full range of NL contexts which map to a semantic concept because they tend to enumerate these contexts.",
            "12": "In contrast, kernel methods allow a convenient mechanism to implicitly work with a potentially infinite number of features which can robustly capture these range of contexts even when the data is noisy.",
            "13": "Our system, KRISP (Kernel-based Robust Interpretation for Semantic Parsing), takes NL sentences paired with their formal meaning representations as training data.",
            "14": "The productions of the formal MRL grammar are treated like semantic concepts.",
            "15": "For each of these productions, a SupportVector Machine (SVM) (Cristianini and ShaweTaylor, 2000) classifier is trained using string similarity as the kernel (Lodhi et al., 2002).",
            "16": "Each classifier then estimates the probability of the production covering different substrings of the sentence.",
            "17": "This information is used to compositionally build a complete meaning representation (MR) of the sentence.",
            "18": "Some of the previous work on semantic parsing has focused on fairly simple domains, primarily, ATIS (Air Travel Information Service) (Price, 1990) whose semantic analysis is equivalent to filling a single semantic frame (Miller et al., 1996; Popescu et al., 2004).",
            "19": "In this paper, we have tested KRISP on two real-world domains in which meaning representations are more complex with richer predicates and nested structures.",
            "20": "Our experiments demonstrate that KRISP compares favor"
          }
        },
        {
          "section": "method",
          "title": "NEXT TO STATE2 Semantic Parsing",
          "sentences": {
            "21": "We call the process of mapping natural language (NL) utterances into their computer-executable meaning representations (MRs) as semantic parsing.",
            "22": "These MRs are expressed in formal languages which we call meaning representation languages (MRLs).",
            "23": "We assume that all MRLs have deterministic context free grammars, which is true for almost all computer languages.",
            "24": "This ensures that every MR will have a unique parse tree.",
            "25": "A learning system for semantic parsing is given a training corpus of NL sentences paired with their respective MRs from which it has to induce a semantic parser which can map novel NL sentences to their correct MRs.",
            "26": "Figure 1 shows an example of an NL sentence and its MR from the CLANG domain.",
            "27": "CLANG (Chen et al., 2003) is the standard formal coach language in which coaching advice is given to soccer agents which compete on a simulated soccer field in the RoboCup 1 Coach Competition.",
            "28": "In the MR of the example, bpos stands for “ball position”.",
            "29": "The second domain we have considered is the GEOQUERY domain (Zelle and Mooney, 1996) which is a query language for a small database of about 800 U.S. geographical facts.",
            "30": "Figure 2 shows an NL query and its MR form in a functional query language.",
            "31": "The parse of the functional query language is also shown along with the involved productions.",
            "32": "This example is also used later to illustrate how our system does semantic parsing.",
            "33": "The MR in the functional query language can be read as if processing a list which gets modified by various functions.",
            "34": "From the innermost expression going outwards it means: the state of Texas, the list containing all the states next to the state of Texas and the list of all the rivers which flow through these states.",
            "35": "This list is finally returned as the answer.",
            "36": "KRISP does semantic parsing using the notion of a semantic derivation of an NL sentence.",
            "37": "In the following subsections, we define the semantic derivation of an NL sentence and its probability.",
            "38": "The task of semantic parsing then is to find the most probable semantic derivation of an NL sentence.",
            "39": "In section 3, we describe how KRISP learns the string classifiers that are used to obtain the probabilities needed in finding the most probable semantic derivation.",
            "40": "We define a semantic derivation, D, of an NL sentence, s, as a parse tree of an MR (not necessarily the correct MR) such that each node of the parse tree also contains a substring of the sentence in addition to a production.",
            "41": "We denote nodes of the derivation tree by tuples (7r, [i..j]), where 7r is its production and [i..j] stands for the substring s[i..j] of s (i.e.",
            "42": "the substring from the ith word to the jth word), and we say that the node or its production covers the substring s[i..j].",
            "43": "The substrings covered by the children of a node are not allowed to overlap, and the substring covered by the parent must be the concatenation of the substrings covered by its children.",
            "44": "Figure 3 shows a semantic derivation of the NL sentence and the MR parse which were shown in figure 2.",
            "45": "The words are numbered according to their position in the sentence.",
            "46": "Instead of non-terminals, productions are shown in the nodes to emphasize the role of productions in semantic derivations.",
            "47": "Sometimes, the children of an MR parse tree node may not be in the same order as are the substrings of the sentence they should cover in a semantic derivation.",
            "48": "For example, if the sentence was “Through the states that border Texas which rivers run?”, which has the same MR as the sentence in figure 3, then the order of the children of the node “RIVER —* TRAVERSE(STATE)” would need to be reversed.",
            "49": "To accommodate this, a semantic derivation tree is allowed to contain MR parse tree nodes in which the children have been permuted.",
            "50": "Note that given a semantic derivation of an NL sentence, it is trivial to obtain the corresponding MR simply as the string generated by the parse.",
            "51": "Since children nodes may be permuted, this step also needs to permute them back to the way they should be according to the MRL productions.",
            "52": "If a semantic derivation gives the correct MR of the NL sentence, then we call it a correct semantic derivation otherwise it is an incorrect semantic derivation.",
            "53": "Let Pπ(u) denote the probability that a production 7r of the MRL grammar covers the NL substring u.",
            "54": "In other words, the NL substring u expresses the semantic concept of a production 7r with probability Pπ(u).",
            "55": "In the next subsection we will describe how KRISP obtains these probabilities using string-kernel based SVM classifiers.",
            "56": "Assuming these probabilities are independent of each other, the probability of a semantic derivation D of a sentence s is then: The task of the semantic parser is to find the most probable derivation of a sentence s.",
            "57": "This task can be recursively performed using the notion of a partial derivation En,s[i..j], which stands for a subtree of a semantic derivation tree with n as the left-hand-side (LHS) non-terminal of the root production and which covers s from index i to j.",
            "58": "For example, the subtree rooted at the node “(STATE —* NEXT TO(STATE),[5..9])” in the derivation shown in figure 3 is a partial derivation which would be denoted as ESTATE,s[5..9].",
            "59": "Note that the derivation D of sentence s is then simply Estart,s[1..|s|], where start is the start symbol of the MRL’s context free grammar, G. Our procedure to find the most probable parwhere partition(s[i..j], t) is a function which returns the set of all partitions of s[i..j] with t elements including their permutations.",
            "60": "A partition of a substring s[i..j] with t elements is a t−tuple containing t non-overlapping substrings of s[i..j] which give s[i..j] when concatenated.",
            "61": "For example, (“the states bordering”, “Texas ?”) is a partition of the substring “the states bordering Texas ?” with 2 elements.",
            "62": "The procedure makeTree(7r, (p1,..",
            "63": ", pt)) constructs a partial derivation tree by making 7r as its root production and making the most probable partial derivation trees found through the recursion as children subtrees which cover the substrings according to the partition (p1, ..",
            "64": ", pt).",
            "65": "The most probable partial derivation En,s[i..j] is found using the above equation by trying all productions 7r = n —* n1..nt in G which have tial deri E� vation n,s[i..j] considers all possible subtrees whose root production has n as its LHS nonterminal and which cover s from index i to j.",
            "66": "Mathematically, the most probable partial derivation E� n,s[i..j]is recursively defined as: n as the LHS, and all partitions with t elements of the substring s[i..j] (n1 to nt are right-handside (RHS) non-terminals of 7r, terminals do not play any role in this process and are not shown for simplicity).",
            "67": "The most probable partial derivation E�STATE,9[5_9] for the sentence shown in figure 3 will be found by trying all the productions in the grammar with STATE as the LHS, for example, one of them being “STATE —* NEXT TO STATE”.",
            "68": "Then for this sample production, all partitions, (p1, p2), of the substring s[5..9] with two elements will be considered, and the most probable derivations ENEXT TO,p, and ESTATE,p, will be found recursively.",
            "69": "The recursion reaches base cases when the productions which have n on the LHS do not have any non-terminal on the RHS or when the substring s[i..j] becomes smaller than the length t.",
            "70": "According to the equation, a production 7r E G and a partition (p1, ..",
            "71": ", pt) E partition(s[i..j], t) will be selected in constructing the most probable partial derivation.",
            "72": "These will be the ones which maximize the product of the probability of 7r covering the substring s[i..j] with the product of probabilities of all the recursively found most probable partial derivations consistent with the partition (p1, ..",
            "73": ",pt). A naive implementation of the above recursion is computationally expensive, but by suitably extending the well known Earley’s context-free parsing algorithm (Earley, 1970), it can be implemented efficiently.",
            "74": "The above task has some resemblance to probabilistic context-free grammar (PCFG) parsing for which efficient algorithms are available (Stolcke, 1995), but we note that our task of finding the most probable semantic derivation differs from PCFG parsing in two important ways.",
            "75": "First, the probability of a production is not independent of the sentence but depends on which substring of the sentence it covers, and second, the leaves of the tree are not individual terminals of the grammar but are substrings of words of the NL sentence.",
            "76": "The extensions needed for Earley’s algorithm are straightforward and are described in detail in (Kate, 2005) but due to space limitation we do not describe them here.",
            "77": "Our extended Earley’s algorithm does a beam search and attempts to find the w (a parameter) most probable semantic derivations of an NL sentence s using the probabilities P,(s[i..j]).",
            "78": "To make this search faster, it uses a threshold, 0, to prune low probability derivation trees ."
          }
        },
        {
          "section": "method",
          "title": "3 KRISP’s Training Algorithm",
          "sentences": {
            "79": "In this section, we describe how KRISP learns the classifiers which give the probabilities P,(u) needed for semantic parsing as described in the previous section.",
            "80": "Given the training corpus of NL sentences paired with their MRs {(si, mi)|i = 1..N}, KRISP first parses the MRs using the MRL grammar, G. We represent the parse of MR, mi, by parse(mi).",
            "81": "Figure 4 shows pseudo-code for KRISP’s training algorithm.",
            "82": "KRISP learns a semantic parser iteratively, each iteration improving upon the parser learned in the previous iteration.",
            "83": "In each iteration, for every production 7r of G, KRISP collects positive and negative example sets.",
            "84": "In the first iteration, the set P(7r) of positive examples for production 7r contains all sentences, si, such that parse(mi) uses the production 7r.",
            "85": "The set of negative examples, N(7r), for production 7r includes all of the remaining training sentences.",
            "86": "Using these positive and negative examples, an SVM classifier 2, C, is trained for each production 7r using a normalized string subsequence kernel.",
            "87": "Following the framework of Lodhi et al.",
            "88": "(2002), we define a kernel between two strings as the number of common subsequences they share.",
            "89": "One difference, however, is that their strings are over characters while our strings are over words.",
            "90": "The more the two strings share, the greater the similarity score will be.",
            "91": "Normally, SVM classifiers only predict the class of the test example but one can obtain class probability estimates by mapping the distance of the example from the SVM’s separating hyperplane to the range [0,1] using a learned sigmoid function (Platt, 1999).",
            "92": "The classifier C, then gives us the probabilities P,(u).",
            "93": "We represent the set of these classifiers by C = IC,|7r E G}.",
            "94": "Next, using these classifiers, the extended Earley’s algorithm, which we call EXTENDED EARLEY in the pseudo-code, is invoked to obtain the w best semantic derivations for each of the training sentences.",
            "95": "The procedure getMR returns the MR for a semantic derivation.",
            "96": "At this point, for many training sentences, the resulting most-probable semantic derivation may not give the correct MR. Hence, next, the system collects more refined positive and negative examples to improve the result in the next iteration.",
            "97": "It is also possible that for some sentences, none of the obtained w derivations give the correct MR. But as will be described shortly, the most probable derivation which gives the correct MR is needed to collect positive and negative examples for the next iteration.",
            "98": "Hence in these cases, a version of the extended Earley’s algorithm, EXTENDED EARLEY CORRECT, is invoked which also takes the correct MR as an argument and returns the best w derivations it finds, all of which give the correct MR. This is easily done by making sure all subtrees derived in the process are present in the parse of the correct MR. From these derivations, positive and negative examples are collected for the next iteration.",
            "99": "Positive examples are collected from the most probable derivation which gives the correct MR, figure 3 showed an example of a derivation which gives the correct MR. At each node in such a derivation, the substring covered is taken as a positive example for its production.",
            "100": "Negative examples are collected from those derivations whose probability is higher than the most probable correct derivation but which do not give the correct MR. Figure 5 shows an example of an incorrect derivation.",
            "101": "Here the function “next to” is missing from the MR it produces.",
            "102": "The following procedure is used to collect negative examples from incorrect derivations.",
            "103": "The incorrect derivation and the most probable correct derivation are traversed simultaneously starting from the root using breadth-first traversal.",
            "104": "The first nodes where their productions differ is detected, and all of the words covered by the these nodes (in both derivations) are marked.",
            "105": "In the correct and incorrect derivations shown in figures 3 and 5 respectively, the first nodes where the productions differ are “(STATE —* NEXT TO(STATE), [5..9])” and “(STATE -* STATEID, [8..9])”.",
            "106": "Hence, the union of words covered by them: 5 to 9 (“the states bordering Texas?”), will be marked.",
            "107": "For each of these marked words, the procedure considers all of the productions which cover it in the two derivations.",
            "108": "The nodes of the productions which cover a marked word in the incorrect derivation but not in the correct derivation are used to collect negative examples.",
            "109": "In the example, the node “(TRAVERSE-*traverse,[1..7])” will be used to collect a negative example (i.e.",
            "110": "the words 1 to 7 ‘‘which rivers run through the states bordering” will be a negative example for the production TRAVERSE—*traverse) because the production covers the marked words “the”, “states” and “bordering” in the incorrect derivation but not in the correct derivation.",
            "111": "With this as a negative example, hopefully in the next iteration, the probability of this derivation will decrease significantly and drop below the probability of the correct derivation.",
            "112": "In each iteration, the positive examples from the previous iteration are first removed so that new positive examples which lead to better correct derivations can take their place.",
            "113": "However, negative examples are accumulated across iterations for better accuracy because negative examples from each iteration only lead to incorrect derivations and it is always good to include them.",
            "114": "To further increase the number of negative examples, every positive example for a production is also included as a negative example for all the other productions having the same LHS.",
            "115": "After a specified number of MAX ITR iterations, the trained classifiers from the last iteration are returned.",
            "116": "Testing involves using these classifiers to generate the most probable derivation of a test sentence as described in the subsection 2.2, and returning its MR. The MRL grammar may contain productions corresponding to constants of the domain, for e.g., state names like “STATEID —* ‘texas’”, or river names like “RIVERID —* ‘colorado’” etc.",
            "117": "Our system allows the user to specify such productions as constant productions giving the NL substrings, called constant substrings, which directly relate to them.",
            "118": "For example, the user may give “Texas” as the constant substring for the production “STATEID —* ‘texas’.",
            "119": "Then KRISP does not learn classifiers for these constant productions and instead decides if they cover a substring of the sentence or not by matching it with the provided constant substrings."
          }
        },
        {
          "section": "evaluation",
          "title": "4 Experiments",
          "sentences": {
            "120": "KRISP was evaluated on CLANG and GEOQUERY domains as described in section 2.",
            "121": "The CLANG corpus was built by randomly selecting 300 pieces of coaching advice from the log files of the 2003 RoboCup Coach Competition.",
            "122": "These formal advice instructions were manually translated into English (Kate et al., 2005).",
            "123": "The GEOQUERY corpus contains 880 English queries collected from undergraduates and from real users of a web-based interface (Tang and Mooney, 2001).",
            "124": "These were manually translated into their MRs.",
            "125": "The average length of an NL sentence in the CLANG corpus is 22.52 words while in the GEOQUERY corpus it is 7.48 words, which indicates that CLANG is the harder corpus.",
            "126": "The average length of the MRs is 13.42 tokens in the CLANG corpus while it is 6.46 tokens in the GEOQUERY corpus.",
            "127": "KRISP was evaluated using standard 10-fold cross validation.",
            "128": "For every test sentence, only the best MR corresponding to the most probable semantic derivation is considered for evaluation, and its probability is taken as the system’s confidence in that MR. Since KRISP uses a threshold, 0, to prune low probability derivation trees, it sometimes may fail to return any MR for a test sentence.",
            "129": "We computed the number of test sentences for which KRISP produced MRs, and the number of these MRs that were correct.",
            "130": "For CLANG, an output MR is considered correct if and only if it exactly matches the correct MR. For GEOQUERY, an output MR is considered correct if and only if the resulting query retrieves the same answer as the correct MR when submitted to the database.",
            "131": "Performance was measured in terms of precision (the percentage of generated MRs that were correct) and recall (the percentage of all sentences for which correct MRs were obtained).",
            "132": "In our experiments, the threshold 0 was fixed to 0.05 and the beam size w was 20.",
            "133": "These parameters were found through pilot experiments.",
            "134": "The maximum number of iterations (MAX ITR) required was only 3, beyond this we found that the system only overfits the training corpus.",
            "135": "We compared our system’s performance with the following existing systems: the string and tree versions of SILT (Kate et al., 2005), a system that learns transformation rules relating NL phrases to MRL expressions; WASP (Wong and Mooney, 2006), a system that learns transformation rules using statistical machine translation techniques; SCISSOR (Ge and Mooney, 2005), a system that learns an integrated syntactic-semantic parser; and CHILL (Tang and Mooney, 2001) an ILP-based semantic parser.",
            "136": "We also compared with the CCG-based semantic parser by Zettlemoyer et al.",
            "137": "(2005), but their results are available only for the GEO880 corpus and their experimental set-up is also different from ours.",
            "138": "Like KRISP, WASP and SCISSOR also give confidences to the MRs they generate which are used to plot precision-recall curves by measuring precisions and recalls at various confidence levels.",
            "139": "The results of the other systems are shown as points on the precision-recall graph.",
            "140": "Figure 6 shows the results on the CLANG corpus.",
            "141": "KRISP performs better than either version of SILT and performs comparable to WASP.",
            "142": "Although SCISSOR gives less precision at lower recall values, it gives much higher maximum recall.",
            "143": "However, we note that SCISSOR requires more supervision for the training corpus in the form of semantically annotated syntactic parse trees for the training sentences.",
            "144": "CHILL could not be run beyond 160 training examples because its Prolog implementation runs out of memory.",
            "145": "For 160 training examples it gave 49.2% precision with 12.67% recall.",
            "146": "Figure 7 shows the results on the GEOQUERY corpus.",
            "147": "KRISP achieves higher precisions than WASP on this corpus.",
            "148": "Overall, the results show that KRISP performs better than deterministic rule-based semantic parsers like CHILL and SILT and performs comparable to other statistical semantic parsers like WASP and SCISSOR.",
            "149": "We have translations of a subset of the GEOQUERY corpus with 250 examples (GEO250 corpus) in three other natural languages: Spanish, Turkish and Japanese.",
            "150": "Since KRISP’s learning algorithm does not use any natural language specific knowledge, it is directly applicable to other natural languages.",
            "151": "Figure 8 shows that KRISP performs competently on other languages as well.",
            "152": "Any real world application in which semantic parsers would be used to interpret natural language of a user is likely to face noise in the input.",
            "153": "If the user is interacting through spontaneous speech and the input to the semantic parser is coming form the output of a speech recognition system then there are many ways in which noise could creep in the NL sentences: interjections (like um’s and ah’s), environment noise (like door slams, phone rings etc.), out-of-domain words, grammatically ill-formed utterances etc.",
            "154": "(Zue and Glass, 2000).",
            "155": "As opposed to the other systems, KRISP’s stringkernel-based semantic parsing does not use hardmatching rules and should be thus more flexible and robust to noise.",
            "156": "We tested this hypothesis by running experiments on data which was artificially corrupted with simulated speech recognition errors.",
            "157": "The interjections, environment noise etc. are likely to be recognized as real words by a speech recognizer.",
            "158": "To simulate this, after every word in a sentence, with some probability Padd, an extra word is added which is chosen with probability proportional to its word frequency found in the British National Corpus (BNC), a good representative sample of English.",
            "159": "A speech recognizer may sometimes completely fail to detect words, so with a probability of Pd,p a word is sometimes dropped.",
            "160": "A speech recognizer could also introduce noise by confusing a word with a high frequency phonetically close word.",
            "161": "We simulate this type of noise by substituting a word in the corpus by another word, w, with probability ped(,)*P(w), where p is a parameter, ed(w) is w’s edit distance (Levenshtein, 1966) from the original word and P(w) is w’s probability proportional to its word frequency.",
            "162": "The edit distance which calculates closeness between words is character-based rather than based on phonetics, but this should not make a significant difference in the experimental results.",
            "163": "Figure 9 shows the results on the CLANG corpus with increasing amounts of noise, from level 0 to level 4.",
            "164": "The noise level 0 corresponds to no noise.",
            "165": "The noise parameters, Padd and Pdrop, were varied uniformly from being 0 at level 0 and 0.1 at level 4, and the parameter p was varied uniformly from being 0 at level 0 and 0.01 at level 4.",
            "166": "We are showing the best F-measure (harmonic mean of precision and recall) for each system at different noise levels.",
            "167": "As can be seen, KRISP’s performance degrades gracefully in the presence of noise while other systems’ performance degrade much faster, thus verifying our hypothesis.",
            "168": "In this experiment, only the test sentences were corrupted, we get qualitatively similar results when both training and test sentences are corrupted.",
            "169": "The results are also similar on the GEOQUERY corpus."
          }
        },
        {
          "section": "conclusions",
          "title": "5 Conclusions",
          "sentences": {
            "170": "We presented a new kernel-based approach to learn semantic parsers.",
            "171": "SVM classifiers based on string subsequence kernels are trained for each of the productions in the meaning representation language.",
            "172": "These classifiers are then used to compositionally build complete meaning representations of natural language sentences.",
            "173": "We evaluated our system on two real-world corpora.",
            "174": "The results showed that our system compares favorably to other existing systems and is particularly robust to noise."
          }
        },
        {
          "section": "acknowledgments",
          "title": "Acknowledgments",
          "sentences": {
            "175": "This research was supported by Defense Advanced Research Projects Agency under grant HR0011-04-1-0007."
          }
        }
      ],
      "doc_id": 13,
      "actual_doc_id": "P06-1115-parscit-section.xml"
    },
    {
      "content": [
        {
          "section": "abstract",
          "title": "Abstract",
          "sentences": {
            "0": "We present a joint model for biomedical event extraction and apply it to four tracks of the BioNLP 2011 Shared Task.",
            "1": "Our model decomposes into three sub-models that concern (a) event triggers and outgoing arguments, (b) event triggers and incoming arguments and (c) protein-protein bindings.",
            "2": "For efficient decoding we employ dual decomposition.",
            "3": "Our results are very competitive: With minimal adaptation of our model we come in second for two of the tasks—right behind a version of the system presented here that includes predictions of the Stanford event extractor as features.",
            "4": "We also show that for the Infectious Diseases task using data from the Genia track is a very effective way to improve accuracy."
          }
        },
        {
          "section": "introduction",
          "title": "1 Introduction",
          "sentences": {
            "5": "This paper presents the UMass entry to the BioNLP 2011 shared task (Kim et al., 2011a).",
            "6": "We introduce a simple joint model for the extraction of biomedical events, and show competitive results for four tracks of the competition.",
            "7": "Our model subsumes three tractable sub-models, one for extracting event triggers and outgoing edges, one for event triggers and incoming edges and one for protein-protein bindings.",
            "8": "Fast and accurate joint inference is provided by combining optimizing methods for these three submodels via dual decomposition (Komodakis et al., 2007; Rush et al., 2010).",
            "9": "Notably, our model constitutes the first joint approach that explicitly predicts which protein should share the same binding event.",
            "10": "So far this has either been done through postprocessing heuristics (Bj6rne et al., 2009; Riedel et al., 2009; Poon and Vanderwende, 2010), or through a local classifier at the end of a pipeline (Miwa et al., 2010).",
            "11": "Our model is very competitive.",
            "12": "For Genia (GE) Task 1 (Kim et al., 2011b) we achieve the secondbest results.",
            "13": "In addition, the best-performing FAUST system (Riedel et al., 2011) is a variant of the model presented here.",
            "14": "Its advantage stems from the fact that it uses predictions of the Stanford system (McClosky et al., 2011a; McClosky et al., 2011b), and hence performs model combination.",
            "15": "The same holds for the Infectious Diseases (ID) track (Pyysalo et al., 2011), where we come in as second right behind the FAUST system.",
            "16": "For the Epigenetics and Posttranslational Modifications (EPI) track (Ohta et al., 2011) we achieve the 4th rank, partly because we did not aim to extract speculations, negations or cellular locations.",
            "17": "Finally, for Genia Task 2 we rank 3rd— with the 1st rank achieved by the FAUST system.",
            "18": "In the following we will briefly describe our model and inference algorithm, as far as this is possible in limited space.",
            "19": "Then we show our results on the three tasks and conclude.",
            "20": "Note we will assume familiarity with the task, and refer the reader to the shared task overview paper for more details."
          }
        },
        {
          "section": "method",
          "title": "2 Biomedical Event Extraction",
          "sentences": {
            "21": "Our goal is to extract biomedical events as shown in figure 1a).",
            "22": "To formulate the search for such structures as an optimization problem, we represent structures through a set of binary variables.",
            "23": "Our representation is inspired by previous work (Riedel et al., 2009; Bj6rne et al., 2009) and based on a projection of events to a labelled graph over tokens in the"
          }
        },
        {
          "section": "method",
          "title": "3 Model",
          "sentences": {
            "24": "We use the following objective to score the structures we like to extract: sentence, as seen figure 1b).",
            "25": "We will first present some basic notation to simplify our exposition.",
            "26": "For each sentence x we have a set candidate trigger words Trig (x), and a set of candidate proteins Prot (x).",
            "27": "We will generally use the indices i and l to denote members of Trig (x), the indices p, q for members of Prot (x) and the index j for members of Cand (x) def = Trig (x) U Prot (x).",
            "28": "We label each candidate trigger i with an event Type t E T (with None E T), and use the binary variable ei,t to indicate this labeling.",
            "29": "We use binary variables ai,l,r to indicate that between i and l there is an edge labelled r E R (with None E R).",
            "30": "The representation so far has been used in previous work (Riedel et al., 2009; Björne et al., 2009).",
            "31": "Its shortcoming is that it does not capture whether two proteins are arguments of the same binding event, or arguments of two binding events with the same trigger.",
            "32": "To overcome this problem, we introduce binary “same Binding” variables bp,Q that are active whenever there is a binding event that has both p and q as arguments.",
            "33": "Our inference algorithm will also need, for each trigger i and protein pair p, q, a binary variable ti,p,Q that indicates that at i there is a binding event with arguments p and q.",
            "34": "All ti,p,Q are summarized in t.",
            "35": "Constructing events from solutions (e, a, b) can be done almost exactly as described by Björne et al.",
            "36": "(2009). However, while Björne et al.",
            "37": "(2009) group arguments according to ad-hoc rules based on dependency paths from trigger to argument, we simply query the variables bp,Q.",
            "38": "with local scoring functions sT (i, t) def = (wT, fT (i, t)), sR (i, j, r)def = (wR, fR (i, j, r)) and sB (p, q) def= (wB, fB (p, q)).",
            "39": "Our model scores all parts of the structure in isolation.",
            "40": "It is a joint model due to the three types of constraints we enforce.",
            "41": "The first type acts on trigger labels and their outgoing edges.",
            "42": "It includes constraints such as “an active label at trigger i requires at least one active outgoing Theme argument”.",
            "43": "The second type enforces consistency between trigger labels and their incoming edges.",
            "44": "That is, if an incoming edge has a label that is not None, the trigger must not be labelled None either.",
            "45": "The third type of constraints ensures that when two proteins p and q are part of the same binding (as indicated by bp,Q = 1), there needs to be a binding event at some trigger i that has p and q as arguments.",
            "46": "We will denote the set of structures (e, a, b) that satisfy all above constraints as Y. To learn w we choose the passive-aggressive online learning algorithm (Crammer and Singer, 2003).",
            "47": "As loss function we apply a weighted sum of false positives and false negative labels and edges.",
            "48": "The weighting scheme penalizes false negatives 3.8 times more than false positives.",
            "49": "For feature vector fT (i, t) we use a collection of representations for the token i: word-form, lemma, POS tag, syntactic heads, syntactic children; membership in two dictionaries used by Riedel et al.",
            "50": "(2009).For fR (a; i, j, r) we use representations of the token pair (i, j) inspired by Miwa et al.",
            "51": "(2010). They contain: labelled and unlabeled n-gram dependency paths; edge and vertex walk features (Miwa et al., 2010), argument and trigger modifiers and heads, words in between (for close distance i and j).",
            "52": "For fB (b; p, q) we use a small subset of the token pair representations in fR.",
            "53": "Algorithm 1 Dual Decomposition.",
            "54": "Inference in our model amounts to solving arg max s(e,a,b).",
            "55": "(1) (e,a,b)EY Our approach to finding the maximizer is dual decomposition (Komodakis et al., 2007; Rush et al., 2010), a technique that allows us to exploit efficient search algorithms for tractable substructures of our problem.",
            "56": "We divide the problem into three sub-problems: (1) finding the highest-scoring trigger labels and edges (e, a) such that constraints on triggers and their outgoing edges are fulfilled; (2) finding the highest-scoring trigger labels and edges (e, a) such that constraints on triggers and their incoming edges are fulfilled; (3) finding the highestscoring pairs of proteins b to appear in the same binding, and make binding event trigger decisions t for these.",
            "57": "Due to space constraints we only state that the first two problems can be solved exactly in O (n2 + nm) time while the last needs O (m2n).",
            "58": "Here n is the number of trigger candidates and m the number of proteins.",
            "59": "The subroutines to solve these three sub-problems are combined in algorithm 1—an instantiation of subgradient descent on the dual of an LP relaxation of problem 1.",
            "60": "In the first three steps in the main loop of this algorithm, the individual sub-problems are solved.",
            "61": "Note that to each subroutine a parameter is passed.",
            "62": "For example, when finding the structure (e, a) that maximizes the objective under the incoming edge constraints, we pass the parameter −λ.",
            "63": "This parameter represents a set of penalties to be added to the objective used for the subproblem.",
            "64": "In this case we have penalties −Ai,e to be added to the scores of trigger-label pairs (i, e), and penalties −Ai,j,r to be added for labelled edges i r→ j.",
            "65": "One way to understand dual decomposition is as iterative tuning of the penalties such that eventually all individual solutions are consistent with each other.",
            "66": "In our case this would mean, among other things, that the solutions (e, a) and (e, a) are identical.",
            "67": "This tuning happens in the second part of the main loop which updates the dual variables λ and µ.",
            "68": "We see, for example, how the penalties Ai,e are decreased by ei,e − ei,e scaled by a step-size at.",
            "69": "Effectively this change to Ai,e will decrease the score of ei,e within bestIn (−λ) by at if ei,e was true while ei,e was false in the current solutions.1 If ei,e was false but ei,e was true, the score is increased by at.",
            "70": "If both agree, no change is needed.",
            "71": "Consistency between solutions also means that the binding decisions in b and t are consistent with the rest of the solution.",
            "72": "This is achieved in algorithm 1 through tuning of the dual variables µ but we omit details for brevity.",
            "73": "For completeness we state how the penalties used for solving the other subproblems are set based on the dual variables λ and µ.",
            "74": "We set cout i,p,q; for the case that j ∈ Prot (x) we get cout i,j,r (λ, µ) def Ai,j,r + Pp µazgl + Pq µazq, otherwise cout i,j,r (λ, µ) def � Ai,j,r . For bestBind (c) we set cq_ —,,2P µiq µl,qµi,Pq.",
            "75": "After basic tokenization and sentence segmentation, we generate a set of protein head tokens Prot (x) for each sentence x based on protein span definitions from the shared task.",
            "76": "To ensure tokens contain not more than one protein we split them at protein boundaries.",
            "77": "Parsing is performed using the Charniak-Johnson parser (Charniak and Johnson, 2005) with the self-trained biomedical parsing model of McClosky and Charniak (2008).",
            "78": "Finally, based on the set of trigger words in the training data, we generate a set of candidate triggers Trig (4"
          }
        },
        {
          "section": "evaluation",
          "title": "4 Results",
          "sentences": {
            "79": "We apply the same model to the GE, ID and EPI tracks, with minor modifications in order to deal with the different event type sets T and role sets R of each track.",
            "80": "Training and testing together took between 30 (EPI) to 120 (GE) minutes using a singlecore implementation.",
            "81": "Our results for GE task 1 and 2 can be seen in table 1.",
            "82": "We also show results for abstracts only (abst.), and for full text only (full).",
            "83": "Note that binding events (BIND) and general regulation events (REG) seem to be harder to extract in full text.",
            "84": "Somewhat surprisingly, for simple events (SVT) the opposite holds.",
            "85": "We also like to point out that for full text extraction we rank first—the second best FAUST system achieves an F1 score of 52.67.",
            "86": "The Infectious Diseases track differs from the Genia track in two important ways.",
            "87": "First, it introduces the event type Process that is allowed to have no arguments at all.",
            "88": "Second, it comes with significantly less training data (152 vs 908 documents).",
            "89": "We can accommodate the first difference by making simple changes in our inference algorithms.",
            "90": "For example, for Process events we do not force the algorithm to pick a Theme argument.",
            "91": "To compensate for the lack of training data we simply add data from the GE track.",
            "92": "This is reasonable because annotations overlap quite significantly.",
            "93": "In table 2 we show the impact of mixing different amounts of ID data (I) and GE data (G) into the training set.",
            "94": "We point out that adding the ID training set twice, and the GENIA set once, leads to the best performance (I/G=2/1).",
            "95": "Remarkably, the F1 score for Process increases by including data, although this data does not include any such events.",
            "96": "This may stem from a shared model of None arguments that is improved with more data.",
            "97": "For this track a different set of events is to be predicted.",
            "98": "However, it is straightforward to adapt our model and algorithms to this setting.",
            "99": "For brevity we only report our total results here and omit a table with details.",
            "100": "The first metric (ALL) includes negation, speculation and cellular location targets.",
            "101": "We omitted these in our model and hence our result of 33.52 F1 is relatively weak.",
            "102": "For the metric that neglects these aspects (CORE), we achieve 64.15 F1 and come in 4th.",
            "103": "Note that in this metric the FAUST system, based on the model presented here, comes in as very close second."
          }
        },
        {
          "section": "conclusions",
          "title": "5 Conclusion",
          "sentences": {
            "104": "We have presented a robust joint model for event extraction from biomedical text that performs well across all tasks.",
            "105": "Remarkably, no feature set or parameter tuning was necessary to achieve this.",
            "106": "We also show substantial improvements for the ID task by adding GENIA data into the training set."
          }
        },
        {
          "section": "acknowledgments",
          "title": "Acknowledgements",
          "sentences": {
            "107": "This work was supported in part by the Center for Intelligent Information Retrieval.",
            "108": "The University of Massachusetts gratefully acknowledges the support of Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no.",
            "109": "FA8750-09-C-0181. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the US government."
          }
        }
      ],
      "doc_id": 14,
      "actual_doc_id": "W11-1807-parscit-section.xml"
    },
    {
      "content": [
        {
          "section": "abstract",
          "title": "Abstract",
          "sentences": {
            "0": "The CIAOSENSO WSD system is based on Conceptual Density, WordNet Domains and frequences of WordNet senses.",
            "1": "This paper describes the upvunige-CIAOSENSO WSD system, we participated in the english all-word task with, and its versions used for the english lexical sample and the WordNet gloss disambiguation tasks.",
            "2": "In the last an additional goal was to check if the disambiguation of glosses, that has been performed during our tests on the SemCor corpus, was done properly or not."
          }
        },
        {
          "section": "introduction",
          "title": "Introduction",
          "sentences": {
            "3": "The CIAOSENSO WSD system is an unsupervised system based on Conceptual Density (Agirre and Rigau, 1995), frequencies of WordNet senses, and WordNet Domains (Magnini and Cavagli`a, 2000).",
            "4": "Conceptual Density (CD) is a measure of the correlation among the sense of a given word and its context.",
            "5": "The foundation of this measure is the Conceptual Distance, defined as the length of the shortest path which connects two concepts in a hierarchical semantic net.",
            "6": "The starting point for our work was the CD formula of Agirre and Rigau (Agirre and Rigau, 1995), which compares areas of subhierarchies.",
            "7": "The noun sense disambiguation in the CIAOSENSO WSD system is performed by means of a formula combining Conceptual Density with WordNet sense frequency (Rosso et al., 2003).",
            "8": "WordNet Domains is an extension of WordNet 1.6, developed at ITC-irstl, where each synset has been annotated with at least one domain label, selected from a set of about two hundred labels hierarchically organized (Magnini and Cavagli`a, 2000).",
            "9": "Since the lexical resource used by the upvunige-CIAOSENSO WSD system is WordNet 2.0 (WN2.0), it has been necessary to map the synsets of WordNet Domains from version 1.6 to the version 2.0.",
            "10": "This has been done in a fully automated way, by using the WordNet mappings for nouns and 'Istituto per la Ricerca Scientifica e Tecnologica, Trento, Italy verbs, and by checking the similarity of synset terms and glosses for adjectives and adverbs.",
            "11": "Some domains have also been assigned by hand in some cases, when necessary."
          }
        },
        {
          "section": "method",
          "title": "1 Noun Sense Disambiguation",
          "sentences": {
            "12": "In our upv-unige-CIAOSENSO WSD system the noun sense disambiguation is carried out by means of the formula presented in (Rosso et al., 2003), which gave good results for the disambiguation of nouns over the SemCor corpus (precision 0.815).",
            "13": "This formula has been derived from the original Conceptual Density formula described in (Agirre and Rigau, 1995): where is the synset at the top of subhierarchy, the number of word senses falling within a subhierarchy, the height of the subhierarchy, and the averaged number of hyponyms for each node (synset) in the subhierarchy.",
            "14": "The numerator expresses the expected area for a subhierarchy containing marks (word senses), while the divisor is the actual area.",
            "15": "Due to the fact that the averaged number of hyponyms for each node in WN2.0 is greater than in WN1.4 (the version which was used originally by Agirre and Rigau), we decided to consider only the relevant part of the subhierarchy determined by the synset paths (from to an ending node) of the senses of both the word to be disambiguated and its context.",
            "16": "The base formula is based on the number of relevant synsets, corresponding to the marks in Formula 1 ( =, but we determine the subhierarchies before adding such marks instead of vice versa like in (Agirre and Rigau, 1995)), divided by the total number of synsets of the subhierarchy.",
            "17": "The original formula and the above one do not take into account sense frecuency.",
            "18": "It is possible that both formulas select subhierarchies with a low frequency related sense.",
            "19": "In some cases this would be a wrong election.",
            "20": "This pushed us to modify the CD formula by including also the information about frequency that comes from WN: where is the number of relevant synsets, is a constant (the best results were obtained over the SemCor corpus with near to 0.10), and is an integer representing the frequency of the subhierarchy-related sense in WN (1 means the most frequent, 2 the second most frequent, etc.).",
            "21": "This means that the first sense of the word (i.e.",
            "22": ", the most frequent) gets at least a density of 1 and one of the less frequent senses will be chosen only if it will exceed the density of the first sense.",
            "23": "The factor was introduced to give more weigth to the subhierarchies with a greater number of relevant synsets, when the same density is obtained among many subhierarchies.",
            "24": "of brake with the context words horn, man, second. Example extracted from the Senseval-3 english-all-words test corpus.",
            "25": "( In figure 1 are shown the resulting WordNet subhierarchies from the disambiguation of brake with the context words horn, man, second from the sentence: “Brakes howled and a horn blared furiously, but the man would have been hit if Phil hadn’t called out to him a second before”, extracted from the all-words test corpus.",
            "26": "The areas of subhierarchies are drawn with a dashed background, the root of subhierarchies are the darker nodes, while the nodes corresponding to the synsets of the word to disambiguate and those of the context words are drawn with a thicker border.",
            "27": "Four subhierarchies have been identified, one for each sense of brake.",
            "28": "The senses of the context words falling out of these subhierarchies are not taken into account.",
            "29": "The resulting CDs are, for each subhierarchy, respectively:,,and, therefore the first one is selected and sense 1 is assigned to brake.",
            "30": "In the upv-unige-CIAOSENSO WSD system, additional weights (Mutual Domain Weights, MDWs) are added to the densities of the subhierarchies corresponding to those senses having the same domain of context nouns’ senses.",
            "31": "Each weight is proportional to the frequency of such senses, and is calculated as, where is an integer representing the frequency of the sense of the word to be disambiguated and gives the same information for the context word.",
            "32": "E.g. if the word to be disambiguated is doctor, the domains for senses 1 and 4 are, respectively, Medicine and School.",
            "33": "Therefore, if one of the context words is university, having the third sense labeled with the domain School, the resulting weight for doctor(4) and university(3) is Those weights are not considered in the upvunige-CIAOSENSO2 system, which has been used only for the all-words task.",
            "34": "We included some adjustment factors based on context hyponyms, in order to assign an higher conceptual density to the related subhierarchy in which a context noun is an hyponym of a sense of the noun to be disambiguated (the hyponymy relation reflects a certain correlation between the two lexemes).",
            "35": "We refer to this technique as to the Specific Context Correction (SCC).",
            "36": "The idea is to select as the winning subhierarchy the one where one or more senses of the context nouns fall beneath the synset of the noun to be disambiguated.",
            "37": "An idea connected to the previous one is to give more weight to those subhierarchies placed in deeper positions.",
            "38": "We named this technique as Cluster Depth Correction (CDC) (we use improperly the word “cluster” here to refere to the relevant part of a subhierarchy).",
            "39": "When a subhierarchy is below a certain averaged depth (which was determined in an empirical way to be approximately 4) and, therefore, its sense of the noun to be disambiguated is more specific, the conceptual density of Formula 3 is augmented proportionally to the number of the contained relevant synsets:,,,, where and indicates, respectively, the and values for the-th sense) . where returns the depth of the current subhierarchy ( ) with respect to the top of the WordNet hierarchy; is the averaged depth of all subhierarchies in SemCor; its value, as said before, was empirically determined to be equal to 4; and is a constant (the best results were obtained, over SemCor, with 0.70).",
            "40": "These depth corrections have been used only in the upv-unige-CIAOSENSO-eaw and upv-unigeCIAOSENSO-ls systems for the english all-words task and english lexical sample tasks.",
            "41": "We found that they are more useful when a large context is available, and this is not the case of the gloss disambiguation task, where the context is very small.",
            "42": "Moreover, in the upv-unige-CIAOSENSO2 system we aimed to achieve the best precision, and these corrections usually allow to improve recall but not precision ."
          }
        },
        {
          "section": "method",
          "title": "2 Adjectives, Verbs and Adverbs SenseDisambiguation",
          "sentences": {
            "43": "The disambiguation of words of POS categories other than noun does not take into account the Conceptual Density.",
            "44": "This has been done for the following reasons: first of all, it could not be used for adjectives and adverbs, since in WordNet there is not a hierarchy for those POS categories.",
            "45": "With regard to verbs, the hierarchy is too shallow to be used efficiently.",
            "46": "Moreover, our system performs the disambiguation one sentence at a time, and this results in having in most cases only one verb for each sentence (with the consequence that no density can be computed).",
            "47": "The sense disambiguation of an adjective is performed only on the basis of the domain weights and the context, constituted by the Closest Noun (CN), i.e., the noun the adjective is referring to (e.g.",
            "48": "in “family of musical instruments” the CN of musical is instruments).",
            "49": "Given one of its senses, we extract the synsets obtained by the antonymy, similar to, pertainymy and attribute relationships.",
            "50": "For each of them, we calculate the MDW with respect to the senses of the context noun.",
            "51": "The weight assigned to the adjective sense is the average between these MDWs.",
            "52": "The selected sense is the one having the maximum average weight.",
            "53": "In order to achieve the maximum coverage, the Factotum domain has been also taken into account to calculate the MDWs between adjective senses and context noun senses.",
            "54": "However, due to the fact that in many cases this domain does not provide a useful information, the weights resulting from a Factotum domain are reduced by a factor.",
            "55": "E.g. suppose to disambiguate the adjective academic referring to the noun credit.",
            "56": "Both academic(1) and credit(6) belong to the domain School.",
            "57": "Furthermore, the Factotum domain contains the senses 1 4 and 7 of credit, and senses 2 and 3 of academic.",
            "58": "The extra synsets obtained by means of the WN relationships are: academia(1):Sociology, pertainym of sense 1; theoretical(3):Factotum and applied(2):Factotum, similar and antonym of sense 2; scholarly(1):Factotum and unscholarly(1):Factotum, similar and antonym of sense 3.",
            "59": "Since there are no senses of credit in the Sociology domain, academia(1) is not taken into account.",
            "60": "Therefore, the resulting weights for academic are: for sense 1; for sense 2; for sense 3.",
            "61": "The weights resulting from the extra synsets are represented within square brackets.",
            "62": "Since the maximum weight is obtained for the first sense, this is the sense assigned to academic.",
            "63": "The sense disambiguation of a verb is done nearly in the same way, but taking into consideration only the MDWs with the verb’s senses and the context words (i.e.",
            "64": ", in the previous example, if we had to disambiguate a verb instead of an adjective, the weights within the square brackets would not have been considered).",
            "65": "In the all-words and the gloss disambiguation tasks the two context words are the noun before and after the verb, whereas in the lexical sample task the context words are four (two before and two after the verb), without regard to their morphological category.",
            "66": "This has been done in order to improve the recall in the latter task, whose test corpus is made up mostly by verbs, since our experiments carried out over the SemCor corpus showed that considering only the noun preceding and following the verb allows for achieving a better precision, while the recall is higher when the 4-word context is used.",
            "67": "The sense disambiguation of adverbs (in every task) is carried out in the same way of the disambiguation of verbs for the lexical sample task.",
            "68": "We are still working on the disambiguation of adverbs, however, by the time we participated in SENSEVAL-3, this was the method providing the best results."
          }
        },
        {
          "section": "method",
          "title": "3 The English All-Words Task",
          "sentences": {
            "69": "We participated in this task with two systems: the upv-unige-CIAOSENSO-eaw system and the upvunige-CIAOSENSO2-eaw system.",
            "70": "The difference between these systems is that in the latter the disambiguation of nouns is carried out considering only the densities of the subhierarchies obtained with the formula (3), while the first one considers the WordNet Domains weights, too.",
            "71": "The nouns have been disambiguated in both systems with a context window of four nouns.",
            "72": "The disambiguation of verbs, as said above, has been carried out considering the noun preceding and following the verb.",
            "73": "Adverbs have been disambiguated with a context window of four words, while adjectives have been disambiguated with the Closest Noun, as described in the previous section.",
            "74": "The text, for every task we participated in, has been previously POS-tagged with the POS-tagger described in (Pla and Molina, 2001).",
            "75": "In the tables below we show the results achieved by the upvunige-CIAOSENSO and upv-unige-CIAOSENSO2 systems in the SENSEVAL-3.",
            "76": "The table 1 shows the “without U” scores, which consider the missing answers as undisambiguated words and not errors (that is, how our system is intended to work).",
            "77": "The baseline MFU, calculated by assigning to the word its most frequent (according to WordNet) sense, is for both precision and recall, having a % coverage.",
            "78": "The results are roughly comparable with those obtained in our previous work over the SemCor.",
            "79": "Considering only the polysemous words in SemCor, our tests gave a precision of and a recall of, with a coverage of 83.55% (if monosemous words were included, the values for precision and recall would be, respectively, 0.692 and 0.602, with a coverage of 87.07%).",
            "80": "In order to have a better understanding of the results, in the following two tables we show the precision and recall results for each morphological category, highlighting those on nouns, being the only category for which the two systems give different answers.",
            "81": "The behaviour of our systems is the same as we observed on the SemCor: the system relying only on Conceptual Density and frequency is more precise, even more than the most-frequent heuristic (over nouns in SemCor the precision obtained by the CIAOSENSO and CIAOSENSO2 systems was, respectively, 0.737 and 0.815, with a MFU baseline of.755).",
            "82": "Whereas the precision needs to be improved over verbs, it overtakes the baseline for nouns and adjectives ."
          }
        },
        {
          "section": "method",
          "title": "4 The English Lexical Sample Task",
          "sentences": {
            "83": "The system participating in this task works in an almost identical manner of the upv-unigeCLAOSENSO-eaw, with the difference that verbs are disambiguated in the same way of adverbs (context of four words, the two preceding and the two following the verb).",
            "84": "The biggest difference with the all-words task is that the training corpus has been used to change the ranking of WordNet senses for the headwords, therefore, it should be more appropriate to consider this version of the upv-unigeCIAOSENSO as an hybrid system.",
            "85": "E.g. in the training corpus the verb mean, having seven senses in WordNet, appears 40 times with the WordNet sixth sense, 23 times with the WN second sense, and eight times with the WN seventh sense; therefore, the ranking of its senses has been changed to the following: 6 2 7 1 3 4 5.",
            "86": "In table 5 we show the POS-specific results from the total ones, in order to highlight the superior performance over nouns."
          }
        },
        {
          "section": "method",
          "title": "5 The WSD of WordNet Glosses Task",
          "sentences": {
            "87": "The upv-unige-CL4OSENSO-gl system is an optimized version for this task, of the upv-unigeCL4OSENSO2-eaw which participated in the allwords task.",
            "88": "The optimization has been done on the basis of the work we carried out over WordNet glosses during the testing of the disambiguation of adjectives over the SemCor corpus.",
            "89": "During that work, we tried to extract from adjective glosses the nouns to be used to calculate additional MDWs, and we obtained a precision of 61.11% for the adjectives in the whole SemCor using the disambiguated glosses, against a 57.10% of precision with the undisambiguated glosses.",
            "90": "This improvement led us to further investigate the structure of wordnet glosses, investigation that took us to apply the following “corrections” to the original system for the SENSEVAL-3 gloss disambiguation task.",
            "91": "First of all, it has been noted that noun glosses often contains references to the direct hypernym and/or the direct hyponyms (e.g.",
            "92": "command(]) in the gloss of behest:“an authoritative command or request”), and its meronyms and holonyms too (e.g.",
            "93": "jaw(3) in the gloss of chuck(3): “a holding device consisting of adjustable jaws...”).",
            "94": "Therefore, we added a weight of for the noun senses being direct hypernyms, or direct hyponyms, of the synset to which belongs the gloss (head synset), and for the senses being meronyms or holonyms of the head synset.",
            "95": "Then, it has been noted that verb glosses often contains references to the direct hypernym (e.g.",
            "96": "walk(]) in the gloss offlounce:“walk emphatically”), thus a weight of is added for the verb senses being direct hypernym of the head verb synset.",
            "97": "A weight is also added when an attribute or pertainymy relationship with the head synset is found.",
            "98": "Finally, we used WordNet Domains to assign extra weights to the senses having the same domain of the head synset (e.g.",
            "99": "heart(2) in the gloss of blood(]):“the fluid that is pumped by the heart”).",
            "100": "The assigned weight is if the domain is different than Factotum, otherwise.",
            "101": "E.g. blood(]) belongs to the domain Medicine; of the ten senses of heart in WordNet, only the second is in the domain Medicine, therefore the second sense of heart gets a weight of (we gave intentionally an higher weight than the other relationships because it seemed to us more meaningful than the other ones).",
            "102": "Although we participated in this task only with the optimized version, we tried to use the standard system for the same task in order to see the difference between them.",
            "103": "The results show that the optimized version performs much better for the gloss disambiguation task than the standard one:"
          }
        },
        {
          "section": "conclusions",
          "title": "6 Conclusions and Further Work",
          "sentences": {
            "104": "The results we obtained in the three tasks of the SENSEVAL-3 we participated in are roughly comparable with those attained in our previous work over the SemCor.",
            "105": "In other words, it seems that our system better disambiguate nouns in comparison to words of the others morphological categories.",
            "106": "A further research direction we plan to investigate is the role of WordNet glosses in the disambiguation, by using the Web as resource to retrieve additional sample sentences, in order to integrate a “leskian” approach within our system.",
            "107": "We aim to enhance the performance over verbs, that is the morphological category in which we are facing most difficulty.",
            "108": "We also took part in the english all-words and english lexical sample tasks in the integrated R2D2Team system, together with other (un)supervised methods based on Maximum Entropy and Hidden Markov Models, obtaining the following results: The integration has been made by means of a voting technique.",
            "109": "We plan to improve the integration by assigning a certain weight to each system."
          }
        },
        {
          "section": "acknowledgments",
          "title": "Acknowledgements",
          "sentences": {
            "110": "This work was supported by the CIAO SENSO MCYT Spain-Italy project (HI 2002-0140) and by the R2D2 CICYT project (TIC2003-07158-C0403).",
            "111": "We are grateful to A. Molina and F. Pla for making the POS-tagger available."
          }
        }
      ],
      "doc_id": 15,
      "actual_doc_id": "W04-0820-parscit-section.xml"
    },
    {
      "content": [
        {
          "section": "abstract",
          "title": "Abstract",
          "sentences": {
            "0": "This paper describes the implementation of a prototype of a grammar based grammar checker for Czech and the basic ideas behind this implementation.",
            "1": "The demo is implemented as an independent program cooperating with Microsoft Word.",
            "2": "The grammar checker uses specialized grammar formalism which generally enables to check errors in languages with a very high degree of word order freedom."
          }
        },
        {
          "section": "introduction",
          "title": "Introduction",
          "sentences": {
            "3": "Automatic grammar checking is one of the fields of natural language processing where simple means do not provide satisfactory results.",
            "4": "This statement is even more true with respect to grammar checking of the so-called free word order languages.",
            "5": "With the growing degree of word order freedom the usability of simple pattern matching techniques decreases.",
            "6": "In languages with such a high degree of word order freedom as in most Slavic languages the set of syntactic errors that may be detected by means of simple pattern matching methods is almost negligible.",
            "7": "This is probably one of the reasons, why even though the famous paper [CH83] was written as long as 13 years ago, there are still very few articles about this topic, except papers like [K94] or [M96] which appeared only during the last three years.",
            "8": "In the present paper we describe the basic ideas behind an implementation of a prototype of a grammar checker for Czech.",
            "9": "During the development of this application we had to solve a number of problems concerning the theoretical background, to develop a formalism allowing efficient implementation and of course to create a grammar and define the structure of the lexical data.",
            "10": "The last but not least problem was to incorporate the prototype into an existing text editor.",
            "11": "How does the system work In order to demonstrate the function of the pivot implementation of our system we decided to connect it to a commercially available text editor.",
            "12": "We intended to create a DLL library with the standard grammar checking interface required by a particular text editor.",
            "13": "This idea turned out to be unrealistic because the necessary interface is among the classified inside information in most companies.",
            "14": "Fortunately there is the possibility to use a concept of Dynamic Data Exchange (DDE) for the communication between programs in the Microsoft Windows environment.",
            "15": "This type of connection is of course much slower than the intended one, but for the purpose of this demonstration the difference in speed is not so important.",
            "16": "Our system can work with any text editor under Windows that contains a macro language supporting the DDE connection.",
            "17": "For the purpose of the pivot implementation of the system we have chosen Microsoft Word 6.0.",
            "18": "The grammar checker is implemented as an independent Windows application (GRAMMAR.EXE) which runs on the background of the Word.",
            "19": "In order to be able to use GRAMMAR.",
            "20": "EXE, we had to create a macro Grammar, assigned to the Grammar Checker item in the Tools menu.",
            "21": "This macro selects a current sentence, sends it to GRAMMAR.EXE via DDE, receives the result and indicates the type of the result to the user.",
            "22": "This activity is being performed for all sentences in the selection or for all sentences from the position of the cursor till the end of document.",
            "23": "The user may get several types of messages about the correctness of the text: a) The macro changes the color of words in the text according to the type of the detected error - the unknown words are marked blue, the pairs of words involved in a syntactic error are marked red.",
            "24": "b) The macro creates a message box with a warning each time there is an undesired result of grammar checking — either there was no result or the sentence was too complicated.",
            "25": "c) In case that the grammar checker identified and localized an error, it creates a message box with a short description of the error(s).",
            "26": "Because the grammar checker is running as an independent application, the user may also look at the complete results provided by it.",
            "27": "When a message box containing an error message appears on the screen, the user may switch to GRAMMAR and get an additional information.",
            "28": "The main window of GRAMMAR is able to provide either the complete list of errors, the statistics concerning for example the number of different syntactic trees built during grammar checking or even the result in the form of a syntactic tree.",
            "29": "We do not suppose that the last option is interesting for a typical user, but if we do have all this information, why should we throw it out?",
            "30": "The architecture of the system 1.Morphological and lexical analysis This part is in fact an extended spelling checker.",
            "31": "The input text is first checked for spelling errors, then the lexical and morphological analysis creates data, which are combined with the information contained in a separate syntactic dictionary.",
            "32": "It would of course be possible to use only one dictionary containing morphosyntactic information about particular words (lemmas), but for the sake of an easier update of information during the development of the system we have decided to keep morphemic and syntactic data in separate files.",
            "33": "2.Grammar checking (extended variant of syntactic parsing) This is the main part of the system.",
            "34": "It tries to analyze the input sentence.",
            "35": "There are three possible results of the analysis: partial trees are very ineffective and they can also slow down substantially the processing of the given sentence."
          }
        },
        {
          "section": "method",
          "title": "3. Evaluation",
          "sentences": {
            "36": "This phase takes the results of the previous phase in the form of syntactic trees containing markers describing individual syntactic inconsistencies.",
            "37": "It tries to locate the source of the error using an algorithm that compares available trees.",
            "38": "According to the settings given by the user the evaluation phase issues warnings or error messages.",
            "39": "The core of the system is the second, grammar checking phase, therefore we will concentrate on the description of that phase.",
            "40": "The design of our system was motivated by a simple and natural idea — the grammar checker should not spend too much time on simple correct sentences.",
            "41": "The composition of a grammar checking module tries to stick to this idea as much as possible.",
            "42": "The processing of an input sentence is divided into three phases: b) Positive nonprojective & negative projective This phase tries to find a syntactic tree which either contains negative symbols or nonprojective constructions.",
            "43": "A nonprojective subtree is a subtree with discontinuous coverage.",
            "44": "It is often the case — for example in wh-sentences — that the sentence may be considered either syntactically incorrect or nonprojective — see examples in [C0L94j.",
            "45": "If such a syntactic tree exists, the evaluation phase tries to decide if there should be an error message, warning or nothing.",
            "46": "Let us present a slightly modified sentence from the previous paragraph: \"Karlovy Zena zalevala lcvetiny\".",
            "47": "(Word for word translation: Charles&apos;[fem.p1.] wife watered flowers).",
            "48": "This sentence is ambiguous, it is either correct and nonprojective (meaning: Woman watered Charles&apos; flowers) or incorrect (disagreement in number between \"Karlovy\" and \"Zena\") and projective.",
            "49": "Both results are achieved by this phase of the grammar checker:"
          }
        },
        {
          "section": "method",
          "title": "KARLOUY",
          "sentences": {
            "50": "c) Negative nonprojective Both nonprojective constructions and negative symbols are allowed.",
            "51": "If this phase succeeds, the evaluation module issues a relevant error message or warning.",
            "52": "In case that neither phase provides any result, no error message is issued.",
            "53": "In case that the user wants to know which sentences were not analyzed properly, s/he may obtain a warning.",
            "54": "Although this division into phases worked fine for short sentences (for the sentences not more than 15 words long the first phase usually took about 1 second on Pentium 75 MHz), long and complicated sentences were unacceptably slow (even tens of seconds).",
            "55": "These results turned our attention to the problem how to speed up the processing of correct sentences even further.",
            "56": "With the growing length of sentences the parsing will be more complex with respect both to the length of the processing and to the number of resulting syntactic structures.",
            "57": "Let us demonstrate the problem on a sample sentence from the corpus of Czech newspaper texts from the newspaper Lidove noviny.",
            "58": "Let us take the sentence: \"KDS neptedpoklada spolupraci se stranou pana Sladka a neni pravdou, e ptedseda ktesfanslcjfch demokratu pan Benda v telefonickern rozhovoru s Petrem Pithartem prosazoval ing.",
            "59": "Dejmala do funkce ministra Zivotniho prostredf\".",
            "60": "(Word for word translation: \"CDP [does] not suppose cooperation with party [of] Mister Sladek and [it] isn&apos;t true, that chairman [of] Christian democrats Mister Benda in telephone discussion with Petr Pithart enforced ing.",
            "61": "Dejmal to function [of] minister [of] environment\").",
            "62": "In this basic form of the sentence, which is an exact transcription of the text from the corpus, the processing by the positive projective phase of our parser takes 13,07s and it provides 26 different variants of syntactic trees.",
            "63": "During the processing there were 2272 items derived.",
            "64": "The testing of this sentence and also of all the following ones was performed on Pentium 75MHz with I6MB RAM.",
            "65": "Such a relatively large number of variants is caused by the fact that our syntactic analysis uses only purely syntactic means - we do not take into account either semantics or textual or sentential context.",
            "66": "That is the reason why free modifiers at the end of our sample sentence create a great number of variants of syntactic structures and thus make the processing longer and more complicated.",
            "67": "In order to demonstrate this problem we will take this sentence and modify it trying to find out what the main source of ineffectiveness of its parsing is.",
            "68": "If we look more closely at the number of ambiguities present with individual words, we notice that the most ambiguous word is the word (abbreviation) \"ing\".",
            "69": "This word form is the same in all cases, genders and numbers.",
            "70": "If we substitute this abbreviation by the full form of the word (\"inZenSira\" [engineer - [gen.]]) we get the following results: the sentence is processed 8,95s, the number of variants decreases by four (22) and the number of derived items is, of course, also smaller (1817).",
            "71": "The gain of speed would be even greater would we have worked with a negative or a nonprojective variant of the parser.",
            "72": "The next step is to delete further groups of words from the input sentence.",
            "73": "Among the suitable candidates there is, for example, the prepositional phrase \"v telefonickem rozhovoru\" (in [the] telephone discussion).",
            "74": "This phrase can be easily checked for grammatical correctness locally, because it has a clear left and right borders (prepositions &apos;vand \"s\").",
            "75": "Here we can easily solve the problem where the nominal group ends on the right hand side.",
            "76": "In general, we need to parse the whole sentence in order to get this information, but in some specific cases we can rely only on the surface word order.",
            "77": "After we had deleted this phrase, the processing time went down to 8,79s, the same number of syntactic representations as in the previous case was derived (22) and the number of items was slightly lower (1789).",
            "78": "This phrase is therefore certainly not the main source of ineffectiveness in parsing.",
            "79": "In order to speed up the processing even more we have to use another type of simplification.",
            "80": "The first step of simplifying the original input sentence represented almost 50% acceleration although it was only a cosmetic change from an abbreviation to a full word form.",
            "81": "From the point of view of localisation of grammatical inconsistencies we can proceed even farther - the group title+surname in fact represents only one item; if we remove titles preceding surnames we do not change syntactic structure of the sentence.",
            "82": "It is locally only a tiny bit simpler.",
            "83": "When we look more closely at the resulting syntactic representation of the previous variants of the input sentence we may notice that the word \"inieqra\" [engineer[gen.]] figures (inadequately, of course, in this case) also as a righthand attribute to the word \"Pithartem[instri\", as it is shown in the following screenshots (for the sake of simplicity we demonstrate only the relevant part of derivation trees).",
            "84": "inlenS7ra Dejmala do funkce ministra 2ivotniho prostredi\".",
            "85": "[\"Chairman [of] Christian democrats Mister Benda in telephone discussion with Petr Pithart enforced ing.",
            "86": "Dejmal to function [of] minister [of] environment.\").",
            "87": "If we take into account the results of the previous examples we should not be surprised by the results.",
            "88": "The processing time is 2,25s, 10 structures were created and 722 items were derived.",
            "89": "Let us remove the word \"inien9ra\" from the input sentence altogether.",
            "90": "This time the processing time is only 3,74s, only 10 structures are created and 1021 items are derived.",
            "91": "Another logical step is to remove all other first names and titles which are placed immediately in front of their governing words.",
            "92": "Those words are \"pana\" [mister [gen.]], \"pan\" and \"Petrem\".",
            "93": "The claim that the first two words are unambiguous is supported by the fact that the form of the word \"pan\" [mister] is different in Czech in case the word is \"independent\" and in case it is used as a title (pana vs. pana [gen.",
            "94": ",acc.], pan vs. pan[nom.]).",
            "95": "When we make this change we get more than 50% shorter processing time, namely 1,71s, also the number of resulting structures is a half of the original number (5) and only 587 items are derived.",
            "96": "Another change we would like to demonstrate is the deletion of all other free modifiers the result of which is a certain \"backbone\" of the sentence.",
            "97": "After having carried out all deletions, we arrive at the following structure: \"KDS neptedpoklada spolupraci a neni pravdou, Ze Benda prosadil Dejmala\".",
            "98": "(Word for word translation: \"CDP [does] notsuppose cooperation and [it] isn&apos;t true, that Benda enforced Dejmal\").",
            "99": "The result of the processing is a unique structure and 141 items are derived in 0,22s.",
            "100": "The last variant of the input sentence will serve as a contrast to the previous ones.",
            "101": "Let us take the last clause of the sentence, namely \"Predseda kresfanskSich demokrattl pan Benda v telefonickem rozhovoru s Petrem Pithartem prosazoval This example and also other test data showed that the main source of ineffectivity are clauses with a big number of free modifiers and adjuncts rather than complex sentences with many clauses.",
            "102": "These results have led us to a layered design of grammar for positive projective parsing.",
            "103": "The core idea of this approach is the following: Syntactic constructions which even in free word order languages may be parsed locally (certain adjectival or prepositional phrases etc).",
            "104": "should be parsed first in order to avoid their mutual unnecessary (from the point of view of grammar checking)! combinations.",
            "105": "This means that the grammar should be divided into certain layers of rules (not necessarily disjunctive), which will be applied one after the other (in principle they may be applied even in cycles, but this options is not used in our implementation).",
            "106": "In the pivot version of our system we use the following layers: with those processing of complex sentences 5th layer: metarules for processing the left sentinel and the right hand side sentential border The application of layers may slow down the processing of short sentences (it has a fixed cost of opening the description file and consulting it during parsing process), therefore it is applied only to sentences longer than certain threshold (currently 15 words).",
            "107": "Another important point is, that the results of parsing in layers provides only positive information (i.e.",
            "108": "it is able to sort out sentences which are certainly correct, but the failure of parsing in layers does not necessarily mean that the sentence is incorrect).",
            "109": "The same approach may not be used for error localization and identification, although the cases when parsing in layers fails on a correct sentence are quite rare.",
            "110": "The implementation of our system was to a big extent influenced by the demand of effectiveness.",
            "111": "For this reason we had to abandon even feature structures as the form of the representation of lexical data.",
            "112": "Our data structure is a set of attribute-value pairs with the data about valency frames of particular words as the only complex values (embedded attribute-value pairs).",
            "113": "An example of the representation of the Czech wordform \"informoval\" ([he] informed) follows: The grammar of the system is composed of metarules representing whole sets of rules of the background formalism called Robust Free Order Dependency Grammar (RFODG).",
            "114": "The limited space of this paper does not allow to present the full description of RFODG here.",
            "115": "The definition may be found for example in [TR96].",
            "116": "The RFODG provides a formal base for the description of nonprojective and incorrect syntactic constructions.",
            "117": "It introduces three measures by means of which it is possible to classify the degree of nonprojectivness and incorrectness of a particular sentence.",
            "118": "In this paper we would like to stress one important feature of this formalism, namely the classification of the set of symbols which are used by RFODG into three types: a) terminals and nonterminals b) deletable and nondeletable symbols c) positive and negative symbols The sets under a) have the usual meaning, the sets under b) serve for the classification of syntactic inconsistencies and the sets under c) serve for their localisation.",
            "119": "The union of terminals and nonterminals is exactly the set of all symbols used by RFODG.",
            "120": "The same holds about the union of deletable and nondeletable symbols and also about the union of positive and negative symbols.",
            "121": "In other words, each symbol used by RFODG belongs to exactly one set from each pair of sets under a), b) and c).",
            "122": "This classification therefore allows to handle rules describing both correct and erroneous syntactic constructions in a uniform way and to use a single grammar for the description of both types of syntactic constructions.",
            "123": "Whenever a metarule describing syntactic inconsistency is used during the parsing process, a negative symbol is inserted into the tree created according to the grammar.",
            "124": "The metarules express a procedural description of the process of checking the applicability of a given metarule to a particular pair of input items A and B (A stands to the left from B i n the input).",
            "125": "In case that a particular rule may be applied to items A and B, a new item X is created.",
            "126": "It is possible to change values of the resulting item X by means of an assignment operator :=. The constraint relaxation technique is implemented in the form of so called \"soft constraints\" - the constraints with an operator? accompanied by an error marker may be relaxed in phases b) and c) (\"hard constraints\" with an operator = may never be relaxed).",
            "127": "The error anticipating rules are marked by a keyword NEGATIVE at the beginning of the rule and are applied only in phases b) and c).",
            "128": "The keyword PROJECTIVE indicates that the rule may be applied only in a projective way.",
            "129": "An example of a (simplified) metarule describing the attachment of a nominal modifier in genitive case from the right hand side of the noun:"
          }
        },
        {
          "section": "method",
          "title": "PROJECTIVEIF A.SYNTCL = n THEN ELSE",
          "sentences": {
            "130": "IF A.SYNTCL = prep2 THEN ELSE"
          }
        },
        {
          "section": "method",
          "title": "FAIL ENDIFENDIFB.SYNTCL = nB.case = genA.RIGHTGEN = yesIF A.TITUL = yes THEN",
          "sentences": {
          }
        },
        {
          "section": "method",
          "title": "IF A.CASE = gen THENIF A.GENDER = B.GENDERTHENIF A.NUM = B.NUMTHEN FAIL ELSE ENDIFELSE ENDIFELSE ENDIFELSE ENDIFX:=AX.RIGHTGEN := noOKENDS",
          "sentences": {
            "131": "The interpretation of the grammar is performed by means of a slightly modified CYK algorithm (a description of this algorithm may be found for example in [S97].",
            "132": "The grammar works with unambiguous input data (ambiguous words are represented as sets of unambiguous items).",
            "133": "All partial parses from the first phase are used in the phases b) and c).",
            "134": "For the purpose of testing and debugging the system we use full parsing even in the first phase.",
            "135": "It is often the case that nondeterministic parsers the author of the grammar has to prevent an unnecessary multiplication of results by means of \"tricks\" which are not supported by the linguistic theory — let us take for example the problem of subject — predicate — object construction.",
            "136": "If we do not put any additional restriction on the order of application of rules then the rule filling the subcategorization slots for subject and object may be applied in two ways, either first filling the slot for the subject and then the object or vice versa.",
            "137": "Both ways create the same syntactic structure.",
            "138": "In such a case it is necessary to apply some additional constraints in the grammar — for example the restriction on the order of subcategorization (an item to the left of a verb should be processed first).",
            "139": "This approach makes the grammar more complicated than it is necessary and it may also influence the quality of results (an error on the left hand side of a verb may also prevent an attachment of the items from the right hand side of the verb).",
            "140": "The interpreter of our grammar solves these situations itself.",
            "141": "Every time a new item is created, the interpreter checks, if such an item with the same structure and coverage already exists.",
            "142": "If yes, the new item is deleted.",
            "143": "This property of the interpreter is used together with other kinds of pruning techniques in all phases of grammar checking.",
            "144": "In addition, there are also some other techniques used especially in phases b) and c).",
            "145": "The work with unambiguous input symbols allows fast parsing in the phase a) (CYK is polynomial with respect to the length of the input), but creates some problems in the context of constraint relaxations used in subsequent phases.",
            "146": "For example, a typical error in \"free word order\" languages is an error in agreement.",
            "147": "Let us suppose that we have the following three input words (the actual lexical value of these words may be neglected): Preposition (accusative or locative) Adjective (animate or inanimate gender, genitive or accusative sing).",
            "148": "Noun (animate, genitive or accusative sing).",
            "149": "These words represent 2 + 4 + 2 = 8 unambiguous items.",
            "150": "If we try to create a prepositional phrase without constraint relaxation, we get one resulting item PP(animate, accusative sing.).",
            "151": "On the other hand after the relaxation of constraints there are 16 items created.",
            "152": "One of them does not contain any syntactic inconsistency, remaining 15 has one or two syntactic inconsistencies.",
            "153": "In a nondeterministic parser all 16 variants are used in the subsequent parsing.",
            "154": "This causes a combinatorial explosion of mostly incorrect results.",
            "155": "There are two ways how to solve this problem.",
            "156": "The first possible solution is to relax the constraints in certain order (to apply a hierarchy on constraints).",
            "157": "We have chosen the other possible way, which prefers the subtrees with minimal number of errors.",
            "158": "Every time a new branch or subtree is created, it is compared with the other branches or subtrees with the same structure and coverage and if it contains more errors than those already existing, it is not parsed further.",
            "159": "This technique substantially speeds up the processing of rules with relaxed constraints, but it has also one rather unpleasant side effect: the syntactic inconsistencies may be suppressed and appear later in a different location.",
            "160": "This makes the task of the evaluating part of our system a bit more difficult, but nevertheless the gain on effectivity not accompanied by the loss of recall justifies the use of this technique."
          }
        },
        {
          "section": "conclusions",
          "title": "Conclusion",
          "sentences": {
            "161": "The main purpose of the demo of our system is to demonstrate a method of grammar based grammar checking of a \"free word order\" language.",
            "162": "The system is far from being ready for commercial exploitation - the main obstacle is the size of the syntactic dictionary used.",
            "163": "Grammar based methods require a complex syntactic information about words.",
            "164": "To build a syntactic dictionary of about 150 000 items is a task which exceeds our current capacities with respect both to manpower and funds.",
            "165": "It would be interesting to continue the work on our system towards the development of statistical methods for this task."
          }
        }
      ],
      "doc_id": 16,
      "actual_doc_id": "A97-1022-parscit-section.xml"
    },
    {
      "content": [
        {
          "section": "abstract",
          "title": "Abstract",
          "sentences": {
            "0": "This paper presents an algorithm for learning the probabilities of optional phonological rules from corpora.",
            "1": "The algorithm is based on using a speech recognition system to discover the surface pronunciations of words in speech corpora; using an automatic system obviates expensive phonetic labeling by hand.",
            "2": "We describe the details of our algorithm and show the probabilities the system has learned for ten common phonological rules which model reductions and coarticulation effects.",
            "3": "These probabilities were derived from a corpus of 7203 sentences of read speech from the Wall Street Journal, and are shown to be a reasonably close match to probabilities from phonetically hand-transcribed data (TIMIT).",
            "4": "Finally, we analyze the probability differences between rule use in male versus female speech, and suggest that the differences are caused by differing average rates of speech."
          }
        },
        {
          "section": "keywords",
          "title": "1 Introduction",
          "sentences": {
            "5": "Phonological rules have formed the basis of phonological theory for decades, although their form and their coverage of the data has changed over the years.",
            "6": "Until recently, however, it was difficult to determine the relationship between hand-written phonological rules and actual speech data.",
            "7": "The current availability of large speech corpora and pronunciation dictionaries has allowed us to connect rules and speech in much tighter ways.",
            "8": "For example, a number of algorithms have recently been proposed which automatically induce phonological rules from dictionaries or corpora (Gasser 1993; Ellison 1992; Daelemans et al. 1994).",
            "9": "While such algorithms have successfully induced syllabicity or harmony constraints, or simple obligatory phonological rules, there has been much less work on non-obligatory (optional) rules.",
            "10": "In part this is because optional rules like flapping, vowel reduction, and various coarticulation effects are postlexical and often products of fast speech, and hence have been considered less central to phonological theory.",
            "11": "In part, however, this is because optional rules are inherently probabilistic.",
            "12": "Where obligatory rules apply to every underlying form which meets the environmental conditions, producing a single surface form, optional rules may not apply, and hence the underlying form may appear as the surface form, unmodified by the rule.",
            "13": "This makes the induction problem non-deterministic, and not solvable by the above algorithms.",
            "14": "1 While optional rules have received less attention in linguistics because of their probabilistic nature, in speech recognition, by contrast, optional rules are commonly used to model pronunciation variation.",
            "15": "In this paper, we employ techniques from speech recognition research to address the problem of assigning probabilities to these optional phonological rules.",
            "16": "We introduce a completely automatic algorithm that explores the coverage of a set of phonological rules on a corpus of lexically transcribed speech using the computational resources of a speech recognition system.",
            "17": "This algorithm belongs to the class of techniques we call Exploratory Computational Phonology, which use statistical pattern recognition tools to explore phonological spaces.",
            "18": "We describe the details of our probability estimation algorithm and also present the probabilities the system has learned for ten common phonological rules which model reductions and coarticulation effects.",
            "19": "Our probabilities are derived from a corpus of 7203 sentences of read speech from the Wall Street Journal (NIST 1993).",
            "20": "We also benchmark the probabilities generated by our system against probabilities from phonetically hand-transcribed data, and show a relatively good fit.",
            "21": "Finally, we analyze the probability differences between rule use in male versus female speech, and suggest that the differences erate phone sequences from word orthography as an are caused by differing average rates of speech.",
            "22": "additional source of pronunciations."
          }
        },
        {
          "section": "introduction",
          "title": "2 The Algorithm",
          "sentences": {
            "23": "In this section we describe our algorithm which assigns probabilities to hand-written, optional phonological rules like flapping.",
            "24": "The algorithm takes a lexicon of underlying forms and applies phonological rules to produce a new lexicon of surface forms.",
            "25": "Then we use a speech recognition system on a large corpus of recorded speech to check how many times each of these surface forms occurred in the corpus.",
            "26": "Finally, by knowing which rules were used to generate each surface form, we can compute a count for each rule.",
            "27": "By combining this with a count of the times a rule did not apply, the algorithm can compute a probability for each rule.",
            "28": "The rest of this section will discuss each of the aspects of the algorithm in detail.",
            "29": "Our base lexicon is quite large; it is used to generate the lexicons for all of our speech recognition work at ICSI.",
            "30": "It contains 160,000 entries (words) with 300,000 pronunciations.",
            "31": "The lexicon contains underlying forms which are very shallow; thus they are post-lexical in the sense that there is no represented relationship between e.g.",
            "32": "'critic' and 'criticism' (where critic is pronounced kritik and criticism kritisizm).",
            "33": "However, the entries do not represent flaps, vowel reductions, and other coarticulatory effects.",
            "34": "In order to collect our 300,000 pronunciations, we combined seven different on-line pronunciation dictionaries, including the five shown in Table 12.",
            "35": "For further information about these sources please refer to CMU (CMU 1993), LIMSI (Lamel 1993), PRONLEX (COMLEX 1994), BRITPRON (Robinson 1994).",
            "36": "A text-to-speech system was used to genBET.",
            "37": "This was expanded to include syllabics, stop closures, and reduced vowels, alveolar flap, and voiced h.",
            "38": "We represent pronunciations with the set of 54 ARPAbet-like phones detailed in Table 2.",
            "39": "All the lexicon sources except LIMSI use ARPABET-like phone sets'.",
            "40": "CMU, BRITPRON, and PRONLEX phone sets include three levels of vowel stress.",
            "41": "The pronunciations from all these sources were mapped into our phone set using a set of obligatory rules for stop closures [bcl, del, gcl, pd, tcl, kcl], and optional rules to introduce the syllabic consonants [el, em, en], reduced vowels [ax, ix, axr], voiced h [hv], and alveolar flap [dx].",
            "42": "We next apply phonological rules to our base lexicon to produce the surface lexicon.",
            "43": "Since the rules are optional, the surface lexicon must contain each underlying pronunciation unmodified, as well as the pronunciation resulting from the application of each relevant phonological rule.",
            "44": "Table 3 gives the 10 phonological rules used in these experiments.",
            "45": "One goal of our rule-application procedure was to build a tagged lexicon to avoid having to implement a phonological-rule parser to parse the surface pronunciations.",
            "46": "In a tagged lexicon, each surface pronunciation is annotated with the names of the phonological rules that applied to produce it.",
            "47": "Thus when the speech recognizer finds a particular pronunciation in the speech input, the list of rules which applied to produce it can simply be looked up in the tagged lexicon.",
            "48": "The algorithm applies rules to pronunciations recursively; when a context matches the left hand side of a phonological rule \"RULE,\" two pronunciations are produced: one unchanged by the rule (marked -RULE), and one with the rule applied (marked +RULE).",
            "49": "The procedure places the +RULE pronunciation on the queue for later recursive rule application, and continues trying to apply phonological rules to the -RULE pronunciation.",
            "50": "See Figure 1 for details of the algorithm.",
            "51": "While our procedure is not guaranteed to terminate, in practice the phonological rules we apply have a finite recursive depth.",
            "52": "The nondeterministic mapping produces a tagged equiprobable multiple pronunciation lexicon of 510,000 pronunciations for 160,000 words.",
            "53": "For example, Table 4 gives our base forms for the word \"butter\": Source Pronunciation TTS b ah t axr BPU b ah t ax BPU b ah t axr CMU b ah t er LIM b ah t axr PLX b ah t er The resulting tagged surface lexicon would have the entries in Table 5.",
            "54": "Given a lexicon with tagged surface pronunciations, the next required step is to count how many times each of these pronunciations occurs in a speech corpus.",
            "55": "The algorithm we use has two steps; PHONETIC LIKELIHOOD ESTIMATION and FORCEDVITERBI ALIGNMENT.",
            "56": "In the first step, PHONETIC LIKELIHOOD ESTIMATION, we examine each 20ms frame of speech data, and probabilistically label each frame with the phones that were likely to produce the data.",
            "57": "That is, for each of the 54 phones in our phone-set, we compute the probability that the slice of acoustic data was produced by that phone.",
            "58": "The result of this labeling is a vector of phone-likelihoods for each acoustic frame.",
            "59": "Our algorithm is based on a multi-layer perceptron (MLP) which is trained to compute the conditional probability of a phone given an acoustic feature vector for one frame, together with 80 ms of surrounding context.",
            "60": "Bourlard & Morgan (1991) and Renals et al.",
            "61": "(1991) show that with a few assumptions, an MLP may be viewed as estimating the probability P(q1x) where q is a phone and x is the input acoustic speech data.",
            "62": "The estimator consists of a simple three-layer feed forward MLP trained with the back-propagation algorithm (see Figure 2).",
            "63": "The input layer consists of 9 frames of input speech data.",
            "64": "Each frame, representing 10 msec of speech, is typically encoded by 9 PLP (Hermansky 1990) coefficients, 9 delta-PLP coefficients, 9 deltadelta PLP coefficients, delta-energy and delta-deltaenergy terms.",
            "65": "Typically, we use 500-4000 hidden units.",
            "66": "The output layer has one unit for each phone.",
            "67": "The MLP is trained on phonetically hand-labeled speech (TIMIT), and then further trained by an iterative Viterbi procedure (forced-Viterbi providing the labels) with Wall Street Journal corpora.",
            "68": "The second step of the algorithm, FORCEDVITERBI ALIGNMENT, takes this vector of likelihoods for each frame and produces the most likely phonetic string for the sentence.",
            "69": "If each word had only a single pronunciation and if each phone had some fixed duration, the phonetic string would be completely determined by the word string.",
            "70": "However, phones vary in length as a function of idiolect and rate of speech, and of course the very fact of optional phonological rules implies multiple possible pronunciations for each word.",
            "71": "These pronunciations are encoded in a hidden Markov model (HMM) for each word.",
            "72": "The Viterbi algorithm is a dynamic programming search, which works by computing for each phone at each frame the most likely string of phones ending in that phone.",
            "73": "Consider a sentence whose first two words are \"of the\", and assume the simplified lexicon in Figure 3.",
            "74": "Each pronunciation of the words 'of' and 'the' is represented by a path through the probabilistic automaton for the word.",
            "75": "For expository simplicity, we have made the (incorrect) assumption that consonants have a duration of 1 frame, and vowel a duration of 2 or 3 frames.",
            "76": "The algorithm analyzes the input frame by frame, keeping track of the best path of phones.",
            "77": "Each path is ranked by its probability, which is computed by multiplying each of the transition probabilities and the phone probabilities for each frame.",
            "78": "Figure 4 shows a schematic of the path computation.",
            "79": "The size of each dot indicates the magnitude of the local phone likelihood.",
            "80": "The maximum path at each point is extended; non-maximal paths are pruned.",
            "81": "The result of the forced-Viterbi alignment on a single sentence is a phonetic labeling for the sentence (see Figure 5 for an example), from which we The probability P(q1x) produced by the MLP for each frame is first converted to the likelihood P(xlq) by dividing by the prior P(q), according to Bayes' rule; we ignore P(x) since it is constant here: P(x q) = P(q x)P(x) I new york city's fresh can produce a phonetic pronunciation for each word.",
            "82": "By running this algorithm on a large corpus of sentences, we produce a list of \"bottom-up\" pronunciations for each word in the corpus.",
            "83": "The rule-tagged surface lexicon described in §2.1 and the counts derived from the forced-Viterbi described in §2.3 can be combined to form a tagged lexicon that also has counts for each pronunciation of each word.",
            "84": "Following is a sample entry from this lexicon for the word Adams which shows the five derivations for its single pronunciation: Each pronunciation of each word in this lexicon is annotated with rule tags.",
            "85": "Since each pronunciation may be derived from different source dictionaries or via different rules, each pronunciation of a word may contain multiple derivations, each consisting of the list of rules which applied to give the pronunciation from the base form.",
            "86": "These tags are either positive, indicating that a rule applied, or negative, indicating that it did not.",
            "87": "To produce the initial rule probabilities, we need to count the number of times each rule applies, out of the number-of times it had the potential to apply.",
            "88": "If each pronunciation only had a single derivation, this would be computed simply as follows: However, since each pronunciation can have multiple derivations, the counts for each rule from each derivation need to be weighted by the probability of the derivation.",
            "89": "The derivation probability is computed simply by multiplying together the probability of each of the applications or non-applications of the rule.",
            "90": "Let Now a single iteration of the rule-probability algorithm must perform the following computation: Since we have no prior knowledge, we make the zero-knowledge initial assumption that P(dip) = IDERI1V S(p)I.",
            "91": "The algorithm can the be run as a successive estimation-maximization to provide successive approximations to P(djp).",
            "92": "For efficiency reasons, we actually compute the probabilities of all rules in parallel, as shown in Figure 6.",
            "93": "For each word/pron pair P E P RON from forced-Viterbi alignment Let DE RIV S(P) be the set of rule derivations of P For every d E DE RIV S(P) For every rule R E d"
          }
        },
        {
          "section": "background",
          "title": "3 Results",
          "sentences": {
            "94": "We ran the estimation algorithm on 7203 sentences (129,864 words) read from the Wail Street Journal.",
            "95": "The corpus (1993 WSJ Hub 2 (WSJ 0) training data) -Consisted of 12 hours of speech, and had 8916 unique words.",
            "96": "Table 6 shows the probabilities for the ten phonological rules described in §2.2.",
            "97": "Note that all of the rules are indeed quite optional; even the most commonly-employed rules, like flapping and h-voicing, only apply on average about 90% of the time.",
            "98": "Many of the other rules, such as the reduced-vowel or reduced-liquid rules, only apply about 50% of the time.",
            "99": "We next attempted to judge the reliability of our automatic rule-probability estimation algorithm by comparing it with hand transcribed pronunciations.",
            "100": "We took the hand-transcribed pronunciations of each word in TIMIT, and computed rule probabilities by the same rule-tag counting procedure used for our forced-Viterbi output.",
            "101": "Figure 7 shows the fit between the automatic and hand-transcribed probabilities.",
            "102": "Since the TIMIT pronunciations were from a completely different data collection effort with a very different corpus and speakers, the closeness of the probabilities is quite encouraging.",
            "103": "Figure 8 breaks down our automatically generated rule probabilities for the Wall Street Journal corpus into male and female speakers.",
            "104": "Notice that many of the rules seem to be employed more often by men than by women.",
            "105": "For example, men are about 5% more likely to flap, more likely to reduce vowels ih and er, and slightly more likely to reduce liquids and nasals.",
            "106": "Since these are coarticulation or fast-speech effects, our initial hypothesis was that the difference between male and female speakers was due to a faster speech-rate by males.",
            "107": "By computing the weighted average seconds per phone for male and female speakers, we found that females had an average of 71 ms/phone, while males had an average of 68 ms/phone, a difference of about 4%, quite correlated with the similar differences in reduction and flapping."
          }
        },
        {
          "section": "related work",
          "title": "4 Related Work",
          "sentences": {
            "108": "Our algorithm for phonological rule probability estimation synthesizes and extends earlier work by (Cohen 1989) and (Wooters 1993).",
            "109": "The idea of using optional phonological rules to construct a speechrecognition lexicon derives from Cohen (1989), who applied optional phonological rules to a baseform dictionary to produce a surface lexicon and then used TIMIT to assign probabilities for each pronunciation.",
            "110": "The use of a forced-Viterbi speech decoder to discover pronunciations from a corpus was proposed by Wooters (1993).",
            "111": "Wesenick & Schiel (1994) independently propose a very similar forced-Viterbidecoder-based technique which they use for measuring the accuracy of hand-written phonology.",
            "112": "Chen (1990) and Riley (1991) model the relationship between phonemes and their allophonic realizations by training decision trees on TIMIT data.",
            "113": "A decision tree is learned for each underlying phoneme specifying its.surface realization in different contexts.",
            "114": "These completely automatic techniques, requiring no hand-written rules, can allow a more fine-grained analysis than our rule-based algorithm.",
            "115": "However, as a consequence, it is more difficult to extract generalizations across classes of phonemes to which rules can apply.",
            "116": "We think that a hybrid between a rule-based and a decision-tree approach could prove quite powerful ."
          }
        },
        {
          "section": "conclusions",
          "title": "5 Conclusion and Future Work",
          "sentences": {
            "117": "Although the paradigm of exploratory computational phonology is only in its infancy, we believe our rule-probability estimation algorithm to be a new and useful instance of the use of probabilistic techniques and spoken-language corpora in computational linguistics.",
            "118": "In Tajchman et al.",
            "119": "(1995) we report on the results of our algorithm on speech recognition performance.",
            "120": "We plan in future work to address a number of shortcomings of these experiments, for example including some spontaneous speech corpora, and looking at a wider variety of rules.",
            "121": "In addition, we have extended our algorithm to induce new pronunciations which generalize over pronunciations seen in the corpus (Wooters & Stolcke 1994).",
            "122": "We now plan to augment our probability estimation to use the pronunciations from this new HMM-induction-based generalization step.",
            "123": "This will require extending our tag-based probability estimation step to parse the phone strings from the forcedViterbi.",
            "124": "In other current work we have also been using this algorithm to model the phonological component of the accent of non-native speakers.",
            "125": "Finally, we hope in future work to be able to combine our rulebased approach with more bottom-up methods like the decision-tree or phonological parsing algorithms to induce rules as well as merely training their probabilities."
          }
        },
        {
          "section": "acknowledgments",
          "title": "Acknowledgments",
          "sentences": {
            "126": "Thanks to Mike Hochberg, Nelson Morgan, Steve Renals, Tony Robinson, Florian Schiel, Andreas Stolcke, and Chuck Wooters.",
            "127": "This work was partially funded by ICSI and an SRI subcontract from ARPA contract MDA904-90-C-5253.",
            "128": "Partial funding also came from ESPRIT project 6487 (The Wernicke project)."
          }
        }
      ],
      "doc_id": 17,
      "actual_doc_id": "P95-1001-parscit-section.xml"
    },
    {
      "content": [
        {
          "section": "abstract",
          "title": "Abstract",
          "sentences": {
            "0": "Identifying and classifying personal, geographic, institutional or other names in a text is an important task for numerous applications.",
            "1": "This paper describes and evaluates a language-independent bootstrapping algorithm based on iterative learning and re-estimation of contextual and morphological patterns captured in hierarchically smoothed trie models.",
            "2": "The algorithm learns from unannotated text and achieves competitive performance when trained on a very short labelled name list with no other required language-specific information, tokenizers or tools."
          }
        },
        {
          "section": "introduction",
          "title": "1 Introduction",
          "sentences": {
            "3": "The ability to determine the named entities in a text has been established as an important task for several natural language processing areas, including information retrieval, machine translation, information extraction and language understanding.",
            "4": "For the 1995 Message Understanding Conference (MUC-6), a separate named entity recognition task was developed and the best systems achieved impressive accuracy (with an F-measure approaching 95%).",
            "5": "What should be underlined here is that these systems were trained for a specific domain and a particular language (English), typically making use of hand-coded rules, taggers, parsers and semantic lexicons.",
            "6": "Indeed, most named entity recognizers that have been published either use tagged text, perform syntactical and morphological analysis or use semantic information for contextual clues.",
            "7": "Even the systems that do not make use of extensive knowledge about a particular language, such as Nominator (Choi et al., 1997), still typically use large data files containing lists of names, exceptions, personal and organizational identifiers.",
            "8": "Our aim has been to build a maximally languageindependent system for both named-entity identification and classification, using minimal information about the source language.",
            "9": "The applicability of AI-style algorithms and supervised methods is limited in the multilingual case because of the cost of knowledge databases and manually annotated corpora.",
            "10": "Therefore, a much more suitable approach is to consider an EM-style bootstrapping algorithm.",
            "11": "In terms of world knowledge, the simplest and most relevant resource for this task is a database of known names.",
            "12": "For each entity class to be recognized and tagged, it is assumed that the user can provide a short list (order of one hundred) of unambiguous examples (seeds).",
            "13": "Of course the more examples provided, the better the results, but what we try to prove is that even with minimal knowledge good results can be achieved.",
            "14": "Additionally some basic particularities of the language should be known: capitalization (if it exists and is relevant — some languages do not make use of capitalization; in others, such as German, the capitalization is not of great help), allowable word separators (if they exist), and a few frequent exceptions (like the pronoun \"I\" in English).",
            "15": "Although such information can be utilised if present, it is not required, and no other assumptions are made in the general model.",
            "16": "The algorithm relies on both word internal and contextual clues as relatively independent evidence sources that drive the bootstrapping algorithm.",
            "17": "The first category refers to the morphological structure of the word and makes use of the paradigm that for certain classes of entities some prefixes and suffixes are good indicators.",
            "18": "For example, knowing that \"Maria\", \"Marinela\" and \"Maricica\" are feminine first names in Romanian, the same classification may be a good guess for \"Mariana\", based on common prefix.",
            "19": "Suffixes are typically even more informative, for example \"-escu\" is an almost perfect indicator of a last name in Romanian, the same applies to \"-wski\" in Polish, \"-ovic\" and \"-ivic\" in SerboCroatian, \"-son\" in English etc.",
            "20": "Such morphological information is automatically learned during bootstrapping.",
            "21": "Contextual patterns (e.g.",
            "22": "\"Mr.\", \"in\" and \"mayor of\" in left context) are also clearly crucial to named entity identification and classification, especially for names that do not follow a typical morphological pattern for their word class, are of foreign origin or polysemous (for example, many places or institutions are named after persons, such as \"Washington\" or \"Madison\", or, in some cases, vice-versa: \"Ion Popescu Topolog\" is the name of a Romanian writer, who added to his name the name of the river \"Topolog\").",
            "23": "Clearly, in many cases, the context for only one occurrence of a new word and its morphological information is not enough to make a decision.",
            "24": "But, as noted in Katz (1996), a newly introduced entity will be repeated, \"if not for breaking the monotonous effect of pronoun use, then for emphasis and clarity\".",
            "25": "Moreover, he claims that the number of instances of the new entity is not associated with the document length but with the importance of the entity with regard to the subject/discourse.",
            "26": "We will use this property in conjunction with the one sense per discourse tendency noted by Gale, Church and Yarowsky (1992b), who showed that words strongly tend to exhibit only one sense in a document/discourse.",
            "27": "By gathering contextual information about the entity from each of its occurrences in the text and using morphological clues as well, we expect to classify entities more effectively than if they are considered in isolation, especially those that are very important with regard to the subject.",
            "28": "When analyzing large texts, a segmentation phase should be considered, so that all the instances of a name in a segment have a high probability of belonging to the same class, and thus the contextual information for all instances within a segment can be used jointly when making a decision.",
            "29": "Since the precision of the segmentation is not critical, a language independent segmentation system like the one presented by Amithay, Richmond and Smith (1997) is adequately reliable for this task.",
            "30": "There are two basic alternatives for handling a text.",
            "31": "The first one is to tokenize it and classify the individual tokens or group of tokens.",
            "32": "This alternative works for languages that use word separators (such as spaces or punctuation), where a relatively simple set of separator patterns can adequately tokenize the text.",
            "33": "The second alternative is to classify entities simply with respect to a given starting and ending character position, without knowing the word boundaries, but just the probability (that can be learned automatically) of a boundary given the neighboring contexts.",
            "34": "This second alternative works for languages like Chinese, where no separators between the words are typically used.",
            "35": "Since for the first class of languages we can define a priori probabilities for boundaries that will match the actual separators, this second approach represents a generalization of the one using tokenized text.",
            "36": "However, the first method, in which the text is tokenized, presents the advantage that statistics for both tokens and types can be kept and, as the results show, the statistics for types seem to be more reliable than those for tokens.",
            "37": "Using the second method, there is no single definition of \"type\", given that there are multiple possible boundaries for each token instance, but there are ways to gather statistics, such as considering what we may call \"probable types\" according to the boundary probabilities or keeping statistics on sistrings (semi-infinite strings).",
            "38": "Some other advantages and disadvantages of the two methods will be discussed below."
          }
        },
        {
          "section": "method",
          "title": "2 The Basic Model",
          "sentences": {
            "39": "Before describing the algorithm, we will present a brief overview of some of its goals: Three important concepts are used in our model: 2.1 Trie structures are used for both morphological and contextual information Tries provide an effective, efficient and flexible data structure for storing both contextual and morphological patterns and statistics.",
            "40": "First, they are very compact representations.",
            "41": "Second, they support a natural hierarchical smoothing procedure for distributional class statistics.",
            "42": "We consider characterbased tries, in which each node contains a probability distribution (when working with tokenized text, two distributions are considered in each node, one for tokens and one for types).",
            "43": "The distribution stored at each node contain the probability of each name class given the history ending at that node.",
            "44": "Each distribution also has two standard classes, named \"questionable\" (unassigned probability mass in terms of entity classes, to be motivated below) and \"non-entity\".",
            "45": "To simplify the notations, we will refer to a start and end point bounded portion of text being analyzed (in order to determine if it represents a named entity or not) as a token.",
            "46": "Two tries are used for context (left and right) and two for internal morphological patterns of tokens.",
            "47": "Figure 1 shows an example of a morphological prefix trie, which stores the characters of tokens from left to right from given starting points (with optional word boundaries indicated by \"#\").",
            "48": "Suffix tries (typically more informative) have equivalent structure but reversed direction.",
            "49": "The left and right context tries have the same structure as well, but the list of links refers now to the tokens which have the particular context represented by the path from the root to the current node.",
            "50": "For right context, the letters are introduced in the trie in normal order, for left context they are considered in the reversed order (in our example, \"Anda\" has as left context \"dna#xela#\").",
            "51": "Similarly, nodes of the context tries contain links to the tokens that occurred in the particular contexts defined by the paths.",
            "52": "Two bipartite graph structures are created in this way by these links.",
            "53": "For reasons that will be explained later, raw counts are kept for the distributions.",
            "54": "The probability of a token/context as being in or indicating a class is computed along the whole path from the root to the terminal node of the token/context.",
            "55": "In this way, effective smoothing is realized for rare tokens or contexts.",
            "56": "Considering a token/context formed from characwhere Ai E [0,1] and E Ai = 1 It is reasonable to expect that smaller lambdas should correspond to smaller indices, or even that Ai &lt; A2 &lt; &lt; An.",
            "57": "In order to keep the number of parameters low, we used the following model: where a,13 E (0, 1), i3 having a small value The symbol F is used instead of P since we have raw distributions (frequencies) and a normalization step is needed to compute the final probability distribution.",
            "58": "A simpler model can use just one parameter (setting /3 = an), but this has limited flexibility in optimizing the hierarchical inheritance - the probability of a class given the first letter is often not very informative for some languages (such as English or Romanian) or, by contrast, may be extremely important for others (e.g.",
            "59": "Japanese). The basic concept of this bootstrapping procedure is to iteratively leverage relatively independent sources of information.",
            "60": "Beginning with some seed names for each class, the algorithm learns contextual patterns that are indicative for those classes and then iteratively learns new class members and word-internal morphological clues.",
            "61": "Through this cycle, probability distributions for class given token, prefix/suffix or context are incrementally refined.",
            "62": "More details are given when describing stage 2 of the algorithm.",
            "63": "opposed to the classical maximum entropy principle When faced with a highly skewed observed class distribution for which there is little confidence due to small sample size, a typical response to this uncertainty in statistical machine learning systems is to backoff or smooth to the more general class distribution, which is typically more uniform.",
            "64": "Unfortunately, this representation is difficult to distinguish from a conditional distribution based on a very large sample (and hence estimated with confidence) that just happens to have a similar fairly uniform true distribution.",
            "65": "One would like a representation that does not obscure this distinction, and represents the uncertainty of the distribution separately.",
            "66": "We resolve this problem while retaining a single probability distribution over classes by adding a separate \"questionable\" (or unassigned) cell that reflects the uncertainty of the distribution.",
            "67": "Probability mass continues to be distributed among the remaining class cells proportional to the observed distribution in the &apos;data, but with a total sum (&lt; 1) that reflects the confidence in the distribution and is equal to 1— P(qiiestionable).",
            "68": "This approach has the advantage of explicitly representing the uncertainty in a given class distribution, facilitating the further development of an interactive system, while retaining a single probability distribution that simplifies trie architecture and model combination.",
            "69": "Incremental learning essentially becomes the process of gradually shifting probability mass from questionable/uncertain to one of the primary categories."
          }
        },
        {
          "section": "method",
          "title": "3 The Algorithm",
          "sentences": {
            "70": "The algorithm can be divided into five stages, which are summarized below.",
            "71": "Stage 0: build the initial training list of class representatives Stage 1: read the text and build the left and right morphological and context tries Stage 2: introduce the training information in the tries and re-estimate the distributions by bootstrapping Stage 3: identify and classify the named entities in the text using competing classifiers Stage 4: update the entity and context training space, using the new extracted information Stage 0: This stage is performed once for each language/task and consists of defining the classes and filling in the initial class seed data with examples provided by the user.",
            "72": "The list of class training names should be as unambiguous as possible and (ideally) also relatively common.",
            "73": "It is also necessary to have a relatively large unannotated text for bootstrapping the contextual models and classifying new named entities.",
            "74": "Examples of such training seeds and text for Romanian language are given in Tables 1 and 21.",
            "75": "For the primary experiments reported in this paper, we have studied a relatively difficult 3-way named entity partition between First (given) names, Last (family) names and Place &apos;names.",
            "76": "The first two tend to be relatively hard to distinguish in most languages.",
            "77": "A 1The text refers to the mayor of a small town of Alba county, who was so drunk while officiating at a wedding that he shook the bride&apos;s hand and kissed the groom.",
            "78": "simpler person/place-based distinction more comparable to the MUC-6 EMAMEX task is evaluated in Table 3(d).",
            "79": "Target Evaluation Text (labels not used for training) Primarul comunei &lt;place> Rosia Montana &lt;/place> judetul &lt;place> Alba &lt;/place> &lt;fname> David &lt;/fname> &lt;lname> Botar &lt;/lname> a intrat in legenda datorita unor intimplari de-a dreptul penibile, relatate in \"Evenimentul zilei\".",
            "80": "Practic, primul gospodar al celei mai bogate comune in aur din &lt;place> Muntii Apuseni &lt;/place> este mai tot timpul beat-crita, drept pentru care, la oficierea unei casatorii, a sarutat mina mirelui, a strins mina miresei si a intocmit certificat de deces in locul celui de casatorie.",
            "81": "Recent, &lt;fname> Andrei &lt;/fname> &lt;lname> Paunescu &lt;/lname> fiul poetului, a intentionat sa achizitioneze gospodaria unei bucurestence care se stabilise de o vreme in &lt;place> Rosia Montana &lt;/place> La primarie Ins.",
            "82": ", turmentatul primar 1-a trimis pe fiul lui &lt;fname> Adrian &lt;/fname> &lt;lname> Paunescu &lt;/lname> sa-i cumpere ceva de baut, pentru a se putea concentra indeajuns asupra hirtiilor tranzactiei imobiliare.",
            "83": "There are two ways to start this stage, either by tokenizing the text or considering it in raw form.",
            "84": "When tokenization is used, each token is inserted in the two morphological tries: one that keeps the letters of the tokens in the normal (prefix) order, another that keeps the letter in the reverse (suffix) order.",
            "85": "For each letter on the path, the raw distributions are changed by adding the a priori probability of the token belonging to each class (language dependent information may be used here).",
            "86": "For• example, in the case of Indo-European languages, if the token starts with an upper-case letter, we add 1 full count (all probability mass) to the \"questionable\" sum, as this entity is initially fully ambiguous.",
            "87": "If the token starts with lower-case (and hence is an unlikely name) in this case we add the bulk of the probability mass 5 (e.g.d ? 0.9) to \"non-entity\" and the remainder (1-5) to \"questionable\" (otherwise unassigned).",
            "88": "Other language-specific orthographic clues could potentially affect this initial probability mass assignment.",
            "89": "When no tokenization is applied, we have to consider possible starting and ending points.",
            "90": "Therefore, the strings (which, for simplicity, we will refer as well as tokens) introduced in the prefix morphological trie and the ones introduced in the suffix trie may differ.",
            "91": "The left context of each token is introduced, letters in reverse order, in the left context trie, with pointers to the token in the morphlogical prefix trie; the right context of each token is introduced, in normal order, in the right context trie, keeping pointers to the token in the suffix trie.",
            "92": "The distributions along the paths are modified according to the a priori distribution of the targeted token.",
            "93": "This stage is the core bootstrapping phase of the algorithm.",
            "94": "In essence, as contextual models become better estimated, they identify additional named entities with increasing confidence, allowing reestimation and improvement of the internal morphological models.",
            "95": "The additional training data that this yields allows the contextual models to be augmented and reestimated, and the cycle continues until convergence.",
            "96": "One approach to this bootstrapping process is to use a standard continuous EM (ExpectationMaximization) family of algorithms (Baum, 1972; Dempster et al., 1977).",
            "97": "The proposed approach outlined below is a discrete variant that is much less computationally intensive, and has the advantage of distinguishing between unknown probability distributions and those which are simply evenly distributed.",
            "98": "The approach is conservative in that it only utilizes the class estimations for newly classified data in the retraining process if the class probability passes a confidence threshold, as defined below.",
            "99": "The concept of confidence threshold can be captured through the following definitions of dominant and semi-dominant.",
            "100": "Let us consider a discrete finite probability distribution P = (pi,..-,pn)• We say that P has a dominant if there is an i in {1...n} such that pi > 0.5, or in other words if We say that P has an a-semi-dominant with respect to an event k, where c> 1, if it does not have k as dominant and there exist i in {1...n} such that A few comments about these definitions are necessary: it can be easily observed that not every distribution has a dominant, even though it has a maximum value.",
            "101": "The second definition, of a-semidominant, makes sense if we consider a particular event k that is not relevant (or the result cannot be measured).",
            "102": "By removing this event and normalizing the rest of the values, we obtain a new distribution (of size n-1) having an a-dominant.",
            "103": "The core of stage 2 is the bootstrapping procedure.",
            "104": "The known names (either from the original training list or otherwise learned data) are inserted sequentially into the morphological tries, modifying the probability distributions of the nodes on the paths accordingly (the data structure is illustrated in Figures 1 and 2). If the new distribution in one of the nodes on the path of a known token gains a dominant (for example \"place\") then the effect of this change is propagated by reestimating other node distributions given this change.",
            "105": "Each distribution on the context paths in which that token occurred in the text is modified, by subtracting from the \"questionable\" mass a quantity proportional to the number of times the respective token was found in that context and adding it to the dominant-position (e.g.",
            "106": "\"place\") mass.",
            "107": "For the newly obtained distributions that gained a dominant (in our example \"place\") in the context trie, the bootstrapping procedure is called for all tokens that occurred in that context, and so on, recursively.",
            "108": "Here it is very important that we consider raw distributions and not normalize them.",
            "109": "For example, if word \"Mariana\" occurs x times with the right context \"merge\" (meaning \"goes\") and the distribution for \"niariana#\" has now been identified with the dominant \"first name\", then x units from the \"questionable\" mass can be moved to \"first name\" mass along the path of \"merge#\" in the right context trie.",
            "110": "If semi-dominants are used instead of dominants then we have to account for the fact that the semi-dominants may change over time, so the probability mass must be moved either from \"questionable\" position or previous semi-dominant position, if a semi-dominant state has been reached before.",
            "111": "It may be easily observed that stage 2 has a sequential characteristic, because the updating is done after reading each name incrementally.",
            "112": "When using dominants the order does not affect the process, because of the fact that once a dominant state is reached, it cannot change to another dominant state in the future (probability mass is moved only from \"questionable\").",
            "113": "In the case of semi-dominants, the data ordering in the training file does influence the learning procedure.",
            "114": "The more conservative strategy of using dominants rather then semi-dominants has, on the other hand, the disadvantage of cancelling or postponing the utilisation of many words.",
            "115": "For example, if both \"questionable\" and \"first name\" have 49% of the mass then subsequent reestimation iterations are not initiated for this data, even though the alternative name classes are very unlikely.",
            "116": "Considering those advantages and disadvantages, we used the less conservative semi-dominant approach as the default model.",
            "117": "In this stage the text is re-analysed sequentially, and for each token (given a start-end point pair) a decision is made.",
            "118": "Here the bipartite structure of the two pairs of tries has a central role: during stage 2, the left context and prefix tries interact with each other and so do the right context and suffix tries, but there&apos;s no interference between the two pairs during the bootstrapping stage.",
            "119": "Therefore, for each instance of a token in the text, four classifiers are available, a different one given by each trie.",
            "120": "The decision with regard to the presence of an entity and its classification is made by combining them.",
            "121": "Comparative trials indicate that higher performance is achieved by initially having the classifiers vote.",
            "122": "Results indicate that the most accurate classifications are obtained from the two independently bootstrapped morphological tries (they incorporate the morphological information about the token to be classified, and, during the bootstrapping, they also incorporate information from all the contexts in which the token occurred).",
            "123": "If the two agree (they have semi-dominants and they are the same) then the corresponding class is returned.",
            "124": "Otherwise, agreement is tested between other paired independent classifiers (in order of empirically measured reliability).",
            "125": "If no agreement is found, then a simple linear combination of all four is considered for the decision.",
            "126": "This approach yields 6% higher F-measure than the simple interpolation of classifiers for the default parameters.",
            "127": "Stage 4: The newly classified tokens and contexts are saved for future use as potential seed data in subsequent named-entity classification on new texts ."
          }
        },
        {
          "section": "method",
          "title": "4 Results",
          "sentences": {
            "128": "The basic measures for evaluation of this work are precision and recall.",
            "129": "Precision (P) represents the percentage of the entities that the system recognized which are actually correct.",
            "130": "Recall (R) represents the percentage of the correct named entities in the text that the system identified.",
            "131": "Both measures are incorporated in the F-measure, F = 2PRAP + R).",
            "132": "It would be inappropriate to compare the results of a language independent system with the ones designed for only one language.",
            "133": "As Day and Palmer (1997) observed, \"the fact that existing systems perform extremely well on mixed-case English newswire corpora is certainly related to the years of research and organized evaluations on this specific task in this language.",
            "134": "It is not clear what resources are required to adapt systems to new languages\".",
            "135": "It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).",
            "136": "Our experiments on Romanian text were consistent with this figure.",
            "137": "In order to obtain a baseline performance for this method we considered the performance of a system that tags only the examples found in one of the the original training wordlists.",
            "138": "We consider this to be a plausible lower bound measure if the training words have not been selected from the test text.",
            "139": "Day and Palmer (1997) showed that a baseline Fmeasure score for the ENAMEX task varies from 21.2% for English to 73.2% for Chinese.",
            "140": "It is important to mention that, when they computed these figures, they trained their language independent system on large annotated corpora (e.g.",
            "141": "the Wall Street Journal for English).",
            "142": "The fact that the precision obtained by the baseline approach is not 100% indicates that the seed training names for each class are not completely unambiguous, and that a certain degree of ambiguity is generally unavoidable (in this case, mainly because of the interference between first names and last names).",
            "143": "Another significant performance measure is forced classification accuracy, where the entities have been previously identified in the text and the only task is selecting their name class.",
            "144": "To obtain baseline performance for this measure, we considered a system that uses the original training word labels if there is an exact match, with all other entities labeled with a default \"last name\" tag, the most common class in all languages studied.",
            "145": "The baseline accuracy was measured at 61.18% for Romanian.",
            "146": "System accuracies range from 77.12% to 91.76% on this same data.",
            "147": "The results shown in Table 3 were obtained for a Romanian text having 12320 words, from which 438 were entities, using a training seed set of 300 names (115 first names, 125 last names, and 60 city/country names).",
            "148": "The baseline measures and default system (a) are as described above.",
            "149": "In configuration (b), the based parameters of the system have been optimized for Romanian, using greedy search on an independent development test (devtest) set, yielding a slight increase in F-measure.",
            "150": "Configuration (c) used the default parameters, but the more conservative \"dominant\" criterion was utilized, clearly favoring precision at the expense of recall.",
            "151": "Configuration (d), which is relevant for the ENAMEX task, represents the performance of the system when classes \"first name\" and \"last name\" are combined into \"person\" (whenever two or more such entities are adjacent, we consider the whole group as a \"person\" entity).",
            "152": "Configuration (e) shows contrastive performance when using standard continuous EM smoothing on the same data and data structures.",
            "153": "Table 4 shows system performance for 5 fairly diverse languages: Romanian, English, Greek, Turkish and Hindi.",
            "154": "The initial 4 rows provide some basic details on the training data available for each language.",
            "155": "Note that when annotators were generating the lists of 150-300 seed words, they had access to a development test from which to extract samples, but they were not constrained to this text and could add additional ones from memory.",
            "156": "Furthermore, it was quite unpredictable how many contexts would actually be found for a given word in the development texts, as some appeared several times and many did not appear at all.",
            "157": "Thus the total number of contextual matches for the seed words was quite variable, from 113-249, and difficult to control.",
            "158": "It is also the case that not all additional contexts bring comparable new benefit, as many secondary instances of the same word in a given related document collection tend to have similar or identical surrounding contexts to the first instance (e.g.",
            "159": "\"Mayor of XXX\" or \"XXX said\"), so in general it is quite difficult to control the actual training information content just by the number of raw seed word types that are annotated.",
            "160": "For each of these languages, 5 levels of information sources are evaluated.",
            "161": "The baseline case is as previously described for Table 3.",
            "162": "The context-only case restricts system training to the two (left and right) contextual tries, ignoring the prefix/suffix morphological information.",
            "163": "The morphology only case, in contrast, restricts the system to only the two (prefix and suffix) morphological models.",
            "164": "These can be estimated from the 3 training wordlists (150-300 words total), but without an independent source of information (e.g.",
            "165": "context) via which bootstrapping can iterate, there is no available path by which these models can learn the behaviour of previously unseen affixes and conquer new territory.",
            "166": "Thus the model is entirely static on just the initial training data.",
            "167": "For the same reasons, the context only model is also static.",
            "168": "In this case there is a possible bootstrapping path using alternating left and right context to expand coverage to new contexts, but this tends to be not robust and was not pursued.",
            "169": "Interestingly, recall for morphology only is typically much higher than in the context only case.",
            "170": "The reason for this is that the morphology models are full hierarchically smoothed character tries rather than word token tries, and hence have much denser initial statistics for small training data sets, proving greater partial matching potential for previously unseen words.",
            "171": "In an effort to test the contribution of the full iterative boostrapping, the \"context and morphology only\" results, are based on the combination of all 4 tries, but Without any bootstrapping.",
            "172": "Thus they are trained exclusively on the 150-300 training examples.",
            "173": "Performance for the combined sources is in all cases greater than for the morphology or context source used alone.",
            "174": "Furthermore, the full iterative bootstrapping clearly yields substantial improvement over the static models, almost exclusively in the form of increased recall (and its corresponding boost the the F-measure).",
            "175": "Cross-language analysis yields further insight.",
            "176": "First, recall is much higher for the 4 languages in which case is explicitly marked and is a clue for named entity identification (Romanian, English, Greek and Turkish) than for a language like Hindi, where there are no case distinctions and hence any word could potentially be a named entity.",
            "177": "A language such as German would be roughly in the middle, where lower-case words have low probability as named entities, but capitalized words are highly ambiguous between common and proper nouns.",
            "178": "Because approximately 96% of words in the Hindi text are not named entities, without additional orthographic clues the prior probability for \"non-entity\" is so strong that the morphological or contextual evidence in favor of one of the named entity classes must be very compelling to overcome this bias.",
            "179": "With only 50 training words per context this is difficult, and in the face of such strong odds against any of the named entity classes the conservative nature of the learning algorithm only braves an entity label (correctly) for 38% more words than the baseline model.",
            "180": "In contrast, its performance on entity classification rather than identification, measured by forced choice accuracy in labelling the given entities, is comparable to all the other languages, with 79% accuracy relative to the 62% baseline.2 Figure 3 demonstrates that the performance of the algorithm is highly sensitive to the size of the training data.",
            "181": "Based on Romanian, the first graph shows that as the size of the raw text for bootstrapping increases, F-measure performance increases roughly logrithmically, due almost exclusively to increases in precision.",
            "182": "(Approximately the same number of unique entities are being identified, but due to the increased number of examples of each, their classification is more accurate).",
            "183": "This is a very encouraging trend, as the web and other online sources provides virtually unlimited raw text in most major languages, and substantial on-line text for virtually all languages.",
            "184": "So extrapolating far beyond the 10K word level is relatively low cost and very feasible.",
            "185": "The second graph shows that F-measure performance also increases roughly logrithmically with the total length of the seed wordlists in the range 40300.",
            "186": "This increase is due entirely to improved recall, which doubles over this small range.",
            "187": "This trend suggests that there is considerable benefit to be gained by additional human annotation, or seed wordlist acquisition from existing online lexicons.",
            "188": "However, relative to case of raw text acquisition, such additional annotations tend to be much costlier, and there is a clear cost-benefit tradeoff to further investment in annotation.",
            "189": "In summary, however, these evaluation results are satisfying in that they (a) show clear and consistent trends across several diverse languages, (b) show clear trends for improvement as training resources grow, and (c) show that comparable (and robust) classification results can be achieved on this diversity of languages."
          }
        },
        {
          "section": "method",
          "title": "5 Future work",
          "sentences": {
            "190": "For future work, natural next steps include incorporating a language independent word segmentation phase like the one proposed by Amitay, Richmond and Smith (1997), to improve the performance on large texts.",
            "191": "Different statistics can be pre-computed for different languages and language families and stored in external files.",
            "192": "For example, the a priori probability of a named entity given the set of characteristics of its representation in the text, such as position, capitalization, and relative position of other entities (e.g.: first name followed by last name).",
            "193": "A further step is the implementation of a supervised active learning system based on the present algorithm, in which the most relevant words for future disambiguation is presented to the user to be classified and the feedback used for bootstrapping.",
            "194": "The selection of candidate examples for tagging would be based on both the unassigned probability mass and the frequency of occurrence.",
            "195": "Active learning strategies (Lewis and Gale, 1994) are a natural path for efficiently selecting contexts for human annotation."
          }
        },
        {
          "section": "conclusions",
          "title": "6 Conclusion",
          "sentences": {
            "196": "This paper has presented an algorithm for the minimally supervised learning of named entity recognizers given short name lists as seed data (typically 40100 example words per entity class).",
            "197": "The algorithm uses hierarchically smoothed trie structures for modeling morphological and contextual probabilities effectively in a language independent framework, overcoming the need for fixed token boundaries or history lengths.",
            "198": "The combination of relatively independent morphological and contextual evidence sources in an iterative bootstrapping framework converges upon a successful named entity recognizer, achieving a competitive 70.5%-75.4% F-measure (measuring both named entity identification and classification) when applied to Romanian text.",
            "199": "Fixed k-way classification accuracy on given entities ranges between 73%-79% on 5 diverse languages for a difficult firstname/lastname/place partition, and approaches 92% accuracy for the simpler person/place discrimination.",
            "200": "These results were achieved using only unannotated training texts, with absolutely no required language-specific information, tokenizers or other tools, and requiring no more than 15 minutes total human effort in training (for short wordlist creation) The observed robust and consistent performance and very rapid, low cost rampup across 5 quite different languages shows the potential for further successful and diverse applications of this work to new languages and domains."
          }
        },
        {
          "section": "acknowledgments",
          "title": "7 Acknowledgements",
          "sentences": {
            "201": "The authors would like to thank Eric Brill, Radu Florian, Shanka,r Kumar, Murat Saraclar, Dimitra Vergyri and Jun Wu for both their feedback on this work and their help in annotating the named-entity data for the languages studied."
          }
        }
      ],
      "doc_id": 18,
      "actual_doc_id": "W99-0612-parscit-section.xml"
    },
    {
      "content": [
        {
          "section": "abstract",
          "title": "1.0 ABSTRACT,",
          "sentences": {
            "0": "This paper describes a computational pragmatic model which is geared towards providing helpful answers to modal and hypothetical questions.",
            "1": "The work brings together elements from formal. semantic theories on modality and question answering, defines a wider, pragmatically flavoured, notion of answerhood based on non-monotonic inference and develops a notion of context, within which aspects of more cognitively oriented theories, such as Relevance Theory, can be accommodated.",
            "2": "The model has been implemented.",
            "3": "The research was funded by ESRC grant number R000231279 ."
          }
        },
        {
          "section": "keywords",
          "title": "Keywords:Semantics, Pragmatics2.0 INTRODUCTION.",
          "sentences": {
            "4": "Answers people give to questions have two basic properties: they may vary depending on the situation a question is asked in, and, especially if the answer is negative, they aim to be \"helpful\".",
            "5": "The context-sensistivity of answering seems obvious and in no need of further demonstration.",
            "6": "What precisely constitutes \"helpfulness\" is harder to pin down.",
            "7": "Modal and hypothetical questions offer an interesting area for investigating \"helpfulness\".",
            "8": "Suppose A and B are going to a party and are discussing how they might travel.",
            "9": "Suppose A asks B Can you drive?",
            "10": "B is correct but perverse to respond Yes, if he knows how to drive, has a valid licence but has no car, or if he has a car but he has lent it to someone.",
            "11": "A more helpful answer might be No. because I haven't got a car.",
            "12": "Note here that there is a range of \"correct\" answers, some of which are Yes: for instance Yes.",
            "13": "I know how to drive, Yes.",
            "14": "I have a licence, even Yes, I have a car, and some of which are No, as in No I haven't got a car tonight.",
            "15": "The range includes No. but! can ask to borrow my wife's car, or even Yes, if I can get my car back which establishes a link with hypothetical questions.",
            "16": "Note also that, for each of these \"correct\" answers, we can imagine contexts in which they would be \"helpful\".",
            "17": "Little is known about the nature of questions and their relationship to appropriate answers, or about how such answers can be computed given some information about what the answerer knows.",
            "18": "Some theories, mainly emerging from the Montague tradition (see Groenendijk and Stockhof [1984J), attempt to define \"semantic\" answerhood (see section 2.2), but fall short when tackling the pragmatic aspect of helpful answers.",
            "19": "Other theories [Sperber & Wilson 19861 offer interesting pragmatic insights but their formulation does not allow for a straightforward implementation.",
            "20": "Furthermore, the problem of answering modal and hypothetical questions is a compounded one which touches on a host of issues including quantification, intensionality, partiality, belief revision, propositional attitudes, etc.",
            "21": "Our research aimed to draw up a formally specified and computationally feasible pragmatic theory which could accommodate formal semantic views on answerhood as well as intuitive insights into \"helpfulness\" and its dependence on context (such as offered by Relevance Theory [Sperber and Wilson 19861).",
            "22": "Furthermore, the model is rigourously constrained as it must be tested by an implementation over some knowledge base representing what an agent knows.",
            "23": "This paper is intended as an overview of the computational model.",
            "24": "As such it does not provide an in-depth account of all aspoets of the investigation; in particular, it does not attempt to give a formal account of a basic theory of pragmatics, which is available elsewhere [Ball et al 19901 Rather, we sketch the background to the problems involved in providing helpful answers to modal and hypothetical questions as a review of the relevant literature and its perceived shortcomings.",
            "25": "We will then proceed to outline the intuitions behind our approach to a model of pragmatics and its implementation, and explain how it accommodates helpful answers to modal and hypothetical questions.",
            "26": "An example is presented."
          }
        },
        {
          "section": "method",
          "title": "3.0 HELPFULNESS, MODALS AND QUESTIONS",
          "sentences": {
            "27": "Though the problem of helpfully answering modal questions touches on many issues, four particular points need to be addressed.",
            "28": "When looking at the example set out in the introduction, the question arises whether the range of \"correct\" answers to some extent corresponds to ambiguity (ability/possibility) residing in the modal can.",
            "29": "Indeed, a large proportion of the literature on :nodality concerns the view that modals are polysemous, depending on the kind and degree of modality they express (epistemic, deantic, etc).",
            "30": "[Palmer 1979, 1986; Quirk et al. 19851.",
            "31": "Usually, attempts are made to identify modal \"primitives\" (ability, permission, etc) and to analyse modal constructs as ambiguous over several \"literal\" meanings involving these primitives.",
            "32": "Invariably, polysemic reductionist approaches to modal constructs run into problems: given any classification of core types of modality, it is often impossible to determine which reading is involved in any particular example [Coates 1983 versus Walton 1988].",
            "33": "Kratzer [1977J takes another view.",
            "34": "She presents a unified anal, ysis of modality which includes the treatment of conditionals (and hence hypotheticals).",
            "35": "Modals are unambiguous and modal constructs are analysed as tri-partite structures Isee also Partee 1988, Heim 19821, comprising a modal operator, a conversational background, and a proposition.",
            "36": "For example, in [Kratzer 19771 the modal must in the following sentences: is traditionally analysed as (a) 'deontic' must indicating duty, (b) 'epistemic' must referring to a piece of knowledge or information, (c) 'dispositional' must, referring to people's dispositions (e.g.",
            "37": "they cannot help sneezing), and (d) 'preferential' must referring to.",
            "38": "preferences and wishes.",
            "39": "Kratzer points out that classifications of modals drawn in the polysemy paradigms never adequately cover the data and that new examples are easily found to demonstrate the need for ever more refined categories of modal meaning.",
            "40": "Kratzer wishes to propose a treatment that brings out the common factor in all uses of must (and of other modals) and suggests that the burden of differentiation is to be placed on a variation M The modal is an operator which takes a context and a proposition.",
            "41": "The truth conditions for must, interpreted as necessity, dictate that the modal construct is true if the proposition (the second argument) logically follows from the context (the first argument).",
            "42": "A similar approach to can (possibility) unpacks its truth conditions as true if the context (the first argument) is logically compatible:(i.e.",
            "43": "does not induce a contradiction) with the proposition (the second argument).",
            "44": "Kratzer works within the classical possible world tradition.",
            "45": "Conversational backgrounds, modelled as sets of propositions, are usually implicit and linked to the utterance situation, though it is not clear by what mechanism.",
            "46": "Kratzer [19831 proceeds to distinguish between different kinds of conversational backgrounds, depending on the information they contain.",
            "47": "She does however experience difficulties when trying to identify different context classes.",
            "48": "Indeed, it is as difficult- to isolate different conversational backgrounds as it is to pinpoint the various meanings a modal might have.",
            "49": "It is also necessary to present a coherent perspective on questions and answers.",
            "50": "Groenendijk and Stockhrif 119841 compile an overview of various treatments of questions that populate the field and investigate desiderata for a semantic theory.",
            "51": "They argue.",
            "52": "that interrogatives are entitled to a meaning of their own (and should not be viewed as, say, hidden imperatives) but that their treatment must show some equivalence with that of indirect questions.",
            "53": "The meaning of a question is to be related inextricably with its answerhood conditions.",
            "54": "Groenendijk and Stockhof work in the possible world tradition and they cast the interpretation of amuestion as'a function which, for every index, returns its true answer.",
            "55": "Consider the following example.",
            "56": "The semantics given to a question Does Peter walk? is a partition of the set of possible worlds into two: those worlds where Peter walks, and those *odds where Peter does not walk.",
            "57": "Both Peter walks and Peter does not walk are possible semantic answers to the question.",
            "58": "Each possible world belongs to one or the other of these partitions, so each possible world offers only one true answer to the question.",
            "59": "This analysis caters for entailment between questions (question Q entails question R if all true answers to Q are also true answers to R) and thus explains entailment between coordinated questions.",
            "60": "Groenendijk and Stockhof elaborate the basic treatment of yes/no questions; wh-questions are reduced to this basic type.",
            "61": "They also provide an interpretation for constituent answers.",
            "62": "They assume that modal questions will be analysed at some other pragmatic level.",
            "63": "The work described constitutes the most extensive treatment of the semantics of questions and answers to date.",
            "64": "However, in our view, it cannot be directly incorporated in a pragmatic model, for two reasons.",
            "65": "First of all, the semantic model assumes completeness of information, and complete mutual awareness of speakers' belief states (but then, why ask questions of one another?).",
            "66": "They do attempt to build, from this, an account of how to reason with partial knowledge but, as they work in a traditional extensional framework, this results in clashes with the semantic theory.",
            "67": "(In short, what a person knows is a set of possible worlds, namely all those possible worlds that are consistent with his/her beliefs.",
            "68": "The semantics of questions is given as a partition over all possible worlds.",
            "69": "In an extensional framework - where intensions are derived from extensions - this means that if a person entertains partial beliefs, he/she cannot know the meaning of a question).",
            "70": "Secondly, there may be more than one true answer to a question, and all should be captured by Groenendijk and Stockhof's theory.",
            "71": "But how are these answers defined, even computed, from the question?",
            "72": "And, as illustrated in the example given in the introduction, even if we know how to generate such answers, how do we define a helpful answer?",
            "73": "It is not easy to give a definition of a \"helpful\" answer off the cuff.",
            "74": "Formal semantic theories have little to say on this issue, though some cognitively oriented frameworks have developed useful views.",
            "75": "Relevance Theory [Sperber and Wilson 19861 has given an account of context-sensitivity in communication.",
            "76": "It postulates that when people understand, they attempt to maximise relevance i.e. they pick the context against which the relevance of an utterance is greatest.",
            "77": "Relevance, thus, is quantifiable and defined by means of extent conditions: an assumption is relevant in a context to the extent that its effects in this context are large and the effort to process it is small.",
            "78": "It should be clear from the onset that the specification of Relevance Theory is, not precise enough to be implemented as it stands.",
            "79": "There are, however, three principles which are interesting for our purpose.",
            "80": "(i) The most relevant context for interpreting a question is that a Yes-answer is desired.",
            "81": "This helps towards explaining why helpful answers are given at all, and why they occur typically with negative answers.",
            "82": "(ii) The selection of relevant contexts is embodied in the human cognitive machinery and ensures that an utterance receives only one interpretation (and not many from which a particular one is selected).",
            "83": "Indeed, as shown in the introduction, there may be more than one true answer to a question but only one appropriate one, which must be characterized.",
            "84": "(iii) The theory specifies that all contextual effects are explained against the background of assumptions which a person may hold and postulates mechanisms by means of which relevant contexts can be pinned down starting from situational information and the utterance itself.",
            "85": "The necessity to give a more precise definition of context becomes obvious from the previous sections.",
            "86": "Questions can only be answered in context, modals seem to receive different interpretations according to varying contexts, and any cognitively appealing notion of \"helpfulness\" or \"relevance\" is stated in terms of contexts.",
            "87": "AU this ties in with current work in formal semantics which explores tri-partite structures (tying in context with propositional content of utterances) as a basic mechanism for semantic interpretation 'Heim 1982, Partee 19881.",
            "88": "However, though current formal semantic theory is steadily increasing the workload of context, its precise nature remains vague.",
            "89": "It is not enough to furnish formal semantic interpretations \"relative\" to some context: a satisfactory approach to a formal but cognitively attractive characterization of \"helpful\" answers seems to war.",
            "90": ", rant a closer look at the content of conversational backgrounds, their relation to the utterance and its situation, and an appreciation of whether they can be computed.",
            "91": "The insights offered by Relevance Theory may be compatible with formal (and computational) semantic theories, and offer a practical starting point when trying to pin down a fuller notion - 258 of context.",
            "92": "In order to investigate this, we need to define our intuitions in an implementable framework.",
            "93": "Please note that it is not our intention to attempt a formalisation or an implementation of Relevance Theory, but merely to define an experimental framework capable of handling contexts in order to derive helpful answers to modal and hypothetical questions, albeit exploiting insights from Relevance Theory if possible.",
            "94": "It is our intuition that, when people communicate, they know different things, or there would be no point in communicating.",
            "95": "Thus it seems that any realistic model of communication must allow for partiality in what agents know.",
            "96": "Since agents retain inferential capabilities, we assume their beliefs are consistent.",
            "97": "As a consequence,.",
            "98": "our model represents agents as partial, consistent sets of propositions.",
            "99": "The notion of proposition deployed is taken straight from Property Theory (Turner 1987, Chierchia et al 1989, see also Ramsay 19901, a weak first order theory with fine grained intensionality.",
            "100": "Questions are not themselves propositions; they are not associated with truth values.",
            "101": "They do, however, entertain a relationship with propositions.",
            "102": "In our view, a simple yes/no question embodies a proposition whose truth value is not, known to the agent asking the question.",
            "103": "An answer to the question is any proposition which, if added to the agent's beliefs, will force truth or falsity of the proposition embodied in the question.",
            "104": "This view on answerhood is much looser than the one adopted by Groenendijk and Stockhof in that it allows answers other than true semantic answers.",
            "105": "Indeed, any answer will do as long as it allows an agent to conclude to the true semantic answer.",
            "106": "Thus, a question Does Peter walk? may be answered by Peter sleeps in this framework (and not just by either of Peter walks or Peter does not walk) as long as that information allows the agent to conclude that Peter walks or that Peter does not iva/k.",
            "107": "This constrains the agent's reasoning capacity which must now deal with partial infonnation.",
            "108": "It also means that agents' beliefs must be subject to revision.",
            "109": "In order to reflect these intuitions in our theory, we extended the language of Property Theory with a predicate which holds of questions, and an operator which, given a proposition, will yield a question.",
            "110": "An axiomatisation governs conjunction of questions.",
            "111": "A relation of answerhood is defined which holds between a question and its answer (a proposition).",
            "112": "The behaviour of this relation is given through axiomatisation of a proof theory.",
            "113": "We adopt a view on modality parallel to Kratzer's: our working hypothesis states that modals are not ambiguous and that any difference in interpretation resides in contextual diversity.",
            "114": "We do not, however, try to classify contexts; a hopeless task which is no different to attempting to classify modal ambiguity.",
            "115": "Can and rnust correspond to the modal operators of possibility and necessity.",
            "116": "Modal constructs are analysed in terms of these operators, a context and a proposition.",
            "117": "A context is a collection of propositions, which is a colts istent subset of the agent's total beliefs.",
            "118": "Necessity is true if the negation of the proposition causes a contradiction in the context; possibility is true if the proposition can be accommodated within the context without giving rise to a contradiction (i.e.",
            "119": "the context can be updated with the proposition).",
            "120": "Questions, whether they are simple or modal, are equally analysed as tri-partite structures comprising an operator, a context and a proposition.",
            "121": "For simple yes/no questions the operator is the QuestionTruth predicate (which can be safely stated in Property Theory).",
            "122": "For modal questions, the operator is the Question counterpart of the appropriate modal operator.",
            "123": "As with Groenendijk and Stockhof, wh-questions are reduced to yes/no questions.",
            "124": "It should follow front the above that Groenendijk and Stockhof's results carry over into this model, as the notion of semantic answerhood is preserved (though in an extended framework).",
            "125": "Following ICratzer, conditionals are treated like modals but the context is updated with the antecedent.",
            "126": "We are not, however, treating counterfactuals at this stage (i.e.",
            "127": "we only treat cases where the context can be updated with the antecedent and where no contradictions occur as a result).",
            "128": "In defining \"helpfulness\", we take the view of Relevance Theory that a positive answer to the proposition embedded in the question is desirable.",
            "129": "As such, yekanswers become uninteresting as they are already maximally helpful.",
            "130": "No-answers, on the other hand, where the proposition cannot be accommodated by the context, can be helpful if they indicate why the proposition is incompatible with a state of affairs, or how the state of affairs might change so that it can be updated with the proposition.",
            "131": "In the theory, this information is available from the logic underpinning the answerhood relation relativised to a context.",
            "132": "However, this furnishes us with a semantics only.",
            "133": "To arrive at some view of how this may interact with pragmatics, the content of contexts must be fleshed out.",
            "134": "Intuition tells us that only one helpful answer is furnished per context.",
            "135": "Following Kratzer, and Relevance Theory, we assume that the burden of being helpful and relevant rests with the mechanism which defines the context for an utterance given a situation.",
            "136": "Many factors may contribute to this mechanism and it seems reasonable that knowledge of the physical circumstances (i.e.",
            "137": "speakers, time, location, etc).",
            "138": "should play a role.",
            "139": "The utterance itself must also contribute.",
            "140": "As the literature offers no detailed information on how to model the relationship between context and utterance, we have developed an implementation of a context machine which, initially, derived context from lexical information.",
            "141": "This implementation was changed and refined in order to attempt to determine experimentally what the requirements for a \"context machine\" may be."
          }
        },
        {
          "section": "method",
          "title": "5.0 TIIE IMPLEMENTATION",
          "sentences": {
            "142": "The implementation of the overall framework consists of a parser, a knowledge base, a context machine and a theorem prover.",
            "143": "The knowledge base, a consistent collection of propositions, is set up to represent the beliefs of an agent who is to answer questions.",
            "144": "For convenience of computation, the items in it are cast as sorted property-theoretic expressions (a sortal hierarchy can be achieved without sorting quantified variables - soiling and closing the world with respect to individuals merely has the effect of rendering the implementation of the first order language decidable).",
            "145": "Each knowledge base item is tagged with keys linking the information it contains with words in the lexicon.",
            "146": "The parser, a bi-directional chart parser [Steel and De Roeck 19871 augmented with feature structures, works from an essen/ tidily context free rule base where semantic translation rules are paired up with the syntactic statement.",
            "147": "The semantic representation delivered by the parser is an expression in Property Theory capturing the structural aspects of question's meaning.",
            "148": "This Property-Theoretic expression is passed to the context, machine.",
            "149": "It yields, from the Property-Theoretic expression, a tripartite structure comprising an operator, a context and a proposition derived from the question.",
            "150": "The role of the context machine is to extract from the knowledge base that information which is relevant to finding a helpful answer to the question.",
            "151": "The proposition delivered by the context machine is given in the language of the logic K-T [Obeid 19901.",
            "152": "K-T is a propositional, non-monotonic logic which employs Kleene's strong three-valued connectives, and which is extended with two modal operators (the language can be propositional as the knowledge base is sorted and closed).",
            "153": "The semantics of the logic are expressed in terms of states of partial information which allow an agent to be uncertain about the truth or falsity of his know!- 259 edge, and where possible, to make assumptions on the basis of what is not known to be false.",
            "154": "The inference rules of K-T are given in the Appendix.",
            "155": "The propositional content of the input question is set against a suitable subset of the agent's knowledge, i.e. the context.",
            "156": "The theorem prover then attempts to prove in the system K-i' that the propositional content of the input question follows from the context.",
            "157": "This it might achieve monotonically; or non-monotonically with the aid of assumptions.",
            "158": "It is the record left in the wake of the proof process in each case, which we interpret in order to provide a helpful answer.",
            "159": "The theorem prover is a three valued, modal analogue of a semantic tableau theorem prover (Beth 1962; Jeffrey 19671.",
            "160": "This method performs a case-wise analysis of all models in which the premises (read context) might be true while contradicting the conclusion (read propositional content of the input.",
            "161": "question). If no such models are found to exist, the theorem is proven.",
            "162": "We employ this method because it allows a user absolute access to every stage of the proof process.",
            "163": "We then exploit this access in order to find a helpful answer.",
            "164": "If a proof succeeds monotonically, the agent's answer is simply Yes.",
            "165": "If it succeeds by means of one or more assumptions, the answer is of the form Yes.",
            "166": "if..., where the body of the if-clause is the information that was assumed.",
            "167": "Where a proof fails, we have the task of determining the reason why it failed - i.e. which assumptions should be made to yield a yes-answer.",
            "168": "The proof process constnicts a tree of which the branches represent individual models.",
            "169": "These models are closely or distantly related to one another according to how much of the proof tree they have in conunon.",
            "170": "A failed proof has one or more models which are consistent, and therefore counterexamples to our intended inference.",
            "171": "We are able to compare these consistent models with closely related inconsistent ones.",
            "172": "We can then identify the contradiction which- is in some sense missing.",
            "173": "- i.e. we point to the particular premise or premises which are too weak to support the inference.",
            "174": "A helpful answer in this case takes the form No, unless...",
            "175": "and the body of the unlessclause is composed of the strengthening required in a premise or premises so that the counterexamples.would no longer arise.",
            "176": "This method remains constant regardless Of the actual content of the context.",
            "177": "Note that a single answer is 'always yielded and that the burden of assuring that its content is \"helpful\" rests entirely on the context machine.",
            "178": "Different implementations of the context selection mechanism have been attempted.",
            "179": "Originally, it operated by intersecting that part of the knowledge base which concerns the individuals and relations mentioned in the utterance.",
            "180": "In this sense, it relied exclusively on lexical information as the process operated by selecting propositions associated with lexical items reflected as objects in the knowledge base.",
            "181": "It used closure on the sortal hierarchy to achieve this.",
            "182": "This approach is compatible with Relevance Theory as it can be argued that Encyclopedic Knowledge can be thus implemented.",
            "183": "A side effect is lexical disambiguation - different readings of a word are associated with different Clusters of information; only compatible information will survive the intersection.",
            "184": "• This version was tested on a knowledge base modelling a building site, containing information about.",
            "185": "buildings, workers, materials and time tables.",
            "186": "The domain proved too complex to allow for any conclusions to be drawn: the diversity of objects whose behaviour needed modelling (including some beyond the current state of the art - e.g.. mass vs count objects, plurals, time and tense, etc).",
            "187": "was prohibitive.",
            "188": "Two other domains were tackled as a consequence: marital relationships and law, and the simple situation of what it takes to drive a car.",
            "189": "Even against simple domains, it became clear that mere reliance on keying lexical information would not be sufficient.",
            "190": "The search space remained large and insufficiently focussed as it included propositions which never contributed to deriving an answer, and a closer interaction between context machine and proof process should be postulated.",
            "191": "It seems that the context selection mechanism must have a model of inference.",
            "192": "An attempt at such a mechanism was developed.",
            "193": "The context machine Mark II extracts from the knowledge base any information which enable the truth of the proposition associated with the question to be derived.",
            "194": "Any implication in the knowledge base with that proposition as a consequent is selected to form part of the context and all rules and assertions which enable the truth of the antecedent of the implication to be derived are also included.",
            "195": "Any other rules, which cannot impinge upon the truth of the goal clause, are omitted as they are 'irrelevant' to the proof.",
            "196": "In a sense, this selection process anticipates the structure of the proof itself.",
            "197": "In the full system, the instantiation of quantified variables in sentences extracted from the knowledge base, is restricted to those individuals mentioned in the question, or relevant to those assertions made about individuals mentioned in the question.",
            "198": "This is implemented using the sorts! hierarchy.",
            "199": "The examples given in Section 5 are derived using this version over a very restricted domain.",
            "200": "Though the results were more satisfactory, the contexts derived in complex domains are still large.",
            "201": "Though all information selected plays a part in time overall proof, the search space is uniform for each proof branch.",
            "202": "It became clear that a full interaction between the structure of the proof and context selection must be achieved.",
            "203": "A third version of the context machine attempted to derive contexts local to particular steps in the proof process.",
            "204": "Though incomplete, the experience gained in the at, tempt convinced us that the selection of 'relevant' contextual information is dynamic.",
            "205": "Information pertaining to particular steps in the derivation of an answer should be local to that step and different 'relevant' contexts should be made accessible as the derivation progresses.",
            "206": "This section elaborates an example to illustrate (i) the basic theorem prover and (ii) the behaviour of context machines.",
            "207": "To simplify the examples, we consider the case where there is only one individual, Anne.",
            "208": "The set up concerns finding a helpful answer to Can Anne drive?",
            "209": "First we present a successful proof, working from an optimal context which yields that Anne can indeed drive.",
            "210": "The rules to the theorem prover (K-T [Obeid 19881) are given in the Appendix.",
            "211": "Notice that premises are theorems of the logic and so any premise of form It is logically equivalent to —M it.",
            "212": "The theorem prover reports that the inference KB I- drive(a) is proven by refutation.",
            "213": "This we know because each path is inconsistent.",
            "214": "The inference was proven monotonically (there was no need for assumptions) and required no sub-proof.",
            "215": "The answer here is Yes: Anne can drive because she has a licence, she owns a car and she has the required skills.",
            "216": "In the second example, the premise that Anne has a licence is removed.",
            "217": "The proof fails to show monotonically that Anne can drive.",
            "218": "The system therefore sets out to assume that Anne might have a licence and thus attempts to fill the gap in the agent's in- 260 formation.",
            "219": "Rule R3 (see Appendix) allows us to infer M it for any It if we cannot prove — it.",
            "220": "In this case, if we can assume the premise M licenced(a) successfully, we can prove the original assertion monotonically.",
            "221": "In this context there are no formulae which might affect the truth of M licenced(a) so our proof succeeds trivially.",
            "222": "The answer here is Yes, if Anne has a licence.",
            "223": "In the next example, we add explicitly that Anne does not have a licence.",
            "224": "We assume that this information is known and does not need a sub-proof.",
            "225": "Again, the proof fails monotonically as in the second example.",
            "226": "An attempt to hold an assumption that Anne has a licence will, however, fail as it will contradict the premise [3] which states that such an assumption is false.",
            "227": "The answer in this case is No. because Anne does not have a licence.",
            "228": "The procedure for dealing with hypotheticals is similar but the context is updated with the antecedent before the proof of the consequent is carried out.",
            "229": "Counterfactuals, which would require total revision of the knowledge base, are not treated.",
            "230": "We can use these examples to illustrate the problems faced with selecting the appropriate contexts to yield helpful answers.",
            "231": "The earliest version of the context machine would have selected all information associated with domain objects directly related to the words in the sentence (Anne and driving), and all information associated with the sonal hierarchy involving those objects.",
            "232": "The union of all these propositions produced a context that was not adequate: only some properties of Anne will affect hr driving, and not all knowledge about vehicles will contribute to finding an answer to whether Anne can drive.",
            "233": "Intersection of clusters of information obtained by closure on the hierarchy has the side effect of achieving lexical disambiguation, but, in complex domains, it excluded some relevant facts from the context, whilst still including propositions which could never play a role in the proof.",
            "234": "A more fine grained approach was needed.",
            "235": "In the second implementation, the context machine selected only those propositions which could lead to a goal.",
            "236": "Any implication in the knowledge base with the goal as a consequent is extracted, as are all assertions that contribute to establishing the truth of any of its antecedents (recursively).",
            "237": "The proof is established against this context as a whole.",
            "238": "Whilst significantly reducing the size of the resulting context as well as focussing its content on what the proof might turn out to be, there are problems with this approach.",
            "239": "Imagine the situation where Anne has the skill to drive, she owns a car, but she does not have a licence because she has to pay her fines.",
            "240": "She did not pay her fines because she has no money.",
            "241": "All this information would be extracted as a total context for answering the The proof to [13] mimicks that of example 2 above, but now, an attempt to establish whether Anne has a licence requires a sub-proof.",
            "242": "The proof fails to close on the assumption that Anne has money.",
            "243": "It cannot be inferred non-monotonically that Anne has money (because of [4]).",
            "244": "The answer in this case is No, be, cause Anne has,no money.",
            "245": "Some explanation is due here.",
            "246": "Though the answer offered in the last example is \"correct\", and there might be situations in which it is helpful, it is intuitively arguable that an answer No. because Anne has no licence is more helpful.",
            "247": "The point is that this version of the context ma,.",
            "248": "chine does not cater for the possibility of giving this latter answer under any conditions.",
            "249": "From this we conclude that a closer interaction between context machine and proof structure is necessary.",
            "250": "A helpful answer should not be confined to the ultimate reasons why the reply is No: the answer should depend upon some measure of \"closeness\" in contexts.",
            "251": "Contrary to the assumptions we made at the start of this project our conclusions lead us to postulate that such a view is indeed necessary to provide a fine grained notion of helpfulness.",
            "252": "We have no treatment of mutual beliefs so far (but Davies [1990] is compatible and promising).",
            "253": "We need to extend the logic so it can reason with varying domains if we are to exploit full the intensionality provided by the Property Theory.",
            "254": "We have started work on a treatment of time and tense in this framework ."
          }
        },
        {
          "section": "method",
          "title": "7.0 CONCLUSIONS",
          "sentences": {
            "255": "We have developed a semantic theory of questions using Prop,.",
            "256": "erty Theory.",
            "257": "We have investigated (i) pragmatic answerhood and (ii) modality using an experimental computational framework.",
            "258": "We believe that the insights gained from the work have beep valuable: they contribute towards our understanding of the requirements for a formally specified and computationally tractable theory of pragmatics which is capable of incorporating insights from cognitively oriented theories.",
            "259": "Furthermore, the experiment has pointed out that some of the intuitions underlying Relevance Theory are accurate and useful, especially with respect to context fefining strategies necessary for characterising helpful answers."
          }
        },
        {
          "section": "method",
          "title": "8.0 REFERENCES",
          "sentences": {
          }
        },
        {
          "section": "method",
          "title": "APPENDIX",
          "sentences": {
            "260": "Complete information is hard to obtain, even in the most manageable situations: in most cases, a reasoner does not know everything that is pertinent to the investigation at hand.",
            "261": "There are propositions whose truth status cannot be decided.",
            "262": "However, most of the classically based non-monotonic formalisms seem to resort to adding intermediary truth values between truth and falsity.",
            "263": "This, in fact, is one of the basic and most important features which distinguishes three-valued logics from the classical one.",
            "264": "Such a difference is reflected semantically by partial models (partially states of information) for three-valued logics as opposed to possible worlds (complete status of knowledge) for classical logic.",
            "265": "In this section, we shall develop the logic K-T.",
            "266": "In K-T a proposition is either accepted as true, accepted as false or not known at all.",
            "267": "The basic language LK_T which we shall use is a propositional logic.",
            "268": "Starting with primitive propositions T (true), F (false), p,q,r...",
            "269": ", more complicated ones are formed via closure under negation —, conjunction &, disjunction V. implication —) and epistemic possibility M. That is, if A and B are well-formed formulae then so are —A, A&B, AVB, A -4 B and MA.",
            "270": "Let N be the dual of M, i.e.",
            "271": "NA=—M. —, -4, & and V are Kleene's stong connectives.",
            "272": "Given A, MA is false if A is false, otherwise MA is true.",
            "273": "& and M may be taken as primitives.",
            "274": "V and -4 may be defined in terms of & and — as follows: e's strong implication -4 is not truth functional, i.e. A 13 is undefined if both A and B are undefined.",
            "275": "We also define a truthfucntional implication as follows: Definition 2.3.",
            "276": "(A B) = M(—A & B) V —A V B. is truth functional in the sense that the truth value of A B is true if both A and B have the same truth value.",
            "277": "Let A = B stand for (ADB)&(BDA).",
            "278": "Definition 2.4. A model structure for LK_T is K = <B, R, g&gt; where B is non-empty set, R is a binary relation on B and g is a truth assignment function g for atomic wffs.",
            "279": "The interpretation of R may be thought of as \"epistemic possible\" extension between states.",
            "280": "Given b, bl are members of B, we shall write b R bl to mean that the state bl is an \"epistemic possible\" extension of the state b.",
            "281": "We employ the notation K I= g A (resp.",
            "282": "K =1 g A) to mean that A is accepted as true (resp.",
            "283": "false) in K with respect to g.",
            "284": "For convenience, reference to g will be omitted except when a confusion may arise.",
            "285": "Let A, B be wffs; then the truth I= and the falsity =1 notions are recursively defined as follows:"
          }
        }
      ],
      "doc_id": 19,
      "actual_doc_id": "E91-1045-parscit-section.xml"
    }
  ]
}
