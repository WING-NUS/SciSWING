<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9440945">
Learning Phonological Rule Probabilities from Speech Corpora
with Exploratory Computational Phonology
</title>
<author confidence="0.997068">
Gary Tajchman; Daniel Jurafsky, and Eric Fosler
</author>
<affiliation confidence="0.996795">
International Computer Science Institute and
University of California at Berkeley
</affiliation>
<address confidence="0.399387">
{ taj chman,jurafsky,fosler} Aicsi.berkeley.edu
</address>
<sectionHeader confidence="0.964419" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999970136363636">
This paper presents an algorithm for learn-
ing the probabilities of optional phonolog-
ical rules from corpora. The algorithm is
based on using a speech recognition sys-
tem to discover the surface pronunciations
of words in speech corpora; using an auto-
matic system obviates expensive phonetic
labeling by hand. We describe the details
of our algorithm and show the probabili-
ties the system has learned for ten common
phonological rules which model reductions
and coarticulation effects. These probabili-
ties were derived from a corpus of 7203 sen-
tences of read speech from the Wall Street
Journal, and are shown to be a reason-
ably close match to probabilities from pho-
netically hand-transcribed data (TIMIT).
Finally, we analyze the probability differ-
ences between rule use in male versus fe-
male speech, and suggest that the differ-
ences are caused by differing average rates
of speech.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999721666666667">
Phonological rules have formed the basis of phono-
logical theory for decades, although their form and
their coverage of the data has changed over the years.
Until recently, however, it was difficult to deter-
mine the relationship between hand-written phono-
logical rules and actual speech data. The current
availability of large speech corpora and pronunci-
ation dictionaries has allowed us to connect rules
and speech in much tighter ways. For example, a
number of algorithms have recently been proposed
which automatically induce phonological rules from
dictionaries or corpora (Gasser 1993; Ellison 1992;
Daelemans et al. 1994).
While such algorithms have successfully induced
syllabicity or harmony constraints, or simple oblig-
</bodyText>
<note confidence="0.349604">
*Currently at Voice Processing Corp, 1 Main St,
Cambridge, MA 02142: tajchmanOvpro.com
</note>
<bodyText confidence="0.999877926829268">
atory phonological rules, there has been much less
work on non-obligatory (optional) rules. In part this
is because optional rules like flapping, vowel reduc-
tion, and various coarticulation effects are postlexi-
cal and often products of fast speech, and hence have
been considered less central to phonological theory.
In part, however, this is because optional rules are
inherently probabilistic. Where obligatory rules ap-
ply to every underlying form which meets the en-
vironmental conditions, producing a single surface
form, optional rules may not apply, and hence the
underlying form may appear as the surface form,
unmodified by the rule. This makes the induction
problem non-deterministic, and not solvable by the
above algorithms. 1
While optional rules have received less attention
in linguistics because of their probabilistic nature,
in speech recognition, by contrast, optional rules are
commonly used to model pronunciation variation. In
this paper, we employ techniques from speech recog-
nition research to address the problem of assign-
ing probabilities to these optional phonological rules.
We introduce a completely automatic algorithm that
explores the coverage of a set of phonological rules
on a corpus of lexically transcribed speech using the
computational resources of a speech recognition sys-
tem. This algorithm belongs to the class of tech-
niques we call Exploratory Computational Phonol-
ogy, which use statistical pattern recognition tools
to explore phonological spaces.
We describe the details of our probability esti-
mation algorithm and also present the probabilities
the system has learned for ten common phonological
rules which model reductions and coarticulation ef-
fects. Our probabilities are derived from a corpus of
7203 sentences of read speech from the Wall Street
Journal (NIST 1993). We also benchmark the prob-
abilities generated by our system against probabil-
ities from phonetically hand-transcribed data, and
show a relatively good fit. Finally, we analyze the
probability differences between rule use in male ver-
</bodyText>
<footnote confidence="0.991231333333333">
1 Note that this is true whether phonological theory
considers these true phonological rules or rather rules of
&quot;phonetic interpretation&quot;.
</footnote>
<page confidence="0.973628">
1
</page>
<bodyText confidence="0.995679666666667">
sus female speech, and suggest that the differences erate phone sequences from word orthography as an
are caused by differing average rates of speech.
additional source of pronunciations.
</bodyText>
<sectionHeader confidence="0.953697" genericHeader="introduction">
2 The Algorithm
</sectionHeader>
<bodyText confidence="0.999938266666667">
In this section we describe our algorithm which as-
signs probabilities to hand-written, optional phono-
logical rules like flapping. The algorithm takes a
lexicon of underlying forms and applies phonologi-
cal rules to produce a new lexicon of surface forms.
Then we use a speech recognition system on a large
corpus of recorded speech to check how many times
each of these surface forms occurred in the corpus.
Finally, by knowing which rules were used to gener-
ate each surface form, we can compute a count for
each rule. By combining this with a count of the
times a rule did not apply, the algorithm can com-
pute a probability for each rule.
The rest of this section will discuss each of the
aspects of the algorithm in detail.
</bodyText>
<subsectionHeader confidence="0.983011">
2.1 The Base Lexicon
</subsectionHeader>
<bodyText confidence="0.999989571428571">
Our base lexicon is quite large; it is used to gen-
erate the lexicons for all of our speech recognition
work at ICSI. It contains 160,000 entries (words)
with 300,000 pronunciations. The lexicon contains
underlying forms which are very shallow; thus they
are post-lexical in the sense that there is no rep-
resented relationship between e.g. &apos;critic&apos; and &apos;criti-
cism&apos; (where critic is pronounced kritik and criticism
kritisizm). However, the entries do not represent
flaps, vowel reductions, and other coarticulatory ef-
fects.
In order to collect our 300,000 pronunciations, we
combined seven different on-line pronunciation dic-
tionaries, including the five shown in Table 12.
</bodyText>
<table confidence="0.993964666666667">
Source Words Base Prons All Prons
CMU 95,781 99,279 399,265
LIMSI 32,873 37,936 49,597
PRONLEX 30,353 30,354 81,936
BRITPRON 77,685 85,450 108,834
TTS 77,383 83,297 111,028
</table>
<tableCaption confidence="0.958009">
Table 1: Pronunciation sources used to build fully
expanded lexicon.
</tableCaption>
<bodyText confidence="0.380735">
For further information about these sources please
refer to CMU (CMU 1993), LIMSI (Lamel 1993),
PRONLEX (COMLEX 1994), BRITPRON (Robin-
son 1994). A text-to-speech system was used to gen-
</bodyText>
<footnote confidence="0.990175428571429">
2Although it was not relevant to the experiments de-
scribed here, our lexicon also included two sources which
directly supply surface forms. These were 13,362 hand-
transcribed pronunciations of 5871 words from TIMIT
(TIMIT 1990), and 230 pronunciations of 36 words de-
rived in-house from the OGI Numbers database (Cole
et al. 1994).
</footnote>
<table confidence="0.36242475">
IPA ARPA ICSI IPA ARPA ICSI
b b b b° - bc1
d d d d° - dcl
g g g go - gcl
P P P 130 _ pcl
t t t t° - tcl
k k k k° - kcl
a aa aa s s s
w ae ae z z z
A ah ah ., sh sh
3 ao ao zh zh
E eh eh f f f
3&apos; er er v v v
t, ih ih 0 th th
i iy iy 8 dh dh
o ow ow tf ch ch
CO uh uh dz jh jh
u uw uw h hh hh
aw aw aw ii - hv
aY ay ay Y Y Y
e ey ey r r r
3&apos; oy oy w w w
1 - el 1 1 1
IP - em m m m
q - en n n n
a - ax 13 ng ng
i - ix f - dx
3&apos; - axr silence h# h#
</table>
<tableCaption confidence="0.67801">
Table 2: Baseform phone set used was the ARPA-
</tableCaption>
<bodyText confidence="0.93339">
BET. This was expanded to include syllabics, stop
closures, and reduced vowels, alveolar flap, and
voiced h.
We represent pronunciations with the set of 54
ARPAbet-like phones detailed in Table 2. All the
lexicon sources except LIMSI use ARPABET-like
phone sets&apos;. CMU, BRITPRON, and PRONLEX
phone sets include three levels of vowel stress. The
pronunciations from all these sources were mapped
into our phone set using a set of obligatory rules
for stop closures [bcl, del, gcl, pd, tcl, kcl], and op-
tional rules to introduce the syllabic consonants [el,
em, en], reduced vowels [ax, ix, axr], voiced h [hv],
and alveolar flap [dx].
</bodyText>
<subsectionHeader confidence="0.9999155">
2.2 Applying Phonological Rules to Build a
Surface Lexicon
</subsectionHeader>
<bodyText confidence="0.9992185">
We next apply phonological rules to our base lexi-
con to produce the surface lexicon. Since the rules
</bodyText>
<footnote confidence="0.8174956">
3The LIMSI pronunciations already included the syl-
labic consonants and reduced vowels. For this reason,
the words found only in the LIMSI source lexicon did
not participate in the probability estimates for the syl-
labic and reduced vowel rules.
</footnote>
<page confidence="0.987511">
2
</page>
<table confidence="0.999595666666667">
Name Code Rule
Reductions
Mid vowels RV1 -stress [aa ae ah ao eh er ey ow uh]—&gt; ax
High vowels RV2 -stress [iy ih uw] —+ ix
R-vowel RV3 -stress er —÷ axr
Syllabic n SL1 [ax ix] n —+ en
Syllabic m SL2 [ax ix] m -- em
Syllabic 1 SL3 [ax ix] 1 —■ el
Syllabic r SL4 [ax ix] r axr
Flapping FL1 [tcl dcl] [t d]---,- dx /V _ [ax ix axr]
Flapping-r FL2 [tel dcl] [t c1]-4 dx /V r _ [ax ix axr]
H-voicing VH1 hh -- hv / [-Fvoice] _ [+voice]
</table>
<tableCaption confidence="0.998891">
Table 3: Phonological Rules
</tableCaption>
<bodyText confidence="0.972398512820513">
are optional, the surface lexicon must contain each
underlying pronunciation unmodified, as well as the
pronunciation resulting from the application of each
relevant phonological rule. Table 3 gives the 10
phonological rules used in these experiments.
One goal of our rule-application procedure was
to build a tagged lexicon to avoid having to imple-
ment a phonological-rule parser to parse the surface
pronunciations. In a tagged lexicon, each surface
pronunciation is annotated with the names of the
phonological rules that applied to produce it. Thus
when the speech recognizer finds a particular pro-
nunciation in the speech input, the list of rules which
applied to produce it can simply be looked up in the
tagged lexicon.
The algorithm applies rules to pronunciations re-
cursively; when a context matches the left hand side
of a phonological rule &quot;RULE,&quot; two pronunciations
are produced: one unchanged by the rule (marked
-RULE), and one with the rule applied (marked
+RULE). The procedure places the +RULE pro-
nunciation on the queue for later recursive rule ap-
plication, and continues trying to apply phonological
rules to the -RULE pronunciation. See Figure 1 for
details of the algorithm. While our procedure is not
guaranteed to terminate, in practice the phonologi-
cal rules we apply have a finite recursive depth.
The nondeterministic mapping produces a tagged
equiprobable multiple pronunciation lexicon of
510,000 pronunciations for 160,000 words. For ex-
ample, Table 4 gives our base forms for the word
&quot;butter&quot;:
Source Pronunciation
TTS b ah t axr
BPU b ah t ax
BPU b ah t axr
CMU b ah t er
LIM b ah t axr
PLX b ah t er
</bodyText>
<tableCaption confidence="0.983432">
Table 4: Base forms for &quot;butter&quot;
</tableCaption>
<figure confidence="0.9704113">
For each lexical item, L, do:
Place all base prons of L onto queue Q
While Q is not empty do:
Dequeue pronunciation P from Q
For each phonological rule R, do:
If context of R could apply to P
Apply R to P, giving P&apos;
Tag P&apos; with +R, put on queue Q
Tag P with -R
Output P with tags
</figure>
<figureCaption confidence="0.999956">
Figure 1: Applying Rules to the Base Lexicon
</figureCaption>
<bodyText confidence="0.9946375">
The resulting tagged surface lexicon would have
the entries in Table 5.
</bodyText>
<subsectionHeader confidence="0.999816">
2.3 Filtering with forced-Viterbi
</subsectionHeader>
<bodyText confidence="0.9999062">
Given a lexicon with tagged surface pronunciations,
the next required step is to count how many times
each of these pronunciations occurs in a speech
corpus. The algorithm we use has two steps;
PHONETIC LIKELIHOOD ESTIMATION and FORCED-
VITERBI ALIGNMENT.
In the first step, PHONETIC LIKELIHOOD ESTI-
MATION, we examine each 20ms frame of speech
data, and probabilistically label each frame with the
phones that were likely to produce the data. That
is, for each of the 54 phones in our phone-set, we
compute the probability that the slice of acoustic
data was produced by that phone. The result of
this labeling is a vector of phone-likelihoods for each
acoustic frame.
Our algorithm is based on a multi-layer percep-
tron (MLP) which is trained to compute the condi-
tional probability of a phone given an acoustic fea-
ture vector for one frame, together with 80 ms of
surrounding context. Bourlard &amp; Morgan (1991)
</bodyText>
<page confidence="0.98951">
3
</page>
<tableCaption confidence="0.531466333333333">
bcl b ah dx ax:+BPU +FL1; +CMU +FL1 +RVI; +PLX +FL1 +RVI
bcl b ah dx axr: +TTS +FL1; +BPU +FL1; +CMU +FL1 -RVI +RV3; +LIM +FL1; +PLX +FL1 -RV1 +RV3
bcl b ah tcl t ax:+BPU -FL1; +cmu -FL1 +RVI; +PLX -FL1 +RVI
bcl b ah tcl t axr:+TTS -FL1; +BPU -FL1; +CMU -FL1 -RVI +RV3; +LIM -FL1; +PLX -FL1 -RV1 +RV3
bcl b ah tcl t er:+CMU -RV1 -RV3; +PLX -RV1 -RV3
Table 5: Resulting tagged entries
</tableCaption>
<bodyText confidence="0.984831">
and Renals et al. (1991) show that with a few as-
sumptions, an MLP may be viewed as estimating
the probability P(q1x) where q is a phone and x
is the input acoustic speech data. The estimator
consists of a simple three-layer feed forward MLP
trained with the back-propagation algorithm (see
Figure 2). The input layer consists of 9 frames of in-
put speech data. Each frame, representing 10 msec
of speech, is typically encoded by 9 PLP (Hermansky
1990) coefficients, 9 delta-PLP coefficients, 9 delta-
delta PLP coefficients, delta-energy and delta-delta-
energy terms. Typically, we use 500-4000 hidden
units. The output layer has one unit for each phone.
The MLP is trained on phonetically hand-labeled
speech (TIMIT), and then further trained by an it-
erative Viterbi procedure (forced-Viterbi providing
the labels) with Wall Street Journal corpora.
</bodyText>
<figureCaption confidence="0.93175">
Figure 2: Phonetic Likelihood Estimator
</figureCaption>
<equation confidence="0.655885">
P(q)
</equation>
<bodyText confidence="0.999905555555556">
The second step of the algorithm, FORCED-
VITERBI ALIGNMENT, takes this vector of likelihoods
for each frame and produces the most likely phonetic
string for the sentence. If each word had only a sin-
gle pronunciation and if each phone had some fixed
duration, the phonetic string would be completely
determined by the word string. However, phones
vary in length as a function of idiolect and rate of
speech, and of course the very fact of optional phono-
logical rules implies multiple possible pronunciations
for each word. These pronunciations are encoded in
a hidden Markov model (HMM) for each word.
The Viterbi algorithm is a dynamic programming
search, which works by computing for each phone at
each frame the most likely string of phones ending
in that phone. Consider a sentence whose first two
words are &quot;of the&quot;, and assume the simplified lexicon
in Figure 3.
</bodyText>
<figureCaption confidence="0.996214">
Figure 3: Pronunciation models for &quot;of&quot; and &quot;the&quot;
</figureCaption>
<bodyText confidence="0.999965222222222">
Each pronunciation of the words &apos;of&apos; and &apos;the&apos;
is represented by a path through the probabilistic
automaton for the word. For expository simplic-
ity, we have made the (incorrect) assumption that
consonants have a duration of 1 frame, and vowel a
duration of 2 or 3 frames. The algorithm analyzes
the input frame by frame, keeping track of the best
path of phones. Each path is ranked by its proba-
bility, which is computed by multiplying each of the
transition probabilities and the phone probabilities
for each frame. Figure 4 shows a schematic of the
path computation. The size of each dot indicates the
magnitude of the local phone likelihood. The max-
imum path at each point is extended; non-maximal
paths are pruned.
The result of the forced-Viterbi alignment on a
single sentence is a phonetic labeling for the sen-
tence (see Figure 5 for an example), from which we
</bodyText>
<table confidence="0.6658587">
Output:
54 Phones
Hidden Layer:
500-4000 Fully
Connected Units
Input Layer: 9 Frames
of 20 RASTA features,
total 180 units
Right Context
-201w-17nts -20m -101ns Iowa 220.1 ...eu
</table>
<bodyText confidence="0.790688285714286">
The probability P(q1x) produced by the MLP for
each frame is first converted to the likelihood P(xlq)
by dividing by the prior P(q), according to Bayes&apos;
rule; we ignore P(x) since it is constant here:
P(x q) =
P(q x)P(x)
I
</bodyText>
<page confidence="0.94647">
4
</page>
<figureCaption confidence="0.9990735">
Figure 4: Computing most-likely phone paths in a
Forced-Viterbi alignment of &apos;of the&apos;
</figureCaption>
<bodyText confidence="0.696573">
new york city&apos;s fresh
</bodyText>
<equation confidence="0.501250466666667">
n y uw y ao r kcl k s ih tcl t iy z f r eh sh
kills landfill on
kcl k ih 1 z 1 ae n dcl f ih 1 aa n
staten island for one
s tcl t ae tcl t en ay 1 ax n dcl f ao r w ah n
dumps four million
dcl d ah m pcl p s f ao r m ih 1 y ix n
gallons of toxic
gcl g ae 1 ax n z ax f tcl t aa kcl k s ix kcl
liquid into nearby
1 ih kcl k w ih dcl en tcl t uw n ih r bcl b ay
freshwater streams every
f r eh sh w ao dx axr s tcl t r iy m z eh v r iy
day
dcl d ey
</equation>
<figureCaption confidence="0.8322215">
Figure 5: A forced-Viterbi phonetic labelling for a
Wall Street Journal sentence
</figureCaption>
<figure confidence="0.943838272727273">
END
ax
P(ax dh). .7
iy
dh P(v I acoustics) -= .9
ah -ah-v-dh-ax-ax-ax
ax-ax-ax-v-dx-iy-iy
•
P(v oh)= .4
START
P(ah I START) = .5
</figure>
<bodyText confidence="0.9993935">
can produce a phonetic pronunciation for each word.
By running this algorithm on a large corpus of sen-
tences, we produce a list of &quot;bottom-up&quot; pronunci-
ations for each word in the corpus.
</bodyText>
<subsectionHeader confidence="0.999042">
2.4 Rule probability estimation
</subsectionHeader>
<bodyText confidence="0.999932">
The rule-tagged surface lexicon described in §2.1 and
the counts derived from the forced-Viterbi described
in §2.3 can be combined to form a tagged lexicon
that also has counts for each pronunciation of each
word. Following is a sample entry from this lexicon
for the word Adams which shows the five derivations
for its single pronunciation:
</bodyText>
<equation confidence="0.794586">
Adams: ae dx ax m z: count=2
derivation 1: +ATS +FL1 -SL2
derivation 2: +EPU +FL1 -SL2
derivation 3: +CMU +FL1 +RV1 -SL2
derivation 4: +LIM +FL1 -SL2
derivation 5: +PLX +FL1 -SL2
</equation>
<bodyText confidence="0.991796714285714">
Each pronunciation of each word in this lexicon is
annotated with rule tags. Since each pronunciation
may be derived from different source dictionaries or
via different rules, each pronunciation of a word may
contain multiple derivations, each consisting of the
list of rules which applied to give the pronunciation
from the base form. These tags are either positive,
indicating that a rule applied, or negative, indicating
that it did not.
To produce the initial rule probabilities, we need
to count the number of times each rule applies, out
of the number-of times it had the potential to apply.
If each pronunciation only had a single derivation,
this would be computed simply as follows:
</bodyText>
<equation confidence="0.939768222222222">
Ct (Rule R applied in p)
P(R)-= E
Ct (Rule R could have applied in p)
pePRON
This could be computed from the tags as :
Ct(+R tags in p)
P(R). E
Ct(-I-R tags in p) Ct(-R tags in p)
pEPRON
</equation>
<bodyText confidence="0.999016428571429">
However, since each pronunciation can have mul-
tiple derivations, the counts for each rule from each
derivation need to be weighted by the probability
of the derivation. The derivation probability is com-
puted simply by multiplying together the probability
of each of the applications or non-applications of the
rule. Let
</bodyText>
<listItem confidence="0.998614272727273">
• DERIVS(p) be the set of all derivations of a
pronunciation p,
• POSRULES(p,r,d) be 1.0 if derivation d of pro-
nunciation p uses rule r, else 0.
• ALLRULES(p,r) be the count of all derivations
of p in which rule r could have applied (i.e. in
which d has either a +R or -R tag).
• P(d1p) be the probability of the derivation d of
pronunciation p.
• PRON be the set of pronunciations derived from
the forced-Viterbi output.
</listItem>
<bodyText confidence="0.697066">
Now a single iteration of the rule-probability al-
gorithm must perform the following computation:
</bodyText>
<page confidence="0.81978">
5
</page>
<equation confidence="0.9947765">
P(r) =E E P(dip)POSRULES(p,r,d)
pEPRON dEDERIVS(P)
</equation>
<bodyText confidence="0.991026230769231">
Since we have no prior knowledge, we make the
zero-knowledge initial assumption that P(dip) =
IDERI1V S(p)I. The algorithm can the be run as a
successive estimation-maximization to provide suc-
cessive approximations to P(djp). For efficiency rea-
sons, we actually compute the probabilities of all
rules in parallel, as shown in Figure 6.
For each word/pron pair P E P RON from
forced-Viterbi alignment
Let DE RIV S(P) be the set of rule
derivations of P
For every d E DE RIV S(P)
For every rule R E d
</bodyText>
<equation confidence="0.748310444444445">
if (R = +RU LE)
then
ruleapp{RU LE} +=
1DERIv s(P)1
else
rulenoapp{RU LE} +=DERI1VS(P)I
For every rule RULE
P(RULE) = ruleapp(RULE)
ruleapp(RULE)+ruleapp(RULE)
</equation>
<figureCaption confidence="0.991743">
Figure 6: Parallel computation of rule probabilities
</figureCaption>
<sectionHeader confidence="0.99882" genericHeader="background">
3 Results
</sectionHeader>
<bodyText confidence="0.999117115384615">
We ran the estimation algorithm on 7203 sentences
(129,864 words) read from the Wail Street Journal.
The corpus (1993 WSJ Hub 2 (WSJ 0) training data)
-Consisted of 12 hours of speech, and had 8916 unique
words. Table 6 shows the probabilities for the ten
phonological rules described in §2.2.
Note that all of the rules are indeed quite op-
tional; even the most commonly-employed rules, like
flapping and h-voicing, only apply on average about
90% of the time. Many of the other rules, such as
the reduced-vowel or reduced-liquid rules, only ap-
ply about 50% of the time.
We next attempted to judge the reliability of
our automatic rule-probability estimation algorithm
by comparing it with hand transcribed pronuncia-
tions. We took the hand-transcribed pronunciations
of each word in TIMIT, and computed rule probabil-
ities by the same rule-tag counting procedure used
for our forced-Viterbi output. Figure 7 shows the fit
between the automatic and hand-transcribed proba-
bilities. Since the TIMIT pronunciations were from
a completely different data collection effort with a
very different corpus and speakers, the closeness of
the probabilities is quite encouraging.
Figure 8 breaks down our automatically generated
rule probabilities for the Wall Street Journal corpus
</bodyText>
<subsectionHeader confidence="0.634592">
Percent of Phonological Rule Use, WSJO vs. TIMIT
</subsectionHeader>
<figure confidence="0.996637538461539">
Percent
100.00
90.00
80.00
70.00
60.00
50.00
40.00
30.00
20.00
10.00
0.00
Rule
</figure>
<figureCaption confidence="0.940145">
Figure 7: Automatic vs Hand-transcribed Probabil-
ities for Phonological Rules
</figureCaption>
<bodyText confidence="0.9999600625">
into male and female speakers. Notice that many of
the rules seem to be employed more often by men
than by women. For example, men are about 5%
more likely to flap, more likely to reduce vowels ih
and er, and slightly more likely to reduce liquids and
nasals.
Since these are coarticulation or fast-speech ef-
fects, our initial hypothesis was that the differ-
ence between male and female speakers was due to
a faster speech-rate by males. By computing the
weighted average seconds per phone for male and
female speakers, we found that females had an av-
erage of 71 ms/phone, while males had an average
of 68 ms/phone, a difference of about 4%, quite cor-
related with the similar differences in reduction and
flapping.
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999933357142857">
Our algorithm for phonological rule probability esti-
mation synthesizes and extends earlier work by (Co-
hen 1989) and (Wooters 1993). The idea of using
optional phonological rules to construct a speech-
recognition lexicon derives from Cohen (1989), who
applied optional phonological rules to a baseform
dictionary to produce a surface lexicon and then
used TIMIT to assign probabilities for each pronun-
ciation. The use of a forced-Viterbi speech decoder
to discover pronunciations from a corpus was pro-
posed by Wooters (1993). Wesenick &amp; Schiel (1994)
independently propose a very similar forced-Viterbi-
decoder-based technique which they use for measur-
ing the accuracy of hand-written phonology.
</bodyText>
<figure confidence="0.9936648">
ALLRULES(p,r)
111111111111111111111
1111111111111111111111111
1111111111111
111111111111111111111111111111111111
1111111111111111111111111111111111111111111
111■11111•1■
11111111111111111111111
&quot;AkIt.&quot;11
FL1 R..2 RVI RV2 RV3 SLI SL2 SL3 SlA VH1 ;
</figure>
<page confidence="0.971979">
6
</page>
<table confidence="0.99942875">
Name Code Rule Pr
Reductions
Mid vowels RV1 -stress [aa ae ah ao eh er ey ow uh]-- ax .60
High vowels RV2 -stress [iy ih uw] , ix .57
Ft-vowel RV3 -stress er —÷ axr .74
Syllabic n SL1 [ax ix] n --+ en .35
Syllabic m SL2 [ax ix] m —+ em .35
Syllabic 1 SL3 [ax ix] 1 , el .72
Syllabic r SL4 [ax ix] r --■ axr .77
Flapping FL1 [tcl dcl] [t d]-, dx /V _ [ax ix axr] .87
Flapping-r FL2 [tcl dcl] [t d]-4 dx /V r _ [ax ix axr] .92
H-voicing VH1 hh ---,. hv / [+voice] _ [+voice] .92
</table>
<tableCaption confidence="0.999365">
Table 6: Results of the Rule-Probability-Estimation Algorithm
</tableCaption>
<figure confidence="0.974990266666667">
Percent of Phonological Rule Use
Percent
female
90.00
1 1 1 1 1 1
male
70.00
60.00
60.00
40.00
30.00
20.00
10.00
0.00
Rule
</figure>
<figureCaption confidence="0.9913335">
Figure 8: Male vs Female Probabilities for Phono-
logical Rules
</figureCaption>
<bodyText confidence="0.998590307692308">
Chen (1990) and Riley (1991) model the relation-
ship between phonemes and their allophonic realiza-
tions by training decision trees on TIMIT data. A
decision tree is learned for each underlying phoneme
specifying its .surface realization in different con-
texts. These completely automatic techniques, re-
quiring no hand-written rules, can allow a more
fine-grained analysis than our rule-based algorithm.
However, as a consequence, it is more difficult to
extract generalizations across classes of phonemes
to which rules can apply. We think that a hybrid
between a rule-based and a decision-tree approach
could prove quite powerful.
</bodyText>
<sectionHeader confidence="0.994828" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999976517241379">
Although the paradigm of exploratory computa-
tional phonology is only in its infancy, we believe
our rule-probability estimation algorithm to be a
new and useful instance of the use of probabilistic
techniques and spoken-language corpora in compu-
tational linguistics. In Tajchman et al. (1995) we
report on the results of our algorithm on speech
recognition performance. We plan in future work
to address a number of shortcomings of these ex-
periments, for example including some spontaneous
speech corpora, and looking at a wider variety of
rules.
In addition, we have extended our algorithm to in-
duce new pronunciations which generalize over pro-
nunciations seen in the corpus (Wooters &amp; Stolcke
1994). We now plan to augment our probability es-
timation to use the pronunciations from this new
HMM-induction-based generalization step. This will
require extending our tag-based probability estima-
tion step to parse the phone strings from the forced-
Viterbi.
In other current work we have also been using
this algorithm to model the phonological component
of the accent of non-native speakers. Finally, we
hope in future work to be able to combine our rule-
based approach with more bottom-up methods like
the decision-tree or phonological parsing algorithms
to induce rules as well as merely training their prob-
abilities.
</bodyText>
<sectionHeader confidence="0.970263" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.568662666666667">
Thanks to Mike Hochberg, Nelson Morgan, Steve Re-
nals, Tony Robinson, Florian Schiel, Andreas Stolcke,
and Chuck Wooters. This work was partially funded
by ICSI and an SRI subcontract from ARPA contract
MDA904-90-C-5253. Partial funding also came from ES-
PRIT project 6487 (The Wernicke project).
</bodyText>
<sectionHeader confidence="0.960844" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.891437">
BOURLARD, H., &amp; N. MORGAN. 1991. Merging mul-
tilayer perceptrons &amp; Hidden Markov Models:
</reference>
<page confidence="0.7429575">
80.00
7
</page>
<reference confidence="0.998159352941176">
Some experiments in continuous speech recog-
nition. In Artificial Neural Networks: Advances
and Applications, ed. by E. Gelenbe. North Hol-
land Press.
CHEN, F. 1990. Identification of contextual factors
for pronounciation networks. In IEEE ICASSP-
90 , 753-756.
CMU, 1993. The Carnegie Mellon Pronouncing Dic-
tionary v0.1. Carnegie Mellon University.
COHEN, M. H., 1989. Phonological Structures for
Speech Recognition. University of California,
Berkeley dissertation.
COLE, R. A., K. ROGINSKI, &amp; M. FANTY., 1994.
The OGI Numbers Database. Oregon Graduate
Institute.
COMLEX, 1994. The COMLEX English Pronounc-
ing Dictionary. copyright Trustees of the Uni-
versity of .Pennsylvania.
DAELEMANS, WALTER, STEVEN GILLIS, &amp; GERT
DURIEUX. 1994. The acquisition of stress: A
data-oriented approach. Computational Lin-
guistics 208.421-451.
ELLISON, T. MARK, 1992. The Machine Learning of
Phonological Structure. University of Western
Australia dissertation.
GASSER, MICHAEL, 1993. Learning words in time:
Towards 6. modular connectionist account of the
acquisition of receptive morphology. Draft.
HERMANSKY, H. 1990. Perceptual linear predictive
(pip) analysis of speech. J. Acoustical Society
of America 87.
LAMEL, LORI, 1993. The Limsi Dictionary.
NIST, 1993. Continuous Speech Recognition Corpus
(WSJ 0). National Institute of Standards and
Technology Speech Disc 11-1.1 to 11-3.1.
RENALS, S., N. MORGAN, H. BOURLARD, M. CO-
HEN, H. FRANCO, C. WOOTERS, &amp; P. KOHN.
1991. Connectionist speech recognition: Sta-
tus and prospects. Technical Report TR-91-070,
ICSI, Berkeley, CA.
RILEY, MICHAEL D. 1991. A statistical model for
generating pronunciation networks. In IEEE
ICASSP-91, 737-740.
ROBINSON, ANTHONY, 1994. The British English
Example Pronunciation Dictionary, v0.1. Cam-
bridge University.
TAJCHMAN, GARY, ERIC FOSLER, &amp; DANIEL Ju-
RAFSKY. 1995. Building multiple pronunciation
models for novel words using exploratory com-
putational phonology. To appear in Eurospeech-
95
TIMIT, 1990. TIMIT Acoustic-Phonetic Continuous
Speech Corpus. National Institute of Standards
and Technology Speech Disc 1-1.1. NTIS Order
No. PB91-505065.
WESENICK, MARIA-BARBARA, &amp; FLORIAN SCHIEL.
1994. Applying speech verification to a large
data base of German to obtain a statistical sur-
vey about rules of pronunciation. In ICSLP-9-4,
279-282.
WOOTERS, CHARLES C., 1993. Lexical Modeling
in a Speaker Independent Speech Understand-
ing System. Berkeley: University of California
dissertation. Available as ICSI TR-92-062.
WOOTERS, CHUCK, &amp; ANDREAS STOLCKE. 1994.
Multiple-pronunciation lexical modeling in a
speaker-independent speech understanding sys-
tem. In /CSLP-94
</reference>
<page confidence="0.998492">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.497502">
<title confidence="0.996282">Learning Phonological Rule Probabilities from Speech Corpora with Exploratory Computational Phonology</title>
<author confidence="0.994963">Gary Tajchman</author>
<author confidence="0.994963">Daniel Jurafsky</author>
<author confidence="0.994963">Eric</author>
<affiliation confidence="0.9331965">International Computer Science Institute University of California at</affiliation>
<email confidence="0.574705">tajchmanAicsi.berkeley.edu</email>
<email confidence="0.574705">jurafskyAicsi.berkeley.edu</email>
<email confidence="0.574705">foslerAicsi.berkeley.edu</email>
<abstract confidence="0.999357826086957">This paper presents an algorithm for learning the probabilities of optional phonological rules from corpora. The algorithm is based on using a speech recognition system to discover the surface pronunciations of words in speech corpora; using an automatic system obviates expensive phonetic labeling by hand. We describe the details of our algorithm and show the probabilities the system has learned for ten common phonological rules which model reductions and coarticulation effects. These probabilities were derived from a corpus of 7203 sentences of read speech from the Wall Street Journal, and are shown to be a reasonably close match to probabilities from phonetically hand-transcribed data (TIMIT). Finally, we analyze the probability differences between rule use in male versus female speech, and suggest that the differences are caused by differing average rates of speech.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H BOURLARD</author>
<author>N MORGAN</author>
</authors>
<title>Merging multilayer perceptrons &amp; Hidden Markov Models: Some experiments in continuous speech recognition.</title>
<date>1991</date>
<booktitle>In Artificial Neural Networks: Advances and Applications,</booktitle>
<editor>ed. by E. Gelenbe.</editor>
<publisher>North Holland Press.</publisher>
<marker>BOURLARD, MORGAN, 1991</marker>
<rawString>BOURLARD, H., &amp; N. MORGAN. 1991. Merging multilayer perceptrons &amp; Hidden Markov Models: Some experiments in continuous speech recognition. In Artificial Neural Networks: Advances and Applications, ed. by E. Gelenbe. North Holland Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F CHEN</author>
</authors>
<title>Identification of contextual factors for pronounciation networks.</title>
<date>1990</date>
<booktitle>In IEEE ICASSP90 ,</booktitle>
<pages>753--756</pages>
<marker>CHEN, 1990</marker>
<rawString>CHEN, F. 1990. Identification of contextual factors for pronounciation networks. In IEEE ICASSP90 , 753-756.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CMU</author>
</authors>
<title>The Carnegie Mellon Pronouncing Dictionary v0.1.</title>
<date>1993</date>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="6126" citStr="CMU 1993" startWordPosition="944" endWordPosition="945">s pronounced kritik and criticism kritisizm). However, the entries do not represent flaps, vowel reductions, and other coarticulatory effects. In order to collect our 300,000 pronunciations, we combined seven different on-line pronunciation dictionaries, including the five shown in Table 12. Source Words Base Prons All Prons CMU 95,781 99,279 399,265 LIMSI 32,873 37,936 49,597 PRONLEX 30,353 30,354 81,936 BRITPRON 77,685 85,450 108,834 TTS 77,383 83,297 111,028 Table 1: Pronunciation sources used to build fully expanded lexicon. For further information about these sources please refer to CMU (CMU 1993), LIMSI (Lamel 1993), PRONLEX (COMLEX 1994), BRITPRON (Robinson 1994). A text-to-speech system was used to gen2Although it was not relevant to the experiments described here, our lexicon also included two sources which directly supply surface forms. These were 13,362 handtranscribed pronunciations of 5871 words from TIMIT (TIMIT 1990), and 230 pronunciations of 36 words derived in-house from the OGI Numbers database (Cole et al. 1994). IPA ARPA ICSI IPA ARPA ICSI b b b b° - bc1 d d d d° - dcl g g g go - gcl P P P 130 _ pcl t t t t° - tcl k k k k° - kcl a aa aa s s s w ae ae z z z A ah ah ., sh</context>
</contexts>
<marker>CMU, 1993</marker>
<rawString>CMU, 1993. The Carnegie Mellon Pronouncing Dictionary v0.1. Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M H COHEN</author>
</authors>
<title>Phonological Structures for Speech Recognition.</title>
<date>1989</date>
<institution>University of California, Berkeley dissertation.</institution>
<marker>COHEN, 1989</marker>
<rawString>COHEN, M. H., 1989. Phonological Structures for Speech Recognition. University of California, Berkeley dissertation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A COLE</author>
<author>K ROGINSKI</author>
<author>M FANTY</author>
</authors>
<title>The OGI Numbers Database. Oregon Graduate Institute.</title>
<date>1994</date>
<marker>COLE, ROGINSKI, FANTY, 1994</marker>
<rawString>COLE, R. A., K. ROGINSKI, &amp; M. FANTY., 1994. The OGI Numbers Database. Oregon Graduate Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>COMLEX</author>
</authors>
<title>The COMLEX English Pronouncing Dictionary. copyright Trustees of the</title>
<date>1994</date>
<institution>University of .Pennsylvania.</institution>
<contexts>
<context position="6169" citStr="COMLEX 1994" startWordPosition="950" endWordPosition="951">izm). However, the entries do not represent flaps, vowel reductions, and other coarticulatory effects. In order to collect our 300,000 pronunciations, we combined seven different on-line pronunciation dictionaries, including the five shown in Table 12. Source Words Base Prons All Prons CMU 95,781 99,279 399,265 LIMSI 32,873 37,936 49,597 PRONLEX 30,353 30,354 81,936 BRITPRON 77,685 85,450 108,834 TTS 77,383 83,297 111,028 Table 1: Pronunciation sources used to build fully expanded lexicon. For further information about these sources please refer to CMU (CMU 1993), LIMSI (Lamel 1993), PRONLEX (COMLEX 1994), BRITPRON (Robinson 1994). A text-to-speech system was used to gen2Although it was not relevant to the experiments described here, our lexicon also included two sources which directly supply surface forms. These were 13,362 handtranscribed pronunciations of 5871 words from TIMIT (TIMIT 1990), and 230 pronunciations of 36 words derived in-house from the OGI Numbers database (Cole et al. 1994). IPA ARPA ICSI IPA ARPA ICSI b b b b° - bc1 d d d d° - dcl g g g go - gcl P P P 130 _ pcl t t t t° - tcl k k k k° - kcl a aa aa s s s w ae ae z z z A ah ah ., sh sh 3 ao ao zh zh E eh eh f f f 3&apos; er er v </context>
</contexts>
<marker>COMLEX, 1994</marker>
<rawString>COMLEX, 1994. The COMLEX English Pronouncing Dictionary. copyright Trustees of the University of .Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>WALTER DAELEMANS</author>
<author>STEVEN GILLIS</author>
<author>GERT DURIEUX</author>
</authors>
<title>The acquisition of stress: A data-oriented approach.</title>
<date>1994</date>
<journal>Computational Linguistics</journal>
<pages>208--421</pages>
<marker>DAELEMANS, GILLIS, DURIEUX, 1994</marker>
<rawString>DAELEMANS, WALTER, STEVEN GILLIS, &amp; GERT DURIEUX. 1994. The acquisition of stress: A data-oriented approach. Computational Linguistics 208.421-451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T MARK ELLISON</author>
</authors>
<date>1992</date>
<institution>The Machine Learning of Phonological Structure. University of Western Australia dissertation.</institution>
<marker>ELLISON, 1992</marker>
<rawString>ELLISON, T. MARK, 1992. The Machine Learning of Phonological Structure. University of Western Australia dissertation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MICHAEL GASSER</author>
</authors>
<title>Learning words in time: Towards 6. modular connectionist account of the acquisition of receptive morphology.</title>
<date>1993</date>
<publisher>Draft.</publisher>
<marker>GASSER, 1993</marker>
<rawString>GASSER, MICHAEL, 1993. Learning words in time: Towards 6. modular connectionist account of the acquisition of receptive morphology. Draft.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H HERMANSKY</author>
</authors>
<title>Perceptual linear predictive (pip) analysis of speech.</title>
<date>1990</date>
<journal>J. Acoustical Society of America</journal>
<volume>87</volume>
<marker>HERMANSKY, 1990</marker>
<rawString>HERMANSKY, H. 1990. Perceptual linear predictive (pip) analysis of speech. J. Acoustical Society of America 87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LORI LAMEL</author>
</authors>
<title>The Limsi Dictionary.</title>
<date>1993</date>
<marker>LAMEL, 1993</marker>
<rawString>LAMEL, LORI, 1993. The Limsi Dictionary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>Continuous Speech Recognition</title>
<date>1993</date>
<booktitle>Corpus (WSJ 0). National Institute of Standards and Technology Speech Disc</booktitle>
<pages>11--1</pages>
<contexts>
<context position="3814" citStr="NIST 1993" startWordPosition="572" endWordPosition="573"> on a corpus of lexically transcribed speech using the computational resources of a speech recognition system. This algorithm belongs to the class of techniques we call Exploratory Computational Phonology, which use statistical pattern recognition tools to explore phonological spaces. We describe the details of our probability estimation algorithm and also present the probabilities the system has learned for ten common phonological rules which model reductions and coarticulation effects. Our probabilities are derived from a corpus of 7203 sentences of read speech from the Wall Street Journal (NIST 1993). We also benchmark the probabilities generated by our system against probabilities from phonetically hand-transcribed data, and show a relatively good fit. Finally, we analyze the probability differences between rule use in male ver1 Note that this is true whether phonological theory considers these true phonological rules or rather rules of &quot;phonetic interpretation&quot;. 1 sus female speech, and suggest that the differences erate phone sequences from word orthography as an are caused by differing average rates of speech. additional source of pronunciations. 2 The Algorithm In this section we des</context>
</contexts>
<marker>NIST, 1993</marker>
<rawString>NIST, 1993. Continuous Speech Recognition Corpus (WSJ 0). National Institute of Standards and Technology Speech Disc 11-1.1 to 11-3.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S RENALS</author>
<author>N MORGAN</author>
<author>H BOURLARD</author>
<author>M COHEN</author>
<author>H FRANCO</author>
<author>C WOOTERS</author>
<author>P KOHN</author>
</authors>
<title>Connectionist speech recognition: Status and prospects.</title>
<date>1991</date>
<tech>Technical Report TR-91-070,</tech>
<location>ICSI, Berkeley, CA.</location>
<marker>RENALS, MORGAN, BOURLARD, COHEN, FRANCO, WOOTERS, KOHN, 1991</marker>
<rawString>RENALS, S., N. MORGAN, H. BOURLARD, M. COHEN, H. FRANCO, C. WOOTERS, &amp; P. KOHN. 1991. Connectionist speech recognition: Status and prospects. Technical Report TR-91-070, ICSI, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MICHAEL D RILEY</author>
</authors>
<title>A statistical model for generating pronunciation networks.</title>
<date>1991</date>
<booktitle>In IEEE ICASSP-91,</booktitle>
<pages>737--740</pages>
<marker>RILEY, 1991</marker>
<rawString>RILEY, MICHAEL D. 1991. A statistical model for generating pronunciation networks. In IEEE ICASSP-91, 737-740.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ANTHONY ROBINSON</author>
</authors>
<title>The British English Example Pronunciation Dictionary, v0.1.</title>
<date>1994</date>
<institution>Cambridge University.</institution>
<marker>ROBINSON, 1994</marker>
<rawString>ROBINSON, ANTHONY, 1994. The British English Example Pronunciation Dictionary, v0.1. Cambridge University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GARY TAJCHMAN</author>
<author>ERIC FOSLER</author>
<author>DANIEL JuRAFSKY</author>
</authors>
<title>Building multiple pronunciation models for novel words using exploratory computational phonology.</title>
<date>1995</date>
<note>To appear in Eurospeech95</note>
<marker>TAJCHMAN, FOSLER, JuRAFSKY, 1995</marker>
<rawString>TAJCHMAN, GARY, ERIC FOSLER, &amp; DANIEL JuRAFSKY. 1995. Building multiple pronunciation models for novel words using exploratory computational phonology. To appear in Eurospeech95</rawString>
</citation>
<citation valid="true">
<authors>
<author>TIMIT</author>
</authors>
<title>TIMIT Acoustic-Phonetic Continuous Speech Corpus.</title>
<date>1990</date>
<booktitle>National Institute of Standards and Technology Speech Disc 1-1.1. NTIS Order No.</booktitle>
<pages>91--505065</pages>
<contexts>
<context position="6462" citStr="TIMIT 1990" startWordPosition="996" endWordPosition="997">781 99,279 399,265 LIMSI 32,873 37,936 49,597 PRONLEX 30,353 30,354 81,936 BRITPRON 77,685 85,450 108,834 TTS 77,383 83,297 111,028 Table 1: Pronunciation sources used to build fully expanded lexicon. For further information about these sources please refer to CMU (CMU 1993), LIMSI (Lamel 1993), PRONLEX (COMLEX 1994), BRITPRON (Robinson 1994). A text-to-speech system was used to gen2Although it was not relevant to the experiments described here, our lexicon also included two sources which directly supply surface forms. These were 13,362 handtranscribed pronunciations of 5871 words from TIMIT (TIMIT 1990), and 230 pronunciations of 36 words derived in-house from the OGI Numbers database (Cole et al. 1994). IPA ARPA ICSI IPA ARPA ICSI b b b b° - bc1 d d d d° - dcl g g g go - gcl P P P 130 _ pcl t t t t° - tcl k k k k° - kcl a aa aa s s s w ae ae z z z A ah ah ., sh sh 3 ao ao zh zh E eh eh f f f 3&apos; er er v v v t, ih ih 0 th th i iy iy 8 dh dh o ow ow tf ch ch CO uh uh dz jh jh u uw uw h hh hh aw aw aw ii - hv aY ay ay Y Y Y e ey ey r r r 3&apos; oy oy w w w 1 - el 1 1 1 IP - em m m m q - en n n n a - ax 13 ng ng i - ix f - dx 3&apos; - axr silence h# h# Table 2: Baseform phone set used was the ARPABET. T</context>
</contexts>
<marker>TIMIT, 1990</marker>
<rawString>TIMIT, 1990. TIMIT Acoustic-Phonetic Continuous Speech Corpus. National Institute of Standards and Technology Speech Disc 1-1.1. NTIS Order No. PB91-505065.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MARIA-BARBARA WESENICK</author>
<author>FLORIAN SCHIEL</author>
</authors>
<title>Applying speech verification to a large data base of German to obtain a statistical survey about rules of pronunciation. In</title>
<date>1994</date>
<pages>9--4</pages>
<marker>WESENICK, SCHIEL, 1994</marker>
<rawString>WESENICK, MARIA-BARBARA, &amp; FLORIAN SCHIEL. 1994. Applying speech verification to a large data base of German to obtain a statistical survey about rules of pronunciation. In ICSLP-9-4, 279-282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CHARLES C WOOTERS</author>
</authors>
<title>Lexical Modeling in a Speaker Independent Speech Understanding System. Berkeley:</title>
<date>1993</date>
<booktitle>University of California dissertation. Available as ICSI</booktitle>
<pages>92--062</pages>
<marker>WOOTERS, 1993</marker>
<rawString>WOOTERS, CHARLES C., 1993. Lexical Modeling in a Speaker Independent Speech Understanding System. Berkeley: University of California dissertation. Available as ICSI TR-92-062.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CHUCK WOOTERS</author>
<author>ANDREAS STOLCKE</author>
</authors>
<title>Multiple-pronunciation lexical modeling in a speaker-independent speech understanding system.</title>
<date>1994</date>
<booktitle>In /CSLP-94</booktitle>
<marker>WOOTERS, STOLCKE, 1994</marker>
<rawString>WOOTERS, CHUCK, &amp; ANDREAS STOLCKE. 1994. Multiple-pronunciation lexical modeling in a speaker-independent speech understanding system. In /CSLP-94</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>