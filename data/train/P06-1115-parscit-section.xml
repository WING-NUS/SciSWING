<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.993189">
Using String-Kernels for Learning Semantic Parsers
</title>
<author confidence="0.897918">
Rohit J. Kate
</author>
<affiliation confidence="0.935408666666667">
Department of Computer Sciences
The University of Texas at Austin
1 University Station C0500
</affiliation>
<address confidence="0.682006">
Austin, TX 78712-0233, USA
</address>
<email confidence="0.998732">
rjkate@cs.utexas.edu
</email>
<author confidence="0.390748">
Raymond J. Mooney
</author>
<affiliation confidence="0.678531333333333">
Department of Computer Sciences
The University of Texas at Austin
1 University Station C0500
</affiliation>
<address confidence="0.621417">
Austin, TX 78712-0233, USA
</address>
<email confidence="0.997882">
mooney@cs.utexas.edu
</email>
<sectionHeader confidence="0.994786" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999968">
We present a new approach for mapping
natural language sentences to their for-
mal meaning representations using string-
kernel-based classifiers. Our system learns
these classifiers for every production in the
formal language grammar. Meaning repre-
sentations for novel natural language sen-
tences are obtained by finding the most
probable semantic parse using these string
classifiers. Our experiments on two real-
world data sets show that this approach
compares favorably to other existing sys-
tems and is particularly robust to noise.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999933169491525">
Computational systems that learn to transform nat-
ural language sentences into formal meaning rep-
resentations have important practical applications
in enabling user-friendly natural language com-
munication with computers. However, most of the
research in natural language processing (NLP) has
been focused on lower-level tasks like syntactic
parsing, word-sense disambiguation, information
extraction etc. In this paper, we have considered
the important task of doing deep semantic parsing
to map sentences into their computer-executable
meaning representations.
Previous work on learning semantic parsers
either employ rule-based algorithms (Tang and
Mooney, 2001; Kate et al., 2005), or use sta-
tistical feature-based methods (Ge and Mooney,
2005; Zettlemoyer and Collins, 2005; Wong and
Mooney, 2006). In this paper, we present a
novel kernel-based statistical method for learn-
ing semantic parsers. Kernel methods (Cristianini
and Shawe-Taylor, 2000) are particularly suitable
for semantic parsing because it involves mapping
phrases of natural language (NL) sentences to se-
mantic concepts in a meaning representation lan-
guage (MRL). Given that natural languages are so
flexible, there are various ways in which one can
express the same semantic concept. It is difficult
for rule-based methods or even statistical feature-
based methods to capture the full range of NL con-
texts which map to a semantic concept because
they tend to enumerate these contexts. In contrast,
kernel methods allow a convenient mechanism to
implicitly work with a potentially infinite number
of features which can robustly capture these range
of contexts even when the data is noisy.
Our system, KRISP (Kernel-based Robust In-
terpretation for Semantic Parsing), takes NL sen-
tences paired with their formal meaning represen-
tations as training data. The productions of the for-
mal MRL grammar are treated like semantic con-
cepts. For each of these productions, a Support-
Vector Machine (SVM) (Cristianini and Shawe-
Taylor, 2000) classifier is trained using string sim-
ilarity as the kernel (Lodhi et al., 2002). Each
classifier then estimates the probability of the pro-
duction covering different substrings of the sen-
tence. This information is used to compositionally
build a complete meaning representation (MR) of
the sentence.
Some of the previous work on semantic pars-
ing has focused on fairly simple domains, primar-
ily, ATIS (Air Travel Information Service) (Price,
1990) whose semantic analysis is equivalent to fill-
ing a single semantic frame (Miller et al., 1996;
Popescu et al., 2004). In this paper, we have
tested KRISP on two real-world domains in which
meaning representations are more complex with
richer predicates and nested structures. Our exper-
iments demonstrate that KRISP compares favor-
</bodyText>
<page confidence="0.969871">
913
</page>
<note confidence="0.8381195">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 913–920,
Sydney, July 2006. c�2006 Association for Computational Linguistics
NL: “If the ball is in our goal area then our player 1 should
intercept it.”
CLANG: ((bpos (goal-area our))
(do our {1} intercept))
Figure 1: An example of an NL advice and its
CLANG MR.
NL: “Which rivers run through the states bordering Texas?”
Functional query language:
</note>
<figure confidence="0.691416875">
answer(traverse(next to(stateid(‘texas’))))
Parse tree of the MR in functional query language:
ANSWER
answer RIVER
ably to other existing systems and is particularly
robust to noise.
TRAVERSE STATE
traverse
</figure>
<sectionHeader confidence="0.726746" genericHeader="method">
NEXT TO STATE
2 Semantic Parsing
</sectionHeader>
<bodyText confidence="0.999822641025641">
We call the process of mapping natural language
(NL) utterances into their computer-executable
meaning representations (MRs) as semantic pars-
ing. These MRs are expressed in formal languages
which we call meaning representation languages
(MRLs). We assume that all MRLs have deter-
ministic context free grammars, which is true for
almost all computer languages. This ensures that
every MR will have a unique parse tree. A learn-
ing system for semantic parsing is given a training
corpus of NL sentences paired with their respec-
tive MRs from which it has to induce a semantic
parser which can map novel NL sentences to their
correct MRs.
Figure 1 shows an example of an NL sentence
and its MR from the CLANG domain. CLANG
(Chen et al., 2003) is the standard formal coach
language in which coaching advice is given to soc-
cer agents which compete on a simulated soccer
field in the RoboCup 1 Coach Competition. In the
MR of the example, bpos stands for “ball posi-
tion”.
The second domain we have considered is the
GEOQUERY domain (Zelle and Mooney, 1996)
which is a query language for a small database of
about 800 U.S. geographical facts. Figure 2 shows
an NL query and its MR form in a functional query
language. The parse of the functional query lan-
guage is also shown along with the involved pro-
ductions. This example is also used later to illus-
trate how our system does semantic parsing. The
MR in the functional query language can be read
as if processing a list which gets modified by vari-
ous functions. From the innermost expression go-
ing outwards it means: the state of Texas, the list
containing all the states next to the state of Texas
and the list of all the rivers which flow through
these states. This list is finally returned as the an-
swer.
</bodyText>
<footnote confidence="0.81075">
1http://www.robocup.org/
</footnote>
<equation confidence="0.848762714285714">
next to STATEID
stateid ‘texas’
Productions:
ANSWER — answer(RIVER) RIVER — TRAVERSE(STATE)
STATE — NEXT TO(STATE) STATE — STATEID
TRAVERSE — traverse NEXT TO — next to
STATEID — stateid(‘texas’)
</equation>
<figureCaption confidence="0.647969">
Figure 2: An example of an NL query and its MR
in a functional query language with its parse tree.
</figureCaption>
<bodyText confidence="0.9997908">
KRISP does semantic parsing using the notion
of a semantic derivation of an NL sentence. In
the following subsections, we define the seman-
tic derivation of an NL sentence and its probabil-
ity. The task of semantic parsing then is to find
the most probable semantic derivation of an NL
sentence. In section 3, we describe how KRISP
learns the string classifiers that are used to obtain
the probabilities needed in finding the most prob-
able semantic derivation.
</bodyText>
<subsectionHeader confidence="0.988287">
2.1 Semantic Derivation
</subsectionHeader>
<bodyText confidence="0.999782">
We define a semantic derivation, D, of an NL sen-
tence, s, as a parse tree of an MR (not necessarily
the correct MR) such that each node of the parse
tree also contains a substring of the sentence in
addition to a production. We denote nodes of the
derivation tree by tuples (7r, [i..j]), where 7r is its
production and [i..j] stands for the substring s[i..j]
of s (i.e. the substring from the ith word to the jth
word), and we say that the node or its production
covers the substring s[i..j]. The substrings cov-
ered by the children of a node are not allowed to
overlap, and the substring covered by the parent
must be the concatenation of the substrings cov-
ered by its children. Figure 3 shows a semantic
derivation of the NL sentence and the MR parse
which were shown in figure 2. The words are num-
bered according to their position in the sentence.
Instead of non-terminals, productions are shown
in the nodes to emphasize the role of productions
in semantic derivations.
Sometimes, the children of an MR parse tree
</bodyText>
<page confidence="0.997236">
914
</page>
<table confidence="0.750983333333333">
(ANSWER → answer(RIVER), [1..9])
(RIVER → TRAVERSE(STATE), [1..9])
(TRAVERSE →traverse, [1..4]) (STATE → NEXT TO(STATE), [5..9])
which1 rivers2 rung through4
(NEXT TO→ next to, [5..7])
they states� bordering7
</table>
<figure confidence="0.740886666666667">
(STATE → STATEID, [8..9])
(STATEID → stateid ‘texas’, [8..9])
Texasg ?9
</figure>
<figureCaption confidence="0.9951515">
Figure 3: Semantic derivation of the NL sentence “Which rivers run through the states bordering Texas?”
which gives MR as answer(traverse(next to(stateid(texas)))).
</figureCaption>
<bodyText confidence="0.999857523809524">
node may not be in the same order as are the sub-
strings of the sentence they should cover in a se-
mantic derivation. For example, if the sentence
was “Through the states that border Texas which
rivers run?”, which has the same MR as the sen-
tence in figure 3, then the order of the children of
the node “RIVER —* TRAVERSE(STATE)” would
need to be reversed. To accommodate this, a se-
mantic derivation tree is allowed to contain MR
parse tree nodes in which the children have been
permuted.
Note that given a semantic derivation of an NL
sentence, it is trivial to obtain the corresponding
MR simply as the string generated by the parse.
Since children nodes may be permuted, this step
also needs to permute them back to the way they
should be according to the MRL productions. If a
semantic derivation gives the correct MR of the
NL sentence, then we call it a correct semantic
derivation otherwise it is an incorrect semantic
derivation.
</bodyText>
<subsectionHeader confidence="0.999134">
2.2 Most Probable Semantic Derivation
</subsectionHeader>
<bodyText confidence="0.9996461">
Let Pπ(u) denote the probability that a production
7r of the MRL grammar covers the NL substring
u. In other words, the NL substring u expresses
the semantic concept of a production 7r with prob-
ability Pπ(u). In the next subsection we will de-
scribe how KRISP obtains these probabilities using
string-kernel based SVM classifiers. Assuming
these probabilities are independent of each other,
the probability of a semantic derivation D of a sen-
tence s is then:
</bodyText>
<equation confidence="0.989172">
P(D) = � Pπ(s[i..j])
(π,[i..j])ED
</equation>
<bodyText confidence="0.999972333333333">
The task of the semantic parser is to find the
most probable derivation of a sentence s. This
task can be recursively performed using the no-
tion of a partial derivation En,s[i..j], which stands
for a subtree of a semantic derivation tree with n
as the left-hand-side (LHS) non-terminal of the
root production and which covers s from index
i to j. For example, the subtree rooted at the
node “(STATE —* NEXT TO(STATE),[5..9])” in
the derivation shown in figure 3 is a partial deriva-
tion which would be denoted as ESTATE,s[5..9].
Note that the derivation D of sentence s is then
simply Estart,s[1..|s|], where start is the start sym-
bol of the MRL’s context free grammar, G.
Our procedure to find the most probable par-
</bodyText>
<equation confidence="0.9824295">
7nakeTree( arg max
?r=n--+nl..nt EG,
(p1, .., pt) E
partition(s[i..j], t)
</equation>
<bodyText confidence="0.908121846153846">
where partition(s[i..j], t) is a function which re-
turns the set of all partitions of s[i..j] with t el-
ements including their permutations. A parti-
tion of a substring s[i..j] with t elements is a
t−tuple containing t non-overlapping substrings
of s[i..j] which give s[i..j] when concatenated.
For example, (“the states bordering”, “Texas ?”)
is a partition of the substring “the states bor-
dering Texas ?” with 2 elements. The proce-
dure makeTree(7r, (p1, .., pt)) constructs a partial
derivation tree by making 7r as its root production
and making the most probable partial derivation
trees found through the recursion as children sub-
trees which cover the substrings according to the
partition (p1, .., pt).
The most probable partial derivation En,s[i..j]
is found using the above equation by trying all
productions 7r = n —* n1..nt in G which have
tial deri
E�
vation
n,s[i..j] considers all possible sub-
trees whose root production has n as its LHS non-
terminal and which cover s from index i to j.
Mathematically, the most probable partial deriva-
tion E� n,s[i..j]is recursively defined as:
</bodyText>
<equation confidence="0.879609333333333">
En,s[i..j] =
�(P�(s[i..j]) P(F&amp;apos;nk,nk )))
k=1..t
</equation>
<page confidence="0.984587">
915
</page>
<bodyText confidence="0.999302865384615">
n as the LHS, and all partitions with t elements
of the substring s[i..j] (n1 to nt are right-hand-
side (RHS) non-terminals of 7r, terminals do not
play any role in this process and are not shown
for simplicity). The most probable partial deriva-
tion E�STATE,9[5_9] for the sentence shown in fig-
ure 3 will be found by trying all the productions
in the grammar with STATE as the LHS, for ex-
ample, one of them being “STATE —* NEXT TO
STATE”. Then for this sample production, all parti-
tions, (p1, p2), of the substring s[5..9] with two el-
ements will be considered, and the most probable
derivations ENEXT TO,p, and ESTATE,p, will be
found recursively. The recursion reaches base
cases when the productions which have n on the
LHS do not have any non-terminal on the RHS or
when the substring s[i..j] becomes smaller than
the length t.
According to the equation, a production 7r E G
and a partition (p1, .., pt) E partition(s[i..j], t)
will be selected in constructing the most probable
partial derivation. These will be the ones which
maximize the product of the probability of 7r cov-
ering the substring s[i..j] with the product of prob-
abilities of all the recursively found most proba-
ble partial derivations consistent with the partition
(p1, ..,pt).
A naive implementation of the above recursion
is computationally expensive, but by suitably ex-
tending the well known Earley’s context-free pars-
ing algorithm (Earley, 1970), it can be imple-
mented efficiently. The above task has some re-
semblance to probabilistic context-free grammar
(PCFG) parsing for which efficient algorithms are
available (Stolcke, 1995), but we note that our task
of finding the most probable semantic derivation
differs from PCFG parsing in two important ways.
First, the probability of a production is not inde-
pendent of the sentence but depends on which sub-
string of the sentence it covers, and second, the
leaves of the tree are not individual terminals of
the grammar but are substrings of words of the NL
sentence. The extensions needed for Earley’s al-
gorithm are straightforward and are described in
detail in (Kate, 2005) but due to space limitation
we do not describe them here. Our extended Ear-
ley’s algorithm does a beam search and attempts
to find the w (a parameter) most probable semantic
derivations of an NL sentence s using the probabil-
ities P,(s[i..j]). To make this search faster, it uses
a threshold, 0, to prune low probability derivation
trees.
</bodyText>
<sectionHeader confidence="0.931629" genericHeader="method">
3 KRISP’s Training Algorithm
</sectionHeader>
<bodyText confidence="0.9987291875">
In this section, we describe how KRISP learns
the classifiers which give the probabilities P,(u)
needed for semantic parsing as described in the
previous section. Given the training corpus of
NL sentences paired with their MRs {(si, mi)|i =
1..N}, KRISP first parses the MRs using the MRL
grammar, G. We represent the parse of MR, mi,
by parse(mi).
Figure 4 shows pseudo-code for KRISP’s train-
ing algorithm. KRISP learns a semantic parser it-
eratively, each iteration improving upon the parser
learned in the previous iteration. In each itera-
tion, for every production 7r of G, KRISP collects
positive and negative example sets. In the first
iteration, the set P(7r) of positive examples for
production 7r contains all sentences, si, such that
parse(mi) uses the production 7r. The set of nega-
tive examples, N(7r), for production 7r includes all
of the remaining training sentences. Using these
positive and negative examples, an SVM classi-
fier 2, C, is trained for each production 7r using
a normalized string subsequence kernel. Follow-
ing the framework of Lodhi et al. (2002), we de-
fine a kernel between two strings as the number of
common subsequences they share. One difference,
however, is that their strings are over characters
while our strings are over words. The more the
two strings share, the greater the similarity score
will be.
Normally, SVM classifiers only predict the class
of the test example but one can obtain class proba-
bility estimates by mapping the distance of the ex-
ample from the SVM’s separating hyperplane to
the range [0,1] using a learned sigmoid function
(Platt, 1999). The classifier C, then gives us the
probabilities P,(u). We represent the set of these
classifiers by C = IC,|7r E G}.
Next, using these classifiers, the extended
Earley’s algorithm, which we call EX-
TENDED EARLEY in the pseudo-code, is invoked
to obtain the w best semantic derivations for each
of the training sentences. The procedure getMR
returns the MR for a semantic derivation. At this
point, for many training sentences, the resulting
most-probable semantic derivation may not give
the correct MR. Hence, next, the system collects
more refined positive and negative examples
to improve the result in the next iteration. It
</bodyText>
<footnote confidence="0.985823">
2We use the LIBSVM package available at: http://
www.csie.ntu.edu.tw/˜cjlin/libsvm/
</footnote>
<page confidence="0.981471">
916
</page>
<table confidence="0.841692857142857">
function TRAIN KRISP(training corpus {(si, mi)|i = 1..N}, MRL grammar G)
for each 7r E G // collect positive and negative examples for the first iteration
for i = 1 to N do
if 7r is used in parse(mi) then
include si in P(7r)
else include si in N (7r)
for iteration = 1 to MAX ITR do
for each 7r E G do
Ce = trainSV M(P(7r), N(7r)) // SVM training
for each 7r E G P(7r) = 4) // empty the positive examples, accumulate negatives though
for i = 1 to N do
D =EXTENDED EARLEY(si, G, P) // obtain best derivations
if 6∃ d E D such that parse(mi) = getMR(d) then
D = D U EXTENDED EARLEY CORRECT(si, G, P, mi) // if no correct derivation then force to find one
d* = arg maxdED&amp;getMR(d)=parse(mi) P(d)
COLLECT POSITIVES(d*) // collect positives from maximum probability correct derivation
for each d E D do
if P(d) &amp;gt; P(d*) and getMR(d) =� parse(mi) then
// collect negatives from incorrect derivation with larger probability than the correct one
COLLECT NEGATIVES(d, d*)
return classifiers C = {Ce|7r E G}
</table>
<figureCaption confidence="0.999005">
Figure 4: KRISP’s training algorithm
</figureCaption>
<bodyText confidence="0.999965027777778">
is also possible that for some sentences, none
of the obtained w derivations give the correct
MR. But as will be described shortly, the most
probable derivation which gives the correct MR is
needed to collect positive and negative examples
for the next iteration. Hence in these cases, a
version of the extended Earley’s algorithm, EX-
TENDED EARLEY CORRECT, is invoked which
also takes the correct MR as an argument and
returns the best w derivations it finds, all of
which give the correct MR. This is easily done by
making sure all subtrees derived in the process are
present in the parse of the correct MR.
From these derivations, positive and negative
examples are collected for the next iteration. Pos-
itive examples are collected from the most prob-
able derivation which gives the correct MR, fig-
ure 3 showed an example of a derivation which
gives the correct MR. At each node in such a
derivation, the substring covered is taken as a pos-
itive example for its production. Negative exam-
ples are collected from those derivations whose
probability is higher than the most probable cor-
rect derivation but which do not give the cor-
rect MR. Figure 5 shows an example of an in-
correct derivation. Here the function “next to”
is missing from the MR it produces. The fol-
lowing procedure is used to collect negative ex-
amples from incorrect derivations. The incorrect
derivation and the most probable correct deriva-
tion are traversed simultaneously starting from the
root using breadth-first traversal. The first nodes
where their productions differ is detected, and all
of the words covered by the these nodes (in both
derivations) are marked. In the correct and incor-
rect derivations shown in figures 3 and 5 respec-
tively, the first nodes where the productions differ
are “(STATE —* NEXT TO(STATE), [5..9])” and
“(STATE -* STATEID, [8..9])”. Hence, the union
of words covered by them: 5 to 9 (“the states
bordering Texas?”), will be marked. For each
of these marked words, the procedure considers
all of the productions which cover it in the two
derivations. The nodes of the productions which
cover a marked word in the incorrect derivation
but not in the correct derivation are used to col-
lect negative examples. In the example, the node
“(TRAVERSE-*traverse,[1..7])” will be used
to collect a negative example (i.e. the words 1
to 7 ‘‘which rivers run through the states border-
ing” will be a negative example for the produc-
tion TRAVERSE—*traverse) because the pro-
duction covers the marked words “the”, “states”
and “bordering” in the incorrect derivation but
not in the correct derivation. With this as a neg-
ative example, hopefully in the next iteration, the
probability of this derivation will decrease signif-
icantly and drop below the probability of the cor-
rect derivation.
In each iteration, the positive examples from
the previous iteration are first removed so that
new positive examples which lead to better cor-
rect derivations can take their place. However,
negative examples are accumulated across iter-
ations for better accuracy because negative ex-
amples from each iteration only lead to incor-
rect derivations and it is always good to include
them. To further increase the number of nega-
tive examples, every positive example for a pro-
duction is also included as a negative example for
all the other productions having the same LHS.
After a specified number of MAX ITR iterations,
</bodyText>
<page confidence="0.944048">
917
</page>
<equation confidence="0.7450766">
(ANSWER— answer(RIVER),[1..9])
(RIVER — TRAVERSE(STATE), [1..9])
(STATE — STATEID, [8..9])
(STATEID — stateid texas, [8..9])
Texas8 ?9
</equation>
<figureCaption confidence="0.964932">
Figure 5: An incorrect semantic derivation of the NL sentence ”Which rivers run through the states
bordering Texas?” which gives the incorrect MR answer(traverse(stateid(texas))).
</figureCaption>
<equation confidence="0.8501665">
(TRAVERSE—traverse, [1..7])
Which1 rivers2 rung throughq they statesg bordering?
</equation>
<bodyText confidence="0.999950789473684">
the trained classifiers from the last iteration are
returned. Testing involves using these classifiers
to generate the most probable derivation of a test
sentence as described in the subsection 2.2, and
returning its MR.
The MRL grammar may contain productions
corresponding to constants of the domain, for e.g.,
state names like “STATEID —* ‘texas’”, or river
names like “RIVERID —* ‘colorado’” etc. Our
system allows the user to specify such produc-
tions as constant productions giving the NL sub-
strings, called constant substrings, which directly
relate to them. For example, the user may give
“Texas” as the constant substring for the produc-
tion “STATEID —* ‘texas’. Then KRISP does
not learn classifiers for these constant productions
and instead decides if they cover a substring of the
sentence or not by matching it with the provided
constant substrings.
</bodyText>
<sectionHeader confidence="0.999008" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.968058">
4.1 Methodology
</subsectionHeader>
<bodyText confidence="0.99995324590164">
KRISP was evaluated on CLANG and GEOQUERY
domains as described in section 2. The CLANG
corpus was built by randomly selecting 300 pieces
of coaching advice from the log files of the 2003
RoboCup Coach Competition. These formal ad-
vice instructions were manually translated into
English (Kate et al., 2005). The GEOQUERY cor-
pus contains 880 English queries collected from
undergraduates and from real users of a web-based
interface (Tang and Mooney, 2001). These were
manually translated into their MRs. The average
length of an NL sentence in the CLANG corpus
is 22.52 words while in the GEOQUERY corpus it
is 7.48 words, which indicates that CLANG is the
harder corpus. The average length of the MRs is
13.42 tokens in the CLANG corpus while it is 6.46
tokens in the GEOQUERY corpus.
KRISP was evaluated using standard 10-fold
cross validation. For every test sentence, only the
best MR corresponding to the most probable se-
mantic derivation is considered for evaluation, and
its probability is taken as the system’s confidence
in that MR. Since KRISP uses a threshold, 0, to
prune low probability derivation trees, it some-
times may fail to return any MR for a test sen-
tence. We computed the number of test sentences
for which KRISP produced MRs, and the number
of these MRs that were correct. For CLANG, an
output MR is considered correct if and only if it
exactly matches the correct MR. For GEOQUERY,
an output MR is considered correct if and only if
the resulting query retrieves the same answer as
the correct MR when submitted to the database.
Performance was measured in terms of precision
(the percentage of generated MRs that were cor-
rect) and recall (the percentage of all sentences for
which correct MRs were obtained).
In our experiments, the threshold 0 was fixed
to 0.05 and the beam size w was 20. These pa-
rameters were found through pilot experiments.
The maximum number of iterations (MAX ITR) re-
quired was only 3, beyond this we found that the
system only overfits the training corpus.
We compared our system’s performance with
the following existing systems: the string and tree
versions of SILT (Kate et al., 2005), a system that
learns transformation rules relating NL phrases
to MRL expressions; WASP (Wong and Mooney,
2006), a system that learns transformation rules
using statistical machine translation techniques;
SCISSOR (Ge and Mooney, 2005), a system that
learns an integrated syntactic-semantic parser; and
CHILL (Tang and Mooney, 2001) an ILP-based
semantic parser. We also compared with the
CCG-based semantic parser by Zettlemoyer et al.
(2005), but their results are available only for the
GEO880 corpus and their experimental set-up is
also different from ours. Like KRISP, WASP and
SCISSOR also give confidences to the MRs they
generate which are used to plot precision-recall
curves by measuring precisions and recalls at vari-
</bodyText>
<page confidence="0.989376">
918
</page>
<figure confidence="0.99832875">
0 10 20 30 40 50 60 70 80 90 100
Recall
0 10 20 30 40 50 60 70 80 90 100
Recall
</figure>
<figureCaption confidence="0.99996">
Figure 7: Results on the GEOQUERY corpus.
</figureCaption>
<bodyText confidence="0.993871333333333">
ous confidence levels. The results of the other sys-
tems are shown as points on the precision-recall
graph.
</bodyText>
<subsectionHeader confidence="0.647988">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.99761165">
Figure 6 shows the results on the CLANG cor-
pus. KRISP performs better than either version
of SILT and performs comparable to WASP. Al-
though SCISSOR gives less precision at lower re-
call values, it gives much higher maximum recall.
However, we note that SCISSOR requires more su-
pervision for the training corpus in the form of se-
mantically annotated syntactic parse trees for the
training sentences. CHILL could not be run be-
yond 160 training examples because its Prolog im-
plementation runs out of memory. For 160 training
examples it gave 49.2% precision with 12.67% re-
call.
Figure 7 shows the results on the GEOQUERY
corpus. KRISP achieves higher precisions than
WASP on this corpus. Overall, the results show
that KRISP performs better than deterministic
rule-based semantic parsers like CHILL and SILT
and performs comparable to other statistical se-
mantic parsers like WASP and SCISSOR.
</bodyText>
<subsectionHeader confidence="0.8934565">
4.3 Experiments with Other Natural
Languages
</subsectionHeader>
<bodyText confidence="0.999291">
We have translations of a subset of the GEOQUERY
corpus with 250 examples (GEO250 corpus) in
</bodyText>
<figure confidence="0.983953">
0 10 20 30 40 50 60 70 80 90 100
Recall
</figure>
<figureCaption confidence="0.909191">
Figure 8: Results of KRISP on the GEO250 corpus
for different natural languages.
</figureCaption>
<bodyText confidence="0.953111333333333">
three other natural languages: Spanish, Turkish
and Japanese. Since KRISP’s learning algorithm
does not use any natural language specific knowl-
edge, it is directly applicable to other natural lan-
guages. Figure 8 shows that KRISP performs com-
petently on other languages as well.
</bodyText>
<subsectionHeader confidence="0.989002">
4.4 Experiments with Noisy NL Sentences
</subsectionHeader>
<bodyText confidence="0.99993264516129">
Any real world application in which semantic
parsers would be used to interpret natural language
of a user is likely to face noise in the input. If the
user is interacting through spontaneous speech and
the input to the semantic parser is coming form
the output of a speech recognition system then
there are many ways in which noise could creep
in the NL sentences: interjections (like um’s and
ah’s), environment noise (like door slams, phone
rings etc.), out-of-domain words, grammatically
ill-formed utterances etc. (Zue and Glass, 2000).
As opposed to the other systems, KRISP’s string-
kernel-based semantic parsing does not use hard-
matching rules and should be thus more flexible
and robust to noise. We tested this hypothesis by
running experiments on data which was artificially
corrupted with simulated speech recognition er-
rors.
The interjections, environment noise etc. are
likely to be recognized as real words by a speech
recognizer. To simulate this, after every word in
a sentence, with some probability Padd, an ex-
tra word is added which is chosen with proba-
bility proportional to its word frequency found in
the British National Corpus (BNC), a good rep-
resentative sample of English. A speech recog-
nizer may sometimes completely fail to detect
words, so with a probability of Pd,p a word is
sometimes dropped. A speech recognizer could
also introduce noise by confusing a word with a
high frequency phonetically close word. We sim-
</bodyText>
<figure confidence="0.9809176">
KRISP
WASP
SCISSOR
SILT-tree
SILT-string
</figure>
<figureCaption confidence="0.981741">
Figure 6: Results on the CLANG corpus.
</figureCaption>
<figure confidence="0.990106764705883">
Precision
100
90
80
50
70
60
KRISP
WASP
SCISSOR
SILT-tree
SILT-string
CHILL
Zettlemoyer et al. (2005)
English
Japanese
Spanish
Turkish
Precision 100
90
80
70
60
50
Precision 100
90
80
70
60
50
919
F-measure
0 1 2 3 4 5
Noise level
</figure>
<figureCaption confidence="0.91574">
Figure 9: Results on the CLANG corpus with in-
creasing amounts of noise in the test sentences.
</figureCaption>
<bodyText confidence="0.999963888888889">
ulate this type of noise by substituting a word in
the corpus by another word, w, with probability
ped(,)*P(w), where p is a parameter, ed(w) is w’s
edit distance (Levenshtein, 1966) from the original
word and P(w) is w’s probability proportional to
its word frequency. The edit distance which calcu-
lates closeness between words is character-based
rather than based on phonetics, but this should not
make a significant difference in the experimental
results.
Figure 9 shows the results on the CLANG cor-
pus with increasing amounts of noise, from level
0 to level 4. The noise level 0 corresponds to no
noise. The noise parameters, Padd and Pdrop, were
varied uniformly from being 0 at level 0 and 0.1 at
level 4, and the parameter p was varied uniformly
from being 0 at level 0 and 0.01 at level 4. We
are showing the best F-measure (harmonic mean
of precision and recall) for each system at differ-
ent noise levels. As can be seen, KRISP’s perfor-
mance degrades gracefully in the presence of noise
while other systems’ performance degrade much
faster, thus verifying our hypothesis. In this exper-
iment, only the test sentences were corrupted, we
get qualitatively similar results when both training
and test sentences are corrupted. The results are
also similar on the GEOQUERY corpus.
</bodyText>
<sectionHeader confidence="0.999735" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999892818181818">
We presented a new kernel-based approach to
learn semantic parsers. SVM classifiers based on
string subsequence kernels are trained for each of
the productions in the meaning representation lan-
guage. These classifiers are then used to com-
positionally build complete meaning representa-
tions of natural language sentences. We evaluated
our system on two real-world corpora. The re-
sults showed that our system compares favorably
to other existing systems and is particularly robust
to noise.
</bodyText>
<sectionHeader confidence="0.998662" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.983144666666667">
This research was supported by Defense Ad-
vanced Research Projects Agency under grant
HR0011-04-1-0007.
</bodyText>
<sectionHeader confidence="0.998668" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998904857142858">
Mao Chen et al. 2003. Users manual: RoboCup soccer server manual for soc-
cer server version 7.07 and later. Available at http://sourceforge.
net/projects/sserver/.
Nello Cristianini and John Shawe-Taylor. 2000. An Introduction to Support
Vector Machines and Other Kernel-based Learning Methods. Cambridge
University Press.
Jay Earley. 1970. An efficient context-free parsing algorithm. Communica-
tions of the Association for Computing Machinery, 6(8):451–455.
R. Ge and R. J. Mooney. 2005. A statistical semantic parser that integrates
syntax and semantics. In Proc. of 9th Conf. on Computational Natural
Language Learning (CoNLL-2005), pages 9–16, Ann Arbor, MI, July.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learning to transform natural
to formal languages. In Proc. of 20th Natl. Conf. on Artificial Intelligence
(AAAI-2005), pages 1062–1068, Pittsburgh, PA, July.
Rohit J. Kate. 2005. A kernel-based approach to learning semantic parsers.
Technical Report UT-AI-05-326, Artificial Intelligence Lab, University of
Texas at Austin, Austin, TX, November.
V. I. Levenshtein. 1966. Binary codes capable of correcting insertions and
reversals. Soviet Physics Doklady, 10(8):707–710, February.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris
Watkins. 2002. Text classification using string kernels. Journal of Ma-
chine Learning Research, 2:419–444.
Scott Miller, David Stallard, Robert Bobrow, and Richard Schwartz. 1996. A
fully statistical approach to natural language interfaces. In Proc. of the 34th
Annual Meeting of the Association for Computational Linguistics (ACL-
96), pages 55–61, Santa Cruz, CA.
John C. Platt. 1999. Probabilistic outputs for support vector machines and
comparisons to regularized likelihood methods. In Alexander J. Smola, Pe-
ter Bartlett, Bernhard Sch¨olkopf, and Dale Schuurmans, editors, Advances
in Large Margin Classifiers, pages 185–208. MIT Press.
Ana-Maria Popescu, Alex Armanasu, Oren Etzioni, David Ko, and Alexander
Yates. 2004. Modern natural language interfaces to databases: Composing
statistical parsing with semantic tractability. In Proc. of 20th Intl. Conf. on
Computational Linguistics (COLING-04), Geneva, Switzerland, August.
Patti J. Price. 1990. Evaluation of spoken language systems: The ATIS do-
main. In Proc. of 3rd DARPA Speech and Natural Language Workshop,
pages 91–95, June.
Andreas Stolcke. 1995. An efficient probabilistic context-free parsing al-
gorithm that computes prefix probabilities. Computational Linguistics,
21(2):165–201.
L. R. Tang and R. J. Mooney. 2001. Using multiple clause constructors in
inductive logic programming for semantic parsing. In Proc. of the 12th
European Conf. on Machine Learning, pages 466–477, Freiburg, Germany.
Yuk Wah Wong and Raymond J. Mooney. 2006. Learning for semantic pars-
ing with statistical machine translation. In Proc. ofHuman Language Tech-
nology Conf. /North American Association for Computational Linguistics
Annual Meeting (HLT/NAACL-2006), New York City, NY. To appear.
John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database
queries using inductive logic programming. In Proc. of 13th Natl. Conf. on
Artificial Intelligence (AAAI-96), pages 1050–1055, Portland, OR, August.
Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to
logical form: Structured classification with probabilistic categorial gram-
mars. In Proc. of 21th Conf. on Uncertainty in Artificial Intelligence (UAI-
2005), Edinburgh, Scotland, July.
Victor W. Zue and James R. Glass. 2000. Conversational interfaces: Advances
and challenges. In Proc. of the IEEE, volume 88(8), pages 1166–1180.
</reference>
<figure confidence="0.995717333333333">
100
80
60
40
20
0
KRISP
WASP
SCISSOR
</figure>
<page confidence="0.917537">
920
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.666406">
<title confidence="0.99992">Using String-Kernels for Learning Semantic Parsers</title>
<author confidence="0.999698">J Rohit</author>
<affiliation confidence="0.939081">Department of Computer The University of Texas at 1 University Station Austin, TX 78712-0233,</affiliation>
<email confidence="0.999521">rjkate@cs.utexas.edu</email>
<author confidence="0.999883">J Raymond</author>
<affiliation confidence="0.984328">Department of Computer The University of Texas at 1 University Station Austin, TX 78712-0233,</affiliation>
<email confidence="0.99979">mooney@cs.utexas.edu</email>
<abstract confidence="0.995723785714286">We present a new approach for mapping natural language sentences to their formal meaning representations using stringkernel-based classifiers. Our system learns these classifiers for every production in the formal language grammar. Meaning representations for novel natural language sentences are obtained by finding the most probable semantic parse using these string classifiers. Our experiments on two realworld data sets show that this approach compares favorably to other existing systems and is particularly robust to noise.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mao Chen</author>
</authors>
<title>Users manual: RoboCup soccer server manual for soccer server version 7.07 and later. Available at http://sourceforge.</title>
<date>2003</date>
<note>net/projects/sserver/.</note>
<marker>Chen, 2003</marker>
<rawString>Mao Chen et al. 2003. Users manual: RoboCup soccer server manual for soccer server version 7.07 and later. Available at http://sourceforge. net/projects/sserver/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nello Cristianini</author>
<author>John Shawe-Taylor</author>
</authors>
<title>An Introduction to Support Vector Machines and Other Kernel-based Learning Methods.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1870" citStr="Cristianini and Shawe-Taylor, 2000" startWordPosition="260" endWordPosition="263">vel tasks like syntactic parsing, word-sense disambiguation, information extraction etc. In this paper, we have considered the important task of doing deep semantic parsing to map sentences into their computer-executable meaning representations. Previous work on learning semantic parsers either employ rule-based algorithms (Tang and Mooney, 2001; Kate et al., 2005), or use statistical feature-based methods (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006). In this paper, we present a novel kernel-based statistical method for learning semantic parsers. Kernel methods (Cristianini and Shawe-Taylor, 2000) are particularly suitable for semantic parsing because it involves mapping phrases of natural language (NL) sentences to semantic concepts in a meaning representation language (MRL). Given that natural languages are so flexible, there are various ways in which one can express the same semantic concept. It is difficult for rule-based methods or even statistical featurebased methods to capture the full range of NL contexts which map to a semantic concept because they tend to enumerate these contexts. In contrast, kernel methods allow a convenient mechanism to implicitly work with a potentially </context>
</contexts>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>Nello Cristianini and John Shawe-Taylor. 2000. An Introduction to Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the Association for Computing Machinery,</journal>
<volume>6</volume>
<issue>8</issue>
<contexts>
<context position="13241" citStr="Earley, 1970" startWordPosition="2172" endWordPosition="2173">aller than the length t. According to the equation, a production 7r E G and a partition (p1, .., pt) E partition(s[i..j], t) will be selected in constructing the most probable partial derivation. These will be the ones which maximize the product of the probability of 7r covering the substring s[i..j] with the product of probabilities of all the recursively found most probable partial derivations consistent with the partition (p1, ..,pt). A naive implementation of the above recursion is computationally expensive, but by suitably extending the well known Earley’s context-free parsing algorithm (Earley, 1970), it can be implemented efficiently. The above task has some resemblance to probabilistic context-free grammar (PCFG) parsing for which efficient algorithms are available (Stolcke, 1995), but we note that our task of finding the most probable semantic derivation differs from PCFG parsing in two important ways. First, the probability of a production is not independent of the sentence but depends on which substring of the sentence it covers, and second, the leaves of the tree are not individual terminals of the grammar but are substrings of words of the NL sentence. The extensions needed for Ear</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. 1970. An efficient context-free parsing algorithm. Communications of the Association for Computing Machinery, 6(8):451–455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ge</author>
<author>R J Mooney</author>
</authors>
<title>A statistical semantic parser that integrates syntax and semantics.</title>
<date>2005</date>
<booktitle>In Proc. of 9th Conf. on Computational Natural Language Learning (CoNLL-2005),</booktitle>
<pages>9--16</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="1665" citStr="Ge and Mooney, 2005" startWordPosition="231" endWordPosition="234">actical applications in enabling user-friendly natural language communication with computers. However, most of the research in natural language processing (NLP) has been focused on lower-level tasks like syntactic parsing, word-sense disambiguation, information extraction etc. In this paper, we have considered the important task of doing deep semantic parsing to map sentences into their computer-executable meaning representations. Previous work on learning semantic parsers either employ rule-based algorithms (Tang and Mooney, 2001; Kate et al., 2005), or use statistical feature-based methods (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006). In this paper, we present a novel kernel-based statistical method for learning semantic parsers. Kernel methods (Cristianini and Shawe-Taylor, 2000) are particularly suitable for semantic parsing because it involves mapping phrases of natural language (NL) sentences to semantic concepts in a meaning representation language (MRL). Given that natural languages are so flexible, there are various ways in which one can express the same semantic concept. It is difficult for rule-based methods or even statistical featurebased methods to capture</context>
<context position="24685" citStr="Ge and Mooney, 2005" startWordPosition="4072" endWordPosition="4075">, the threshold 0 was fixed to 0.05 and the beam size w was 20. These parameters were found through pilot experiments. The maximum number of iterations (MAX ITR) required was only 3, beyond this we found that the system only overfits the training corpus. We compared our system’s performance with the following existing systems: the string and tree versions of SILT (Kate et al., 2005), a system that learns transformation rules relating NL phrases to MRL expressions; WASP (Wong and Mooney, 2006), a system that learns transformation rules using statistical machine translation techniques; SCISSOR (Ge and Mooney, 2005), a system that learns an integrated syntactic-semantic parser; and CHILL (Tang and Mooney, 2001) an ILP-based semantic parser. We also compared with the CCG-based semantic parser by Zettlemoyer et al. (2005), but their results are available only for the GEO880 corpus and their experimental set-up is also different from ours. Like KRISP, WASP and SCISSOR also give confidences to the MRs they generate which are used to plot precision-recall curves by measuring precisions and recalls at vari918 0 10 20 30 40 50 60 70 80 90 100 Recall 0 10 20 30 40 50 60 70 80 90 100 Recall Figure 7: Results on t</context>
</contexts>
<marker>Ge, Mooney, 2005</marker>
<rawString>R. Ge and R. J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proc. of 9th Conf. on Computational Natural Language Learning (CoNLL-2005), pages 9–16, Ann Arbor, MI, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Kate</author>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning to transform natural to formal languages.</title>
<date>2005</date>
<booktitle>In Proc. of 20th Natl. Conf. on Artificial Intelligence (AAAI-2005),</booktitle>
<pages>1062--1068</pages>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="1602" citStr="Kate et al., 2005" startWordPosition="221" endWordPosition="224">entences into formal meaning representations have important practical applications in enabling user-friendly natural language communication with computers. However, most of the research in natural language processing (NLP) has been focused on lower-level tasks like syntactic parsing, word-sense disambiguation, information extraction etc. In this paper, we have considered the important task of doing deep semantic parsing to map sentences into their computer-executable meaning representations. Previous work on learning semantic parsers either employ rule-based algorithms (Tang and Mooney, 2001; Kate et al., 2005), or use statistical feature-based methods (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006). In this paper, we present a novel kernel-based statistical method for learning semantic parsers. Kernel methods (Cristianini and Shawe-Taylor, 2000) are particularly suitable for semantic parsing because it involves mapping phrases of natural language (NL) sentences to semantic concepts in a meaning representation language (MRL). Given that natural languages are so flexible, there are various ways in which one can express the same semantic concept. It is difficult for rule-ba</context>
<context position="22620" citStr="Kate et al., 2005" startWordPosition="3724" endWordPosition="3727">the user may give “Texas” as the constant substring for the production “STATEID —* ‘texas’. Then KRISP does not learn classifiers for these constant productions and instead decides if they cover a substring of the sentence or not by matching it with the provided constant substrings. 4 Experiments 4.1 Methodology KRISP was evaluated on CLANG and GEOQUERY domains as described in section 2. The CLANG corpus was built by randomly selecting 300 pieces of coaching advice from the log files of the 2003 RoboCup Coach Competition. These formal advice instructions were manually translated into English (Kate et al., 2005). The GEOQUERY corpus contains 880 English queries collected from undergraduates and from real users of a web-based interface (Tang and Mooney, 2001). These were manually translated into their MRs. The average length of an NL sentence in the CLANG corpus is 22.52 words while in the GEOQUERY corpus it is 7.48 words, which indicates that CLANG is the harder corpus. The average length of the MRs is 13.42 tokens in the CLANG corpus while it is 6.46 tokens in the GEOQUERY corpus. KRISP was evaluated using standard 10-fold cross validation. For every test sentence, only the best MR corresponding to </context>
<context position="24450" citStr="Kate et al., 2005" startWordPosition="4039" endWordPosition="4042">R when submitted to the database. Performance was measured in terms of precision (the percentage of generated MRs that were correct) and recall (the percentage of all sentences for which correct MRs were obtained). In our experiments, the threshold 0 was fixed to 0.05 and the beam size w was 20. These parameters were found through pilot experiments. The maximum number of iterations (MAX ITR) required was only 3, beyond this we found that the system only overfits the training corpus. We compared our system’s performance with the following existing systems: the string and tree versions of SILT (Kate et al., 2005), a system that learns transformation rules relating NL phrases to MRL expressions; WASP (Wong and Mooney, 2006), a system that learns transformation rules using statistical machine translation techniques; SCISSOR (Ge and Mooney, 2005), a system that learns an integrated syntactic-semantic parser; and CHILL (Tang and Mooney, 2001) an ILP-based semantic parser. We also compared with the CCG-based semantic parser by Zettlemoyer et al. (2005), but their results are available only for the GEO880 corpus and their experimental set-up is also different from ours. Like KRISP, WASP and SCISSOR also giv</context>
</contexts>
<marker>Kate, Wong, Mooney, 2005</marker>
<rawString>R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learning to transform natural to formal languages. In Proc. of 20th Natl. Conf. on Artificial Intelligence (AAAI-2005), pages 1062–1068, Pittsburgh, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
</authors>
<title>A kernel-based approach to learning semantic parsers.</title>
<date>2005</date>
<tech>Technical Report UT-AI-05-326,</tech>
<institution>Artificial Intelligence Lab, University of Texas at Austin,</institution>
<location>Austin, TX,</location>
<contexts>
<context position="13920" citStr="Kate, 2005" startWordPosition="2284" endWordPosition="2285">ance to probabilistic context-free grammar (PCFG) parsing for which efficient algorithms are available (Stolcke, 1995), but we note that our task of finding the most probable semantic derivation differs from PCFG parsing in two important ways. First, the probability of a production is not independent of the sentence but depends on which substring of the sentence it covers, and second, the leaves of the tree are not individual terminals of the grammar but are substrings of words of the NL sentence. The extensions needed for Earley’s algorithm are straightforward and are described in detail in (Kate, 2005) but due to space limitation we do not describe them here. Our extended Earley’s algorithm does a beam search and attempts to find the w (a parameter) most probable semantic derivations of an NL sentence s using the probabilities P,(s[i..j]). To make this search faster, it uses a threshold, 0, to prune low probability derivation trees. 3 KRISP’s Training Algorithm In this section, we describe how KRISP learns the classifiers which give the probabilities P,(u) needed for semantic parsing as described in the previous section. Given the training corpus of NL sentences paired with their MRs {(si, </context>
</contexts>
<marker>Kate, 2005</marker>
<rawString>Rohit J. Kate. 2005. A kernel-based approach to learning semantic parsers. Technical Report UT-AI-05-326, Artificial Intelligence Lab, University of Texas at Austin, Austin, TX, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting insertions and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<contexts>
<context position="28922" citStr="Levenshtein, 1966" startWordPosition="4787" endWordPosition="4788">phonetically close word. We simKRISP WASP SCISSOR SILT-tree SILT-string Figure 6: Results on the CLANG corpus. Precision 100 90 80 50 70 60 KRISP WASP SCISSOR SILT-tree SILT-string CHILL Zettlemoyer et al. (2005) English Japanese Spanish Turkish Precision 100 90 80 70 60 50 Precision 100 90 80 70 60 50 919 F-measure 0 1 2 3 4 5 Noise level Figure 9: Results on the CLANG corpus with increasing amounts of noise in the test sentences. ulate this type of noise by substituting a word in the corpus by another word, w, with probability ped(,)*P(w), where p is a parameter, ed(w) is w’s edit distance (Levenshtein, 1966) from the original word and P(w) is w’s probability proportional to its word frequency. The edit distance which calculates closeness between words is character-based rather than based on phonetics, but this should not make a significant difference in the experimental results. Figure 9 shows the results on the CLANG corpus with increasing amounts of noise, from level 0 to level 4. The noise level 0 corresponds to no noise. The noise parameters, Padd and Pdrop, were varied uniformly from being 0 at level 0 and 0.1 at level 4, and the parameter p was varied uniformly from being 0 at level 0 and 0</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>V. I. Levenshtein. 1966. Binary codes capable of correcting insertions and reversals. Soviet Physics Doklady, 10(8):707–710, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>Craig Saunders</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
<author>Chris Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--419</pages>
<contexts>
<context position="2996" citStr="Lodhi et al., 2002" startWordPosition="440" endWordPosition="443"> contrast, kernel methods allow a convenient mechanism to implicitly work with a potentially infinite number of features which can robustly capture these range of contexts even when the data is noisy. Our system, KRISP (Kernel-based Robust Interpretation for Semantic Parsing), takes NL sentences paired with their formal meaning representations as training data. The productions of the formal MRL grammar are treated like semantic concepts. For each of these productions, a SupportVector Machine (SVM) (Cristianini and ShaweTaylor, 2000) classifier is trained using string similarity as the kernel (Lodhi et al., 2002). Each classifier then estimates the probability of the production covering different substrings of the sentence. This information is used to compositionally build a complete meaning representation (MR) of the sentence. Some of the previous work on semantic parsing has focused on fairly simple domains, primarily, ATIS (Air Travel Information Service) (Price, 1990) whose semantic analysis is equivalent to filling a single semantic frame (Miller et al., 1996; Popescu et al., 2004). In this paper, we have tested KRISP on two real-world domains in which meaning representations are more complex wit</context>
<context position="15363" citStr="Lodhi et al. (2002)" startWordPosition="2521" endWordPosition="2524">iteration improving upon the parser learned in the previous iteration. In each iteration, for every production 7r of G, KRISP collects positive and negative example sets. In the first iteration, the set P(7r) of positive examples for production 7r contains all sentences, si, such that parse(mi) uses the production 7r. The set of negative examples, N(7r), for production 7r includes all of the remaining training sentences. Using these positive and negative examples, an SVM classifier 2, C, is trained for each production 7r using a normalized string subsequence kernel. Following the framework of Lodhi et al. (2002), we define a kernel between two strings as the number of common subsequences they share. One difference, however, is that their strings are over characters while our strings are over words. The more the two strings share, the greater the similarity score will be. Normally, SVM classifiers only predict the class of the test example but one can obtain class probability estimates by mapping the distance of the example from the SVM’s separating hyperplane to the range [0,1] using a learned sigmoid function (Platt, 1999). The classifier C, then gives us the probabilities P,(u). We represent the se</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins. 2002. Text classification using string kernels. Journal of Machine Learning Research, 2:419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>David Stallard</author>
<author>Robert Bobrow</author>
<author>Richard Schwartz</author>
</authors>
<title>A fully statistical approach to natural language interfaces.</title>
<date>1996</date>
<booktitle>In Proc. of the 34th Annual Meeting of the Association for Computational Linguistics (ACL96),</booktitle>
<pages>55--61</pages>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="3456" citStr="Miller et al., 1996" startWordPosition="512" endWordPosition="515">productions, a SupportVector Machine (SVM) (Cristianini and ShaweTaylor, 2000) classifier is trained using string similarity as the kernel (Lodhi et al., 2002). Each classifier then estimates the probability of the production covering different substrings of the sentence. This information is used to compositionally build a complete meaning representation (MR) of the sentence. Some of the previous work on semantic parsing has focused on fairly simple domains, primarily, ATIS (Air Travel Information Service) (Price, 1990) whose semantic analysis is equivalent to filling a single semantic frame (Miller et al., 1996; Popescu et al., 2004). In this paper, we have tested KRISP on two real-world domains in which meaning representations are more complex with richer predicates and nested structures. Our experiments demonstrate that KRISP compares favor913 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 913–920, Sydney, July 2006. c�2006 Association for Computational Linguistics NL: “If the ball is in our goal area then our player 1 should intercept it.” CLANG: ((bpos (goal-area our)) (do our {1} intercept)) Figure 1: An example of an NL a</context>
</contexts>
<marker>Miller, Stallard, Bobrow, Schwartz, 1996</marker>
<rawString>Scott Miller, David Stallard, Robert Bobrow, and Richard Schwartz. 1996. A fully statistical approach to natural language interfaces. In Proc. of the 34th Annual Meeting of the Association for Computational Linguistics (ACL96), pages 55–61, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.</title>
<date>1999</date>
<booktitle>Advances in Large Margin Classifiers,</booktitle>
<pages>185--208</pages>
<editor>In Alexander J. Smola, Peter Bartlett, Bernhard Sch¨olkopf, and Dale Schuurmans, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="15885" citStr="Platt, 1999" startWordPosition="2611" endWordPosition="2612">ing a normalized string subsequence kernel. Following the framework of Lodhi et al. (2002), we define a kernel between two strings as the number of common subsequences they share. One difference, however, is that their strings are over characters while our strings are over words. The more the two strings share, the greater the similarity score will be. Normally, SVM classifiers only predict the class of the test example but one can obtain class probability estimates by mapping the distance of the example from the SVM’s separating hyperplane to the range [0,1] using a learned sigmoid function (Platt, 1999). The classifier C, then gives us the probabilities P,(u). We represent the set of these classifiers by C = IC,|7r E G}. Next, using these classifiers, the extended Earley’s algorithm, which we call EXTENDED EARLEY in the pseudo-code, is invoked to obtain the w best semantic derivations for each of the training sentences. The procedure getMR returns the MR for a semantic derivation. At this point, for many training sentences, the resulting most-probable semantic derivation may not give the correct MR. Hence, next, the system collects more refined positive and negative examples to improve the r</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>John C. Platt. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Alexander J. Smola, Peter Bartlett, Bernhard Sch¨olkopf, and Dale Schuurmans, editors, Advances in Large Margin Classifiers, pages 185–208. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Alex Armanasu</author>
<author>Oren Etzioni</author>
<author>David Ko</author>
<author>Alexander Yates</author>
</authors>
<title>Modern natural language interfaces to databases: Composing statistical parsing with semantic tractability.</title>
<date>2004</date>
<booktitle>In Proc. of 20th Intl. Conf. on Computational Linguistics (COLING-04),</booktitle>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="3479" citStr="Popescu et al., 2004" startWordPosition="516" endWordPosition="519">tVector Machine (SVM) (Cristianini and ShaweTaylor, 2000) classifier is trained using string similarity as the kernel (Lodhi et al., 2002). Each classifier then estimates the probability of the production covering different substrings of the sentence. This information is used to compositionally build a complete meaning representation (MR) of the sentence. Some of the previous work on semantic parsing has focused on fairly simple domains, primarily, ATIS (Air Travel Information Service) (Price, 1990) whose semantic analysis is equivalent to filling a single semantic frame (Miller et al., 1996; Popescu et al., 2004). In this paper, we have tested KRISP on two real-world domains in which meaning representations are more complex with richer predicates and nested structures. Our experiments demonstrate that KRISP compares favor913 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 913–920, Sydney, July 2006. c�2006 Association for Computational Linguistics NL: “If the ball is in our goal area then our player 1 should intercept it.” CLANG: ((bpos (goal-area our)) (do our {1} intercept)) Figure 1: An example of an NL advice and its CLANG MR.</context>
</contexts>
<marker>Popescu, Armanasu, Etzioni, Ko, Yates, 2004</marker>
<rawString>Ana-Maria Popescu, Alex Armanasu, Oren Etzioni, David Ko, and Alexander Yates. 2004. Modern natural language interfaces to databases: Composing statistical parsing with semantic tractability. In Proc. of 20th Intl. Conf. on Computational Linguistics (COLING-04), Geneva, Switzerland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patti J Price</author>
</authors>
<title>Evaluation of spoken language systems: The ATIS domain.</title>
<date>1990</date>
<booktitle>In Proc. of 3rd DARPA Speech and Natural Language Workshop,</booktitle>
<pages>91--95</pages>
<contexts>
<context position="3362" citStr="Price, 1990" startWordPosition="498" endWordPosition="499">ctions of the formal MRL grammar are treated like semantic concepts. For each of these productions, a SupportVector Machine (SVM) (Cristianini and ShaweTaylor, 2000) classifier is trained using string similarity as the kernel (Lodhi et al., 2002). Each classifier then estimates the probability of the production covering different substrings of the sentence. This information is used to compositionally build a complete meaning representation (MR) of the sentence. Some of the previous work on semantic parsing has focused on fairly simple domains, primarily, ATIS (Air Travel Information Service) (Price, 1990) whose semantic analysis is equivalent to filling a single semantic frame (Miller et al., 1996; Popescu et al., 2004). In this paper, we have tested KRISP on two real-world domains in which meaning representations are more complex with richer predicates and nested structures. Our experiments demonstrate that KRISP compares favor913 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 913–920, Sydney, July 2006. c�2006 Association for Computational Linguistics NL: “If the ball is in our goal area then our player 1 should interce</context>
</contexts>
<marker>Price, 1990</marker>
<rawString>Patti J. Price. 1990. Evaluation of spoken language systems: The ATIS domain. In Proc. of 3rd DARPA Speech and Natural Language Workshop, pages 91–95, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="13427" citStr="Stolcke, 1995" startWordPosition="2199" endWordPosition="2200">erivation. These will be the ones which maximize the product of the probability of 7r covering the substring s[i..j] with the product of probabilities of all the recursively found most probable partial derivations consistent with the partition (p1, ..,pt). A naive implementation of the above recursion is computationally expensive, but by suitably extending the well known Earley’s context-free parsing algorithm (Earley, 1970), it can be implemented efficiently. The above task has some resemblance to probabilistic context-free grammar (PCFG) parsing for which efficient algorithms are available (Stolcke, 1995), but we note that our task of finding the most probable semantic derivation differs from PCFG parsing in two important ways. First, the probability of a production is not independent of the sentence but depends on which substring of the sentence it covers, and second, the leaves of the tree are not individual terminals of the grammar but are substrings of words of the NL sentence. The extensions needed for Earley’s algorithm are straightforward and are described in detail in (Kate, 2005) but due to space limitation we do not describe them here. Our extended Earley’s algorithm does a beam sear</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>Andreas Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):165–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Tang</author>
<author>R J Mooney</author>
</authors>
<title>Using multiple clause constructors in inductive logic programming for semantic parsing.</title>
<date>2001</date>
<booktitle>In Proc. of the 12th European Conf. on Machine Learning,</booktitle>
<pages>466--477</pages>
<location>Freiburg, Germany.</location>
<contexts>
<context position="1582" citStr="Tang and Mooney, 2001" startWordPosition="217" endWordPosition="220">form natural language sentences into formal meaning representations have important practical applications in enabling user-friendly natural language communication with computers. However, most of the research in natural language processing (NLP) has been focused on lower-level tasks like syntactic parsing, word-sense disambiguation, information extraction etc. In this paper, we have considered the important task of doing deep semantic parsing to map sentences into their computer-executable meaning representations. Previous work on learning semantic parsers either employ rule-based algorithms (Tang and Mooney, 2001; Kate et al., 2005), or use statistical feature-based methods (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006). In this paper, we present a novel kernel-based statistical method for learning semantic parsers. Kernel methods (Cristianini and Shawe-Taylor, 2000) are particularly suitable for semantic parsing because it involves mapping phrases of natural language (NL) sentences to semantic concepts in a meaning representation language (MRL). Given that natural languages are so flexible, there are various ways in which one can express the same semantic concept. It is d</context>
<context position="22769" citStr="Tang and Mooney, 2001" startWordPosition="3747" endWordPosition="3750">ant productions and instead decides if they cover a substring of the sentence or not by matching it with the provided constant substrings. 4 Experiments 4.1 Methodology KRISP was evaluated on CLANG and GEOQUERY domains as described in section 2. The CLANG corpus was built by randomly selecting 300 pieces of coaching advice from the log files of the 2003 RoboCup Coach Competition. These formal advice instructions were manually translated into English (Kate et al., 2005). The GEOQUERY corpus contains 880 English queries collected from undergraduates and from real users of a web-based interface (Tang and Mooney, 2001). These were manually translated into their MRs. The average length of an NL sentence in the CLANG corpus is 22.52 words while in the GEOQUERY corpus it is 7.48 words, which indicates that CLANG is the harder corpus. The average length of the MRs is 13.42 tokens in the CLANG corpus while it is 6.46 tokens in the GEOQUERY corpus. KRISP was evaluated using standard 10-fold cross validation. For every test sentence, only the best MR corresponding to the most probable semantic derivation is considered for evaluation, and its probability is taken as the system’s confidence in that MR. Since KRISP u</context>
<context position="24782" citStr="Tang and Mooney, 2001" startWordPosition="4086" endWordPosition="4089">ough pilot experiments. The maximum number of iterations (MAX ITR) required was only 3, beyond this we found that the system only overfits the training corpus. We compared our system’s performance with the following existing systems: the string and tree versions of SILT (Kate et al., 2005), a system that learns transformation rules relating NL phrases to MRL expressions; WASP (Wong and Mooney, 2006), a system that learns transformation rules using statistical machine translation techniques; SCISSOR (Ge and Mooney, 2005), a system that learns an integrated syntactic-semantic parser; and CHILL (Tang and Mooney, 2001) an ILP-based semantic parser. We also compared with the CCG-based semantic parser by Zettlemoyer et al. (2005), but their results are available only for the GEO880 corpus and their experimental set-up is also different from ours. Like KRISP, WASP and SCISSOR also give confidences to the MRs they generate which are used to plot precision-recall curves by measuring precisions and recalls at vari918 0 10 20 30 40 50 60 70 80 90 100 Recall 0 10 20 30 40 50 60 70 80 90 100 Recall Figure 7: Results on the GEOQUERY corpus. ous confidence levels. The results of the other systems are shown as points o</context>
</contexts>
<marker>Tang, Mooney, 2001</marker>
<rawString>L. R. Tang and R. J. Mooney. 2001. Using multiple clause constructors in inductive logic programming for semantic parsing. In Proc. of the 12th European Conf. on Machine Learning, pages 466–477, Freiburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. ofHuman Language Technology Conf. /North American Association for Computational Linguistics Annual Meeting (HLT/NAACL-2006),</booktitle>
<location>New York City, NY.</location>
<note>To appear.</note>
<contexts>
<context position="1720" citStr="Wong and Mooney, 2006" startWordPosition="239" endWordPosition="242">al language communication with computers. However, most of the research in natural language processing (NLP) has been focused on lower-level tasks like syntactic parsing, word-sense disambiguation, information extraction etc. In this paper, we have considered the important task of doing deep semantic parsing to map sentences into their computer-executable meaning representations. Previous work on learning semantic parsers either employ rule-based algorithms (Tang and Mooney, 2001; Kate et al., 2005), or use statistical feature-based methods (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006). In this paper, we present a novel kernel-based statistical method for learning semantic parsers. Kernel methods (Cristianini and Shawe-Taylor, 2000) are particularly suitable for semantic parsing because it involves mapping phrases of natural language (NL) sentences to semantic concepts in a meaning representation language (MRL). Given that natural languages are so flexible, there are various ways in which one can express the same semantic concept. It is difficult for rule-based methods or even statistical featurebased methods to capture the full range of NL contexts which map to a semantic </context>
<context position="24562" citStr="Wong and Mooney, 2006" startWordPosition="4056" endWordPosition="4059">d MRs that were correct) and recall (the percentage of all sentences for which correct MRs were obtained). In our experiments, the threshold 0 was fixed to 0.05 and the beam size w was 20. These parameters were found through pilot experiments. The maximum number of iterations (MAX ITR) required was only 3, beyond this we found that the system only overfits the training corpus. We compared our system’s performance with the following existing systems: the string and tree versions of SILT (Kate et al., 2005), a system that learns transformation rules relating NL phrases to MRL expressions; WASP (Wong and Mooney, 2006), a system that learns transformation rules using statistical machine translation techniques; SCISSOR (Ge and Mooney, 2005), a system that learns an integrated syntactic-semantic parser; and CHILL (Tang and Mooney, 2001) an ILP-based semantic parser. We also compared with the CCG-based semantic parser by Zettlemoyer et al. (2005), but their results are available only for the GEO880 corpus and their experimental set-up is also different from ours. Like KRISP, WASP and SCISSOR also give confidences to the MRs they generate which are used to plot precision-recall curves by measuring precisions an</context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Yuk Wah Wong and Raymond J. Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proc. ofHuman Language Technology Conf. /North American Association for Computational Linguistics Annual Meeting (HLT/NAACL-2006), New York City, NY. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proc. of 13th Natl. Conf. on Artificial Intelligence (AAAI-96),</booktitle>
<pages>1050--1055</pages>
<location>Portland, OR,</location>
<contexts>
<context position="5454" citStr="Zelle and Mooney, 1996" startWordPosition="837" endWordPosition="840">ing system for semantic parsing is given a training corpus of NL sentences paired with their respective MRs from which it has to induce a semantic parser which can map novel NL sentences to their correct MRs. Figure 1 shows an example of an NL sentence and its MR from the CLANG domain. CLANG (Chen et al., 2003) is the standard formal coach language in which coaching advice is given to soccer agents which compete on a simulated soccer field in the RoboCup 1 Coach Competition. In the MR of the example, bpos stands for “ball position”. The second domain we have considered is the GEOQUERY domain (Zelle and Mooney, 1996) which is a query language for a small database of about 800 U.S. geographical facts. Figure 2 shows an NL query and its MR form in a functional query language. The parse of the functional query language is also shown along with the involved productions. This example is also used later to illustrate how our system does semantic parsing. The MR in the functional query language can be read as if processing a list which gets modified by various functions. From the innermost expression going outwards it means: the state of Texas, the list containing all the states next to the state of Texas and th</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proc. of 13th Natl. Conf. on Artificial Intelligence (AAAI-96), pages 1050–1055, Portland, OR, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proc. of 21th Conf. on Uncertainty in Artificial Intelligence (UAI2005),</booktitle>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="1696" citStr="Zettlemoyer and Collins, 2005" startWordPosition="235" endWordPosition="238">in enabling user-friendly natural language communication with computers. However, most of the research in natural language processing (NLP) has been focused on lower-level tasks like syntactic parsing, word-sense disambiguation, information extraction etc. In this paper, we have considered the important task of doing deep semantic parsing to map sentences into their computer-executable meaning representations. Previous work on learning semantic parsers either employ rule-based algorithms (Tang and Mooney, 2001; Kate et al., 2005), or use statistical feature-based methods (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006). In this paper, we present a novel kernel-based statistical method for learning semantic parsers. Kernel methods (Cristianini and Shawe-Taylor, 2000) are particularly suitable for semantic parsing because it involves mapping phrases of natural language (NL) sentences to semantic concepts in a meaning representation language (MRL). Given that natural languages are so flexible, there are various ways in which one can express the same semantic concept. It is difficult for rule-based methods or even statistical featurebased methods to capture the full range of NL contexts </context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proc. of 21th Conf. on Uncertainty in Artificial Intelligence (UAI2005), Edinburgh, Scotland, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor W Zue</author>
<author>James R Glass</author>
</authors>
<title>Conversational interfaces: Advances and challenges.</title>
<date>2000</date>
<booktitle>In Proc. of the IEEE,</booktitle>
<volume>88</volume>
<issue>8</issue>
<pages>1166--1180</pages>
<contexts>
<context position="27430" citStr="Zue and Glass, 2000" startWordPosition="4531" endWordPosition="4534">competently on other languages as well. 4.4 Experiments with Noisy NL Sentences Any real world application in which semantic parsers would be used to interpret natural language of a user is likely to face noise in the input. If the user is interacting through spontaneous speech and the input to the semantic parser is coming form the output of a speech recognition system then there are many ways in which noise could creep in the NL sentences: interjections (like um’s and ah’s), environment noise (like door slams, phone rings etc.), out-of-domain words, grammatically ill-formed utterances etc. (Zue and Glass, 2000). As opposed to the other systems, KRISP’s stringkernel-based semantic parsing does not use hardmatching rules and should be thus more flexible and robust to noise. We tested this hypothesis by running experiments on data which was artificially corrupted with simulated speech recognition errors. The interjections, environment noise etc. are likely to be recognized as real words by a speech recognizer. To simulate this, after every word in a sentence, with some probability Padd, an extra word is added which is chosen with probability proportional to its word frequency found in the British Natio</context>
</contexts>
<marker>Zue, Glass, 2000</marker>
<rawString>Victor W. Zue and James R. Glass. 2000. Conversational interfaces: Advances and challenges. In Proc. of the IEEE, volume 88(8), pages 1166–1180.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>