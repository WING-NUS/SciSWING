<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000013">
<title confidence="0.9992325">
CollabRank: Towards a Collaborative Approach to Single-Document
Keyphrase Extraction
</title>
<author confidence="0.987691">
Xiaojun Wan and Jianguo Xiao
</author>
<affiliation confidence="0.921086">
Institute of Computer Science and Technology
Peking University, Beijing 100871, China
</affiliation>
<email confidence="0.989342">
{wanxiaojun,xiaojianguo}@icst.pku.edu.cn
</email>
<bodyText confidence="0.999777136363637">
ing (Zhang et al., 2004; Hammouda et al., 2005)
and document summarization (Berger and Mittal,
2000; Buyukkokten et al., 2001).
Keyphrases are usually manually assigned by
authors, especially for journal or conference arti-
cles. However, the vast majority of documents
(e.g. news articles, magazine articles) do not
have keyphrases, therefore it is beneficial to
automatically extract a few keyphrases from a
given document to deliver the main content of
the document. Here, keyphrases are selected
from within the body of the input document,
without a predefined list (i.e. controlled vocabu-
lary). Most previous work focuses on keyphrase
extraction for journal or conference articles,
while this paper focus on keyphrase extraction
for news articles because news article is one of
the most popular document genres on the web
and most news articles have no author-assigned
keyphrases.
Very often, keyphrases of all single documents
in a document set are required to be extracted.
However, all previous methods extract key-
phrases for a specified document based only on
the information contained in that document, such
as the phrase’s TFIDF, position and other syntac-
tic information in the document. One common
assumption of existing methods is that the docu-
ments are independent of each other. Hence the
keyphrase extraction task is conducted separately
without interactions for each document. However,
the multiple documents within an appropriate
cluster context usually have mutual influences
and contain useful clues which can help to ex-
tract keyphrases from each other. For example,
two documents about the same topic “earth-
quake” would share a few common phrases, e.g.
“earthquake”, “victim”, and they can provide
additional knowledge for each other to better
evaluate and extract salient keyphrases from each
other. The idea is borrowed from human’s per-
ception that a user would better understand a
topic expressed in a document if the user reads
more documents about the same topic.
</bodyText>
<sectionHeader confidence="0.997417" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999737217391305">
Previous methods usually conduct the
keyphrase extraction task for single docu-
ments separately without interactions for
each document, under the assumption
that the documents are considered inde-
pendent of each other. This paper pro-
poses a novel approach named Col-
labRank to collaborative single-document
keyphrase extraction by making use of
mutual influences of multiple documents
within a cluster context. CollabRank is
implemented by first employing the clus-
tering algorithm to obtain appropriate
document clusters, and then using the
graph-based ranking algorithm for col-
laborative single-document keyphrase ex-
traction within each cluster. Experimental
results demonstrate the encouraging per-
formance of the proposed approach. Dif-
ferent clustering algorithms have been
investigated and we find that the system
performance relies positively on the qual-
ity of document clusters.
</bodyText>
<sectionHeader confidence="0.948827" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999165333333333">
A keyphrase is defined as a meaningful and sig-
nificant expression consisting of one or more
words in a document. Appropriate keyphrases
can be considered as a highly condensed sum-
mary for a document, and they can be used as a
label for the document to supplement or replace
the title or summary, thus facilitating users’ fast
browsing and reading. Moreover, document key-
phrases have been successfully used in the fol-
lowing IR and NLP tasks: document indexing
(Gutwin et al., 1999), document classification
(Krulwich and Burkey, 1996), document cluster-
</bodyText>
<footnote confidence="0.9973125">
© 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved.
</footnote>
<page confidence="0.979934">
969
</page>
<note confidence="0.96949">
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 969–976
Manchester, August 2008
</note>
<bodyText confidence="0.999679225">
Based on the above assumption, we propose a
novel framework for collaborative single-
document keyphrase extraction by making use of
the additional information from multiple docu-
ments within an appropriate cluster context. The
collaborative framework for keyphrase extraction
consists of the step of obtaining the cluster con-
text and the step of collaborative keyphrase ex-
traction in each cluster. In this study, the cluster
context is obtained by applying the clustering
algorithm on the document set, and we have in-
vestigated how the cluster context influences the
keyphrase extraction performance by employing
different clustering algorithms. The graph-based
ranking algorithm is employed for collaborative
keyphrase extraction for each document in a
specified cluster. Instead of making only use of
the word relationships in a single document, the
algorithm can incorporate the “voting” or “rec-
ommendations” between words in all the docu-
ments of the cluster, thus making use of the
global information existing in the cluster context.
The above implementation of the collaborative
framework is denoted as CollabRank in this pa-
per.
Experiments have been performed on a dataset
consisting of 308 news articles with human-
annotated keyphrases, and the results demon-
strate the good effectiveness of the CollabRank
approach. We also find that the extraction per-
formance is positively correlated with the quality
of cluster context, and existing clustering algo-
rithms can yield appropriate cluster context for
collaborative keyphrase extraction.
The rest of this paper is organized as follows:
Section 2 introduces the related work. The pro-
posed CollabRank is described in detail in Sec-
tion 3. Empirical evaluation is demonstrated in
Section 4 and lastly we conclude this paper in
Section 5.
</bodyText>
<sectionHeader confidence="0.999499" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999504621212121">
The methods for keyphrase (or keyword) extrac-
tion can be roughly categorized into either unsu-
pervised or supervised.
Unsupervised methods usually involve assign-
ing a saliency score to each candidate phrases by
considering various features. Krulwich and Bur-
key (1996) use heuristics based on syntactic
clues to extract keyphrases from a document.
Barker and Cornacchia (2000) propose a simple
system for choosing noun phrases from a docu-
ment as keyphrases. Muñoz (1996) uses an unsu-
pervised learning algorithm to discover two-word
keyphrases. The algorithm is based on Adaptive
Resonance Theory (ART) neural networks.
Steier and Belew (1993) use the mutual informa-
tion statistics to discover two-word keyphrases.
Tomokiyo and Hurst (2003) use pointwise KL-
divergence between multiple language models
for scoring both phraseness and informativeness
of phrases. More recently, Mihalcea and Tarau
(2004) propose the TextRank model to rank key-
words based on the co-occurrence links between
words. Such algorithms make use of “voting” or
“recommendations” between words to extract
keyphrases.
Supervised machine learning algorithms have
been proposed to classify a candidate phrase into
either keyphrase or not. GenEx (Turney, 2000)
and Kea (Frank et al., 1999; Witten et al., 1999)
are two typical systems, and the most important
features for classifying a candidate phrase are the
frequency and location of the phrase in the
document. More linguistic knowledge has been
explored by Hulth (2003). Statistical associations
between keyphrases have been used to enhance
the coherence of the extracted keyphrases (Tur-
ney, 2003). Song et al. (2003) present an infor-
mation gain-based keyphrase extraction system
called KPSpotter. Medelyan and Witten (2006)
propose KEA++ that enhances automatic key-
phrase extraction by using semantic information
on terms and phrases gleaned from a domain-
specific thesaurus. Nguyen and Kan (2007) focus
on keyphrase extraction in scientific publications
by using new features that capture salient mor-
phological phenomena found in scientific key-
phrases.
The tasks of keyphrase extraction and docu-
ment summarization are similar and thus they
have been conducted in a uniform framework.
Zha (2002) proposes a method for simultaneous
keyphrase extraction and text summarization by
using the heterogeneous sentence-to-word rela-
tionships. Wan et al. (2007a) propose an iterative
reinforcement approach to simultaneous key-
phrase extraction and text summarization. Other
related works include web page keyword extrac-
tion (Kelleher and Luz, 2005; Zhang et al., 2005;
Chen et al., 2005), advertising keywords finding
(Yih et al., 2006).
To the best of our knowledge, all previous
work conducts the task of keyphrase extraction
for each single document independently, without
making use of the collaborative knowledge in
multiple documents. We focus on unsupervised
methods in this study.
</bodyText>
<page confidence="0.986499">
970
</page>
<sectionHeader confidence="0.969793" genericHeader="method">
3 The Proposed CollabRank Approach
</sectionHeader>
<subsectionHeader confidence="0.995538">
3.1 Framework Description
</subsectionHeader>
<bodyText confidence="0.999715846153846">
Given a document set for keyphrase extraction of
each single document, CollabRank first employs
the clustering algorithm to group the documents
into a few clusters. The documents within each
cluster are expected to be topic-related and each
cluster can be considered as a context for any
document in the cluster. Given a document clus-
ter, CollabRank makes use of the global word
relationships in the cluster to evaluate and rank
candidate phrases for each single document in
the cluster based on the graph-based ranking al-
gorithm. Figure 1 gives the framework of the
proposed approach.
</bodyText>
<listItem confidence="0.863564842105263">
1. Document Clustering: Group the documents in the
document set D into a few clusters using the cluster-
ing algorithm;
2. Collaborative Keyphrase Extraction: For each
cluster C, perform the following steps respectively
to extract keyphrases for single documents in the
cluster in a batch mode:
1) Cluster-level Word Evaluation: Build a
global affinity graph G based on all candidate
words restricted by syntactic filters in the documents
of the given cluster C, and employ the graph-ranking
based algorithm to compute the cluster-level sali-
ency score for each word.
2) Document-level Keyphase Extraction: For
any single document d in the cluster, evaluate the
candidate phrases in the document based on the
scores of the words contained in the phrases, and fi-
nally choose a few phrases with highest scores as
the keyphrases of the document.
</listItem>
<figureCaption confidence="0.999626">
Figure 1. The Framework of CollabRank
</figureCaption>
<bodyText confidence="0.999966918367347">
In the first step of the above framework, dif-
ferent clustering algorithms will yield different
clusters. The documents in a high-quality cluster
are usually deemed to be highly topic-related (i.e.
appropriate cluster context), while the documents
in a low-quality cluster are usually not topic-
related (i.e. inappropriate cluster context). The
quality of a cluster will influence the reliability
of the contextual information for evaluating the
words in the cluster. A number of clustering al-
gorithms will be investigated in the experiments,
including the agglomerative algorithm (both av-
erage-link and complete-link), the divisive algo-
rithm, and the kmeans algorithm (Jain et al.,
1999), whose details will be described in the
evalution section.
In the second step of the above framework,
substep 1) aims to evaluate all candidate words
in the cluster based on the graph-based ranking
algorithm. The global affinity graph aims to re-
flect the cluster-level co-occurrence relationships
between all candidate words in the documents of
the given cluster. The saliency scores of the
words are computed based on the global affinity
graph to indicate how much information about
the main topic the words reflect. Substep 2) aims
to evaluate candidate phrases of each single
document based on the cluster-level word scores,
and then choose a few salient phrases as key-
phrases of the document. Substep 1) is performed
on all documents in the cluster in order to evalu-
ate the words from a global perspective, while
substep 2) is performed on each single document
in order to extract keyphrases from a local per-
spective. A keyphrase of a document is expected
to include highly salient words. We can see that
the keyphrase extraction tasks are conducted in a
batch mode for each cluster. The substeps of 1)
and 2) will be described in next sections respec-
tively. If substep 1) is performed on each single
document without considering the cluster context,
the approach is degenerated into the simple Tex-
tRank model (Mihalcea and Tarau, 2004), which
is denoted as SingleRank in this paper.
It is noteworthy that in addition to the graph-
based ranking algorithm, other keyphrase extrac-
tion methods can also be integrated in the pro-
posed collaborative framework to exploit the col-
laborative knowledge in the cluster context.
</bodyText>
<subsectionHeader confidence="0.999454">
3.2 Cluster-Level Word Evaluation
</subsectionHeader>
<bodyText confidence="0.999973590909091">
Like the PageRank algorithm (Page et al., 1998),
the graph-based ranking algorithm employed in
this study is essentially a way of deciding the
importance of a vertex within a graph based on
global information recursively drawn from the
entire graph. The basic idea is that of “voting” or
“recommendation” between the vertices. A link
between two vertices is considered as a vote cast
from one vertex to the other vertex. The score
associated with a vertex is determined by the
votes that are cast for it, and the score of the ver-
tices casting these votes.
Formally, given a specified cluster C, let G=(V,
E) be an undirected graph to reflect the relation-
ships between words in the cluster. V is the set of
vertices and each vertex is a candidate word2 in
the cluster. Because not all words in the docu-
ments are good indicators of keyphrases, the
words added to the graph are restricted with syn-
tactic filters, i.e., only the words with a certain
part of speech are added. As in Mihalcea and
Tarau (2004), the documents are tagged by a
</bodyText>
<page confidence="0.686939">
2 The original words are used without stemming.
971
</page>
<bodyText confidence="0.95940271875">
POS tagger, and only the nouns and adjectives
are added into the vertex set3. E is the set of
edges, which is a subset of V×V. Each edge eij in
E is associated with an affinity weight aff(vi,vj)
between words vi and vj. The weight is computed
based on the co-occurrence relation between the
two words, controlled by the distance between
word occurrences. The co-occurrence relation
can express cohesion relationships between
words. Two vertices are connected if the corre-
sponding words co-occur at least once within a
window of maximum k words, where k can be set
anywhere from 2 to 20 words. The affinity
weight aff(vi,vj) is simply set to be the count of
the controlled co-occurrences between the words
vi and vj in the whole cluster as follows:
aff v i , v j ∑ count d v , v
( ) = ( i j
d C
∈
where countd(vi,vj) is the count of the controlled
co-occurrences between words vi and vj in docu-
ment d.
The graph is built based on the whole cluster
and it is called Global Affinity Graph. The big-
gest difference between CollabRank and
SingleRank is that SingleRank builds a local
graph based on each single document.
We use an affinity matrix M to describe G
with each entry corresponding to the weight of
an edge in the graph. M = (Mi,j)|V|×|V |is defined as
follows:
</bodyText>
<equation confidence="0.9897986">
M = ⎨
i,j ⎩
⎧aff v , v , v
0 otherwise
, ( ) if links with and ;
i j i j
v i j
≠
(2)
~
</equation>
<bodyText confidence="0.7557325">
Then M is normalized to M as follows to make
the sum of each row equal to 1:
</bodyText>
<equation confidence="0.969157222222222">
|V ||V|
⎧
~M , M ≠
i,j i,j
⎪⎨ M if 0
i,j
∑ = ∑ =
M = (3)
i,j j 1 j 1⎪⎩
</equation>
<bodyText confidence="0.993074833333333">
Based on the global affinity graph G, the clus-
ter-level saliency score WordScoreclus(vi) for
word vi can be deduced from those of all other
words linked with it and it can be formulated in a
recursive form as in the PageRank algorithm:
And the matrix form is:
</bodyText>
<equation confidence="0.9447345">
e
|V |(5)
</equation>
<footnote confidence="0.99842875">
3 The corresponding POS tags of the candidate words
include “JJ”, “NN”, “NNS”, “NNP”, “NNPS”. We
used the Stanford log-linear POS tagger (Toutanova
and Manning, 2000) in this study.
</footnote>
<equation confidence="0.42666">
r
</equation>
<bodyText confidence="0.961008217391304">
where λ= [WordScoreclus (vi )]|V|×1 is the vector of
word saliency scores. er is a vector with all ele-
ments equaling to 1. µ is the damping factor usu-
ally set to 0.85, as in the PageRank algorithm.
The above process can be considered as a
Markov chain by taking the words as the states
and the corresponding transition matrix is given
byµM T + (1− µ) e e T . The stationary probabil-
|V|
ity distribution of each state is obtained by the
principal eigenvector of the transition matrix.
For implementation, the initial scores of the
words are set to 1 and the iteration algorithm in
Equation (4) is adopted to compute the new
scores of the words. Usually the convergence of
the iteration algorithm is achieved when the dif-
ference between the scores computed at two suc-
cessive iterations for any words falls below a
given threshold (0.0001 in this study).
For SingleRank, the saliency score Word-
Scoredoc(vi) for word vi is computed in the same
iterative way based on the local graph for the
single document.
</bodyText>
<subsectionHeader confidence="0.991447">
3.3 Document-Level Keyphrase Extraction
</subsectionHeader>
<bodyText confidence="0.999960388888889">
After the scores of all candidate words in the
cluster have been computed, candidate phrases
are selected and evaluated for each single docu-
ment in the cluster. The candidate words (i.e.
nouns and adjectives) of a specified document d
in the cluster, which is a subset of V, are marked
in the document text, and sequences of adjacent
candidate words are collapsed into a multi-word
phrase. The phrases ending with an adjective are
not allowed, and only the phrases ending with a
noun are collected as the candidate phrases for
the document. For instance, in the following sen-
tence: “Mad/JJ cow/NN disease/NN has/VBZ
killed/VBN 10,000/CD cattle/NNS”, the candi-
date phrases are “Mad cow disease” and “cattle”.
The score of a candidate phrase pi is computed
by summing the cluster-level saliency scores of
the words contained in the phrase.
</bodyText>
<equation confidence="0.952444666666667">
PhraseScore(pi) = ∑ WordScoreclus (vj) (6)
vj pi
∈
</equation>
<bodyText confidence="0.999915285714286">
All the candidate phrases in the document are
ranked in decreasing order of the phrase scores
and the top n phrases are selected as the key-
phrases of the document. n ranges from 1 to 20 in
this study. Similarly for SingleRank, the phrase
score is computed based on the document-level
saliency scores of the words.
</bodyText>
<equation confidence="0.989104">
~WordScorelus (vi) = µ ⋅ ∑ WordScorelus (vj)⋅Mj,i + (1V )
all j≠
I (4)
) (1)
0 , otherwise
</equation>
<page confidence="0.988225">
972
</page>
<sectionHeader confidence="0.988144" genericHeader="method">
4 Empirical Evaluation
</sectionHeader>
<subsectionHeader confidence="0.992724">
4.1 Data Set
</subsectionHeader>
<bodyText confidence="0.99993925">
To our knowledge, there is no gold standard
news dataset with assigned keyphrases for
evaluation. So we manually annotated the
DUC2001 dataset (Over, 2001) and used the
annotated dataset for evaluation in this study.
The dataset was originally used for document
summarization. It consisted of 309 news articles
collected from TREC-9, in which two articles
were duplicate (i.e. d05a\FBIS-41815 and
d05a\FBIS-41815~). The average length of the
documents was 740 words. Two graduate stu-
dents were employed to manually label the key-
phrases for each document. At most 10 key-
phrases could be assigned to each document. The
annotation process lasted two weeks. The Kappa
statistic for measuring inter-agreement among
annotators was 0.70. And the annotation conflicts
between the two subjects were solved by discus-
sion. Finally, 2488 keyphrases were labeled for
the dataset. The average keyphrase number per
document was 8.08 and the average word num-
ber per keyphrase was 2.09.
The articles have been grouped into 30 clusters
manually by NIST annotators for multi-
document summarization, and the documents
within each cluster were topic-related or relevant.
The manually labeled clusters were considered as
the ground truth clusters or gold clusters. In order
to investigate existing clustering algorithms, the
documents in the clusters were mixed together to
form the whole document set for automatic clus-
tering.
</bodyText>
<subsectionHeader confidence="0.987732">
4.2 Document Clustering Algorithm
</subsectionHeader>
<bodyText confidence="0.947964152173913">
In the experiments, several popular clustering
algorithms and random clustering algorithms are
explored to produce cluster contexts. Note that
we have already known the number (i.e. 30) of
the clusters for the dataset beforehand and thus
we simply use it as input for the following clus-
tering algorithms4.
Gold Standard Clustering: It is a pseudo
clustering algorithm by manually grouping the
documents. We use the ground truth clusters as
the upperbound of the following automatic clus-
tering algorithms.
Kmeans Clustering: It is a partition based
clustering algorithm. The algorithm randomly
4 How to obtain the number of desired clusters is not
the focus of this study.
selects 30 documents as the initial centroids of
the 30 clusters and then iteratively assigns all
documents to the closest cluster, and recomputes
the centroid of each cluster, until the centroids do
not change. The similarity between a document
and a cluster centroid is computed using the
standard Cosine measure.
Agglomerative (AverageLink) Clustering: It
is a bottom-up hierarchical clustering algorithm
and starts with the points as individual clusters
and, at each step, merges the most similar or
closest pair of clusters, until the number of the
clusters reduces to the desired number 30. The
similarity between two clusters is computed us-
ing the AverageLink method, which computes
the average of the Cosine similarity values be-
tween any pair of documents belonging to the
two clusters respectively as follows:
where di, dj are two documents in cluster c1 and
cluster c2 respectively, and |c1 |and |c2 |are respec-
tively the numbers of documents in clusters c1
and c2.
Agglomerative (CompleteLink) Clustering:
It differs from the above agglomerative (Aver-
ageLink) clustering algorithm only in that the
similarity between two clusters is computed us-
ing the CompleteLink method, which computes
the minimum of the Cosine similarity values be-
tween any pair of documents belonging to the
two clusters respectively as follows:
</bodyText>
<equation confidence="0.718359">
sim c , c _ diec ,djec sim di, dj
( 1 2) min 1 2 { (
</equation>
<bodyText confidence="0.9938721875">
Divisive Clustering: It is a top-down hierar-
chical clustering algorithm and starts with one,
all-inclusive cluster and, at each step, splits the
largest cluster (i.e. the cluster with most docu-
ments) into two small clusters using the Kmeans
algorithm until the number of clusters increases
to the desired number 30.
Random Clustering: It produces 30 clusters
by randomly assigning each document into one
of the k clusters. Three different randomization
processes are performed and we denote them as
Random1, Random2 and Random3, respectively.
CollabRank relies on the clustering algorithm
for document clustering, and the combination of
CollabRank and any clustering algorithm will be
investigated.
</bodyText>
<subsectionHeader confidence="0.988504">
4.3 Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.9992115">
For evaluation of document clustering results, we
adopt the widely used F-Measure to measure the
</bodyText>
<equation confidence="0.9957635">
m n
sim (c1, c2 )
∑∑ sim(di, dj) (7)
i_1 j_1
X c2
_
c1
)} (8)
</equation>
<page confidence="0.98808">
973
</page>
<bodyText confidence="0.999884352941177">
performance of the clustering algorithm (i.e. the
quality of the clusters) by comparing the pro-
duced clusters with the gold clusters (classes)
(Jain et al., 1999).
For evaluation of keyphrase extraction results,
the automatic extracted keyphrases are compared
with the manually labeled keyphrases. The words
are converted to their corresponding basic forms
using word stemming before comparison. The
precision p=countcorrect/countsystem, recall
r=countcorrect/counthuman, F-measure (F=2pr/(p+r))
are used as evaluation metrics, where countcorrect
is the total number of correct keyphrases ex-
tracted by the system, and countsystem is the total
number of automatic extracted keyphrases, and
counthuman is the total number of human-labeled
keyphrases.
</bodyText>
<subsectionHeader confidence="0.987795">
4.4 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.999264">
First of all, we show the document clustering
results in Table 1. The gold standard clustering
result is the upperbound of all automatic cluster-
ing results. Seen from the table, all the four
popular clustering algorithms (i.e. CompleteLink,
AverageLink, KMeans and Divisive) perform
much better than the three random clustering al-
gorithms (i.e. Random1, Random2 and Ran-
dom3). Different clustering results lead to differ-
ent document relationships and a high-quality
cluster produced by popular algorithms is
deemed to build an appropriate cluster context
for collaborative keyphrase extraction.
</bodyText>
<table confidence="0.998580888888889">
Clustering Algorithm F-Measure
Gold 1.000
CompleteLink 0.907
AverageLink 0.877
Divisive 0.924
Kmeans 0.866
Random1 0.187
Random2 0.189
Random3 0.183
</table>
<tableCaption confidence="0.999667">
Table 1. Clustering Results
</tableCaption>
<bodyText confidence="0.998664909090909">
Now we show the results for keyphrase extrac-
tion. In the experiments, the keyphrase number is
typically set to 10 and the co-occurrence window
size is also simply set to 10. Table 2 gives the
comparison results of baseline methods and the
proposed CollabRank methods with different
clustering algorithms. The TFIDF baseline com-
putes the word scores for each single document
based on the word’s TFIDF value. The SingleR-
ank baseline computes the word scores for each
single document based on the graph-based rank-
ing algorithm. The two baselines do not make
use of the cluster context.
Seen from Table 2, the CollabRank methods
with the gold standard clustering algorithm or
popular clustering algorithms (i.e. Kmeans,
CompleteLink, AverageLink and Divisive) per-
form much better than the baseline methods over
all three metrics. The results demonstrate the
good effectiveness of the proposed collaborative
framework. We can also see that the performance
is positively correlated with the clustering results.
The CollabRank method with the best perform-
ing gold standard clustering results achieves the
best performance. While the methods with low-
quality clustering results (i.e. the three random
clustering results) do not perform well, even
much worse than the baseline SingleRank
method. This is because that the documents in a
low-quality cluster are not truly topic-related,
and the mutual influences between the docu-
ments are not reliable for evaluating words from
a global perspective.
</bodyText>
<table confidence="0.99971652631579">
System Precision Recall F-measure
TFIDF 0.232 0.281 0.254
SingleRank 0.247 0.303 0.272
CollabRank 0.283 0.348 0.312
(Gold)
CollabRank 0.276 0.339 0.304
(Kmeans)
CollabRank 0.281 0.345 0.310
(CompleteLink)
CollabRank 0.277 0.340 0.306
(AverageLink)
CollabRank 0.274 0.337 0.302
(Divisive)
CollabRank 0.210 0.258 0.232
(Random1)
CollabRank 0.216 0.265 0.238
(Random2)
CollabRank 0.209 0.257 0.231
(Random3)
</table>
<tableCaption confidence="0.999402">
Table 2. Keyphrase Extraction Results
</tableCaption>
<bodyText confidence="0.999130176470588">
In order to investigate how the co-occurrence
window size k and the keyphrase number n influ-
ence the performance, we first vary k from 2 to
20 when n is fixed as 10 and the results are
shown in Figures 2-4 over three metrics respec-
tively. The results demonstrate that all the meth-
ods are not significantly affected by the window
size. We then vary n from 1 to 20 when k is fixed
as 10 and the results are shown in Figures 5-7.
The results demonstrate that the precision values
decrease with the increase of n, and the recall
values increases with the increase of n, while the
F-measure values first increase and then tend to
decrease with the increase of n.
We can also see from Figures 2-7 that the Col-
labRank methods with high-quality clustering
results always perform better than the baseline
</bodyText>
<page confidence="0.991163">
974
</page>
<bodyText confidence="0.99630325">
SingleRank method under different window sizes
and different keyphrase numbers, and they al-
ways lead to poor performance with low-quality
clustering results. This further proves that an ap-
propriate cluster context is very important for the
CollabRank method. Fortunately, existing clus-
tering algorithms can obtain the desired cluster
context.
</bodyText>
<figure confidence="0.99830544">
0.29
0.27
Precision
0.25
0.23
0.21
0.19
0.24
0.22
0.2
0.31
Recall
0.29
0.27
0.25
0.23
0.32
0.35
0.3
0.33
0.28
F-measure
0.26
2 4 6 8 10 12 14 16 18 20
Window size k
</figure>
<figureCaption confidence="0.995108">
Figure 2. Precision vs. Window Size k
</figureCaption>
<figure confidence="0.91169">
2 4 6 8 10 12 14 16 18 20
Window size k
</figure>
<figureCaption confidence="0.971733">
Figure 3. Recall vs. Window Size k
</figureCaption>
<figure confidence="0.912738">
2 4 6 8 10 12 14 16 18 20
Window size k
</figure>
<figureCaption confidence="0.98959">
Figure 4. F-measure vs. Window Size k
</figureCaption>
<figure confidence="0.999758205882353">
0.3
0.47
0.47
0.42
0.25
0.42
0.37
0.32
F-measure
Precision
0.37
0.2
Recall
0.27
0.32
0.22
0.15
0.17
0.27
0.12
0.1
0.22
0.07
0.05
0.02
0.17
0 1 7
SingleRank ColabRank(Gold) CollabRank(Kmeans)
y p h r a s e n u m b e n
CollabRank(CompleteLink) ColabRank(AverageLink) CollabRank(Divisive)
CollabRank(Random1) ColabRank(Random2) CollabRank(Random3)
P r s
1 2 4 6 8 10 12 14 16 18 20
Keyphrase number n
</figure>
<figureCaption confidence="0.999016">
Figure 5. Precision vs. Keyphrase Number
</figureCaption>
<figure confidence="0.994333666666667">
n
1 2 4 6 8 10 12 14 16 18 20
Keyphrase number n
</figure>
<figureCaption confidence="0.986996">
Figure 6. Recall vs. Keyphrase Number n
</figureCaption>
<figure confidence="0.9938465">
1 2 4 6 8 10 12 14 16 18 20
Keyphrase number n
</figure>
<figureCaption confidence="0.999619">
Figure 7. F-measure vs. Keyphrase Num-
</figureCaption>
<bodyText confidence="0.967663451612903">
ber n
The proposed CollabRank method makes only
use of the global information based on the global
graph for the cluster. In order to investigate the
relative contributions from the whole cluster and
the single document to the final performance, we
experiment with the method named RankFusion
which makes both of the cluster-level global in-
formation and the document-level local informa-
tion. The overall word score WordScorefusion(vi)
for word vi in a document in RankFusion is a lin-
ear combination of the global word score and the
local word score as follows:
WordScorefus ion (vi) = A • WordScoreclus (vi) + (1− A) • WordScoredoc (vi) (9)
where A∈[0,1] is the fusion weight. Then the
phrase score is computed based on the fusion
scores of the words. The RankFusion method is
the same with CollabRank if A=1 and it is the
same with SingleRank if A=0.
Figure 8 shows the F-measure curves for the
RankFusion methods with different high-quality
clustering algorithms under different fusion
weights. We can see that when A∈(0.5,1), the
RankFusion methods with high-quality clusters
can outperform both the corresponding SingleR-
ank and the corresponding CollabRank. However,
the performance improvements of RankFusion
over CollabRank are not significant. We can
conclude that the cluster-level global information
plays the key role for evaluating the true saliency
of the words.
</bodyText>
<figureCaption confidence="0.993506">
Figure 8. RankFusion Results (F-measure) vs. Fusion
</figureCaption>
<note confidence="0.204935">
Weight A
</note>
<sectionHeader confidence="0.988719" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999867333333333">
In this paper, we propose a novel approach
named CollabRank for collaborative single-
document keyphrase extraction, which makes use
of the mutual influences between documents in
appropriate cluster context to better evaluate the
saliency of words and phrases. Experimental re-
</bodyText>
<figure confidence="0.991374090909091">
F-measure
0.32
0.31
0.29
0.28
0.27
0.3
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Fusion weight λ
Gold Kmeans CompleteLink
AverageLink Divisive
</figure>
<page confidence="0.981288">
975
</page>
<bodyText confidence="0.9999882">
sults demonstrate the good effectiveness of Col-
labRank. We also find that the clustering algo-
rithm is important for obtaining the appropriate
cluster context and the low-quality clustering
results will deteriorate the extraction perform-
ance. It is encouraging that most existing popular
clustering algorithms can meet the demands of
the proposed approach.
The proposed collaborative framework has
more implementations than the implementation
based on the graph-based ranking algorithm in
this study. In future work, we will explore other
keyphrase extraction methods in the proposed
collaborative framework to validate the robust-
ness of the framework.
</bodyText>
<sectionHeader confidence="0.998821" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.874731666666667">
This work was supported by the National Science
Foundation of China (No.60703064), the Re-
search Fund for the Doctoral Program of Higher
Education of China (No.20070001059) and the
National High Technology Research and Devel-
opment Program of China (No.2008AA01Z421).
</bodyText>
<sectionHeader confidence="0.886898" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999110534883721">
A. Berger and V. Mittal. 2000. OCELOT: A system for
summarizing Web Pages. In Proceedings of SIGIR2000.
K. Barker and N. Cornacchia. 2000. Using nounphrase
heads to extract document keyphrases. In Canadian Confer-
ence on AI.
O. Buyukkokten, H. Garcia-Molina, and A. Paepcke. 2001.
Seeing the whole in parts: text summarization for web
browsing on handheld devices. In Proceedings of
WWW2001.
M. Chen, J.-T. Sun, H.-J. Zeng and K.-Y. Lam. 2005. A
practical system for keyphrase extraction for web pages. In
Proceedings of CIKM2005.
E. Frank, G. W. Paynter, I. H. Witten, C. Gutwin, and C. G.
Nevill-Manning. 1999. Domain-specific keyphrase extrac-
tion. Proceedings of IJCAI-99, pp. 668-673.
C. Gutwin, G. W. Paynter, I. H. Witten, C. G. Nevill-
Manning and E. Frank. 1999. Improving browsing in digital
libraries with keyphrase indexes. Journal of Decision Sup-
port Systems, 27, 81-104.
K. M. Hammouda, D. N. Matute and M. S. Kamel. 2005.
CorePhrase: keyphrase extraction for document clustering.
In Proceedings of MLDM2005.
A. Hulth. 2003. Improved automatic keyword extraction
given more linguistic knowledge. In Proceedings of
EMNLP2003, Japan, August.
A. K. Jain, M. N. Murty and P. J. Flynn. 1999. Data cluster-
ing: a review. ACM Computing Surveys, 31(3):264-323.
D. Kelleher and S. Luz. 2005. Automatic hypertext key-
phrase detection. In Proceedings of IJCAI2005.
B. Krulwich and C. Burkey. 1996. Learning user informa-
tion interests through the extraction of semantically signifi-
cant phrases. In AAAI 1996 Spring Symposium on Machine
Learning in Information Access.
O. Medelyan and I. H. Witten. 2006. Thesaurus based auto-
matic keyphrase indexing. In Proceedings of JCDL2006.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing order
into texts. In Proceedings of EMNLP2004.
A. Muñoz. 1996. Compound key word generation from
document databases using a hierarchical clustering ART
model. Intelligent Data Analysis, 1(1).
T. D. Nguyen and M.-Y. Kan. 2007. Keyphrase extraction
in scientific publications. In Proceedings of ICADL2007.
P. Over. 2001. Introduction to DUC-2001: an intrinsic
evaluation of generic news text summarization systems. In
Proceedings of DUC2001.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998. The
pagerank citation ranking: Bringing order to the web. Tech-
nical report, Stanford Digital Libraries.
M. Song, I.-Y. Song and X. Hu. 2003. KPSpotter: a flexible
information gain-based keyphrase extraction system. In
Proceedings of WIDM2003.
A. M. Steier and R. K. Belew. 1993. Exporting
phrases: A statistical analysis of topical language. In
Proceedings of Second Symposium on Document Analysis
and Information Retrieval, pp. 179-190.
T. Tomokiyo and M. Hurst. 2003. A language model ap-
proach to keyphrase extraction. In: Proceedings of ACL
Workshop on Multiword Expressions.
K. Toutanova and C. D. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy Part-of-
Speech tagger. In Proceedings of EMNLP/VLC-2000.
P. D. Turney. 2000. Learning algorithms for keyphrase ex-
traction. Information Retrieval, 2:303-336.
P. D. Turney. 2003. Coherent keyphrase extraction via web
mining. In Proc. of IJCAI-03, pages 434–439.
X. Wan, J. Yang and J. Xiao. 2007a. Towards an iterative
reinforcement approach for simultaneous document summa-
rization and keyword extraction. In Proceedings of
ACL2007.
I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and C. G.
Nevill-Manning. 1999. KEA: Practical automatic keyphrase
extraction. Proceedings of Digital Libraries 99 (DL&apos;99), pp.
254-256.
W.-T. Yih, J. Goodman and V. R. Carvalho. 2006. Finding
advertising keywords on web pages. In Proceedings of
WWW2006.
H. Y. Zha. 2002. Generic summarization and keyphrase
extraction using mutual reinforcement principle and sen-
tence clustering. In Proceedings of SIGIR2002, pp. 113-120.
Y. Zhang, N. Zincir-Heywood, and E. Milios. 2004. Term-
Based Clustering and Summarization of Web Page Collec-
tions. In Proceedings of the Seventeenth Conference of the
Canadian Society for Computational Studies of Intelligence.
Y. Zhang, N. Zincir-Heywood and E. Milios. 2005. Narra-
tive text classification for automatic key phrase extraction in
web document corpora. In Proceedings of WIDM2005.
</reference>
<page confidence="0.995255">
976
</page>
</variant>
</algorithm>
<algorithm name="AAMatching" version="110505">
  <results time="1336575211" date="Wed May  9 22:53:30 SGT 2012">
    <authors>
      <author>
        <fullname source="parscit">Jianguo Xiao</fullname>
        <institutions>
          <institution>Institute of Computer Science and Technology Peking University, Beijing 100871, China</institution>
        </institutions>
      </author>
      <author>
        <fullname source="parscit">Xiaojun Wan</fullname>
        <institutions>
          <institution>Institute of Computer Science and Technology Peking University, Beijing 100871, China</institution>
        </institutions>
      </author>
    </authors>
    <institutions>
      <institution>Institute of Computer Science and Technology Peking University, Beijing 100871, China</institution>
    </institutions>
  </results>
</algorithm>

</algorithms>